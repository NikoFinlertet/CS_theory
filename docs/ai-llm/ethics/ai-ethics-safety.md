# üõ°Ô∏è –≠—Ç–∏–∫–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ò–ò

## ‚öñÔ∏è –ü—Ä–∏–Ω—Ü–∏–ø—ã —ç—Ç–∏—á–Ω–æ–≥–æ –ò–ò

### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã
```
–≠—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –ò–ò:
‚îú‚îÄ‚îÄ –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å (Fairness)
‚îú‚îÄ‚îÄ –ü–æ–¥–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å (Accountability) 
‚îú‚îÄ‚îÄ –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å (Transparency)
‚îú‚îÄ‚îÄ –û–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å (Explainability)
‚îú‚îÄ‚îÄ –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å
‚îú‚îÄ‚îÄ –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å
‚îî‚îÄ‚îÄ –ù–µ–¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è
```

### –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ (Bias)
```python
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

def evaluate_bias(y_true, y_pred, sensitive_attributes):
    """–û—Ü–µ–Ω–∫–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ –∑–∞—â–∏—â–µ–Ω–Ω—ã–º –∞—Ç—Ä–∏–±—É—Ç–∞–º"""
    results = {}
    
    for attr_name, attr_values in sensitive_attributes.items():
        results[attr_name] = {}
        
        for value in np.unique(attr_values):
            mask = attr_values == value
            group_y_true = y_true[mask]
            group_y_pred = y_pred[mask]
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã
            group_accuracy = np.mean(group_y_true == group_y_pred)
            
            # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å
            if np.sum(group_y_pred == 1) > 0:
                ppv = np.sum((group_y_true == 1) & (group_y_pred == 1)) / np.sum(group_y_pred == 1)
            else:
                ppv = 0
            
            results[attr_name][value] = {
                'accuracy': group_accuracy,
                'positive_predictive_value': ppv,
                'sample_size': len(group_y_true)
            }
    
    return results

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
bias_results = evaluate_bias(y_test, predictions, {'gender': gender_data, 'age_group': age_groups})
```

### –ú–µ—Ç—Ä–∏–∫–∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏
```python
def fairness_metrics(y_true, y_pred, sensitive_attr):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏"""
    
    # Demographic Parity
    positive_rate_group_0 = np.mean(y_pred[sensitive_attr == 0])
    positive_rate_group_1 = np.mean(y_pred[sensitive_attr == 1])
    demographic_parity = abs(positive_rate_group_0 - positive_rate_group_1)
    
    # Equal Opportunity
    tpr_group_0 = np.mean(y_pred[(sensitive_attr == 0) & (y_true == 1)])
    tpr_group_1 = np.mean(y_pred[(sensitive_attr == 1) & (y_true == 1)])
    equal_opportunity = abs(tpr_group_0 - tpr_group_1)
    
    # Equalized Odds
    fpr_group_0 = np.mean(y_pred[(sensitive_attr == 0) & (y_true == 0)])
    fpr_group_1 = np.mean(y_pred[(sensitive_attr == 1) & (y_true == 0)])
    equalized_odds = abs(equal_opportunity) + abs(fpr_group_0 - fpr_group_1)
    
    return {
        'demographic_parity': demographic_parity,
        'equal_opportunity': equal_opportunity,
        'equalized_odds': equalized_odds
    }
```

---

## üîç –û–±—ä—è—Å–Ω–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π (XAI)

### SHAP (SHapley Additive exPlanations)
```python
import shap
from sklearn.ensemble import RandomForestClassifier

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = RandomForestClassifier()
model.fit(X_train, y_train)

# SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
shap.summary_plot(shap_values[1], X_test, feature_names=feature_names)
shap.waterfall_plot(explainer.expected_value[1], shap_values[1][0], X_test.iloc[0])
```

### LIME (Local Interpretable Model-agnostic Explanations)
```python
import lime
import lime.lime_tabular

# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏—Ç–µ–ª—è
explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train.values,
    feature_names=feature_names,
    class_names=['Class 0', 'Class 1'],
    mode='classification'
)

# –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
explanation = explainer.explain_instance(
    X_test.iloc[0].values, 
    model.predict_proba,
    num_features=10
)

explanation.show_in_notebook(show_table=True)
```

---

## üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ò–ò

### Adversarial Attacks
```python
import torch
import torch.nn.functional as F

def fgsm_attack(model, data, target, epsilon=0.1):
    """Fast Gradient Sign Method –∞—Ç–∞–∫–∞"""
    data.requires_grad_(True)
    
    output = model(data)
    loss = F.cross_entropy(output, target)
    
    model.zero_grad()
    loss.backward()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ adversarial –ø—Ä–∏–º–µ—Ä–∞
    perturbation = epsilon * data.grad.data.sign()
    adversarial_data = data + perturbation
    
    return adversarial_data

def pgd_attack(model, data, target, epsilon=0.1, alpha=0.01, num_iter=10):
    """Projected Gradient Descent –∞—Ç–∞–∫–∞"""
    adversarial_data = data.clone().detach()
    
    for _ in range(num_iter):
        adversarial_data.requires_grad_(True)
        output = model(adversarial_data)
        loss = F.cross_entropy(output, target)
        
        model.zero_grad()
        loss.backward()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        adversarial_data = adversarial_data + alpha * adversarial_data.grad.sign()
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –¥–æ–ø—É—Å—Ç–∏–º—É—é –æ–±–ª–∞—Å—Ç—å
        perturbation = torch.clamp(adversarial_data - data, -epsilon, epsilon)
        adversarial_data = data + perturbation
        adversarial_data = torch.clamp(adversarial_data, 0, 1)
        adversarial_data = adversarial_data.detach()
    
    return adversarial_data
```

### –ó–∞—â–∏—Ç–∞ –æ—Ç –∞—Ç–∞–∫
```python
def adversarial_training(model, train_loader, optimizer, epochs=10):
    """Adversarial –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏"""
    model.train()
    
    for epoch in range(epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            # –û–±—ã—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
            optimizer.zero_grad()
            output = model(data)
            clean_loss = F.cross_entropy(output, target)
            
            # Adversarial –ø—Ä–∏–º–µ—Ä—ã
            adversarial_data = fgsm_attack(model, data, target, epsilon=0.1)
            adv_output = model(adversarial_data)
            adv_loss = F.cross_entropy(adv_output, target)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
            total_loss = 0.5 * clean_loss + 0.5 * adv_loss
            total_loss.backward()
            optimizer.step()
```

---

## üîê –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö

### Differential Privacy
```python
import numpy as np

class DifferentialPrivacy:
    def __init__(self, epsilon=1.0):
        self.epsilon = epsilon
    
    def add_laplace_noise(self, value, sensitivity):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –õ–∞–ø–ª–∞—Å–∞"""
        scale = sensitivity / self.epsilon
        noise = np.random.laplace(0, scale)
        return value + noise
    
    def private_mean(self, data, sensitivity=1.0):
        """–ü—Ä–∏–≤–∞—Ç–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ"""
        true_mean = np.mean(data)
        return self.add_laplace_noise(true_mean, sensitivity / len(data))
    
    def private_histogram(self, data, bins):
        """–ü—Ä–∏–≤–∞—Ç–Ω–∞—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞"""
        hist, _ = np.histogram(data, bins)
        private_hist = [self.add_laplace_noise(count, 1.0) for count in hist]
        return np.maximum(private_hist, 0)  # –£–±–∏—Ä–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
dp = DifferentialPrivacy(epsilon=1.0)
private_avg = dp.private_mean(sensitive_data)
```

### Federated Learning
```python
class FederatedLearning:
    def __init__(self, global_model):
        self.global_model = global_model
        self.client_weights = []
    
    def client_update(self, client_data, client_labels, epochs=5):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–ª–∏–µ–Ω—Ç–µ"""
        local_model = copy.deepcopy(self.global_model)
        optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)
        
        for epoch in range(epochs):
            optimizer.zero_grad()
            output = local_model(client_data)
            loss = F.cross_entropy(output, client_labels)
            loss.backward()
            optimizer.step()
        
        return local_model.state_dict()
    
    def federated_averaging(self, client_weights):
        """–§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤"""
        global_weights = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        for key in client_weights[0].keys():
            global_weights[key] = torch.zeros_like(client_weights[0][key])
        
        # –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ
        for client_weight in client_weights:
            for key in global_weights.keys():
                global_weights[key] += client_weight[key]
        
        for key in global_weights.keys():
            global_weights[key] = global_weights[key] / len(client_weights)
        
        self.global_model.load_state_dict(global_weights)
```

---

## ‚ö†Ô∏è –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤

### –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∏—Å–∫–æ–≤ –ò–ò
```python
class AIRiskAssessment:
    def __init__(self):
        self.risk_categories = {
            'bias_discrimination': '–ü—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è',
            'privacy_breach': '–ù–∞—Ä—É—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏',
            'security_vulnerability': '–£—è–∑–≤–∏–º–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',
            'algorithmic_harm': '–ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π –≤—Ä–µ–¥',
            'human_autonomy': '–£–≥—Ä–æ–∑–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –∞–≤—Ç–æ–Ω–æ–º–∏–∏'
        }
    
    def assess_risk(self, model_type, use_case, data_sensitivity):
        """–û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ –ò–ò —Å–∏—Å—Ç–µ–º—ã"""
        risk_scores = {}
        
        # –§–∞–∫—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞
        high_risk_models = ['deep_learning', 'ensemble', 'neural_network']
        sensitive_use_cases = ['healthcare', 'finance', 'criminal_justice', 'hiring']
        
        for risk_type in self.risk_categories:
            base_risk = 0.3  # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞
            
            # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∏—Å–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–∞–∫—Ç–æ—Ä–æ–≤
            if model_type in high_risk_models:
                base_risk += 0.2
            
            if use_case in sensitive_use_cases:
                base_risk += 0.3
            
            if data_sensitivity == 'high':
                base_risk += 0.2
            
            risk_scores[risk_type] = min(base_risk, 1.0)
        
        return risk_scores
    
    def generate_mitigation_plan(self, risk_scores):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∏—Å–∫–æ–≤"""
        mitigation_strategies = []
        
        for risk_type, score in risk_scores.items():
            if score > 0.7:
                strategies = self.get_mitigation_strategies(risk_type)
                mitigation_strategies.extend(strategies)
        
        return mitigation_strategies
    
    def get_mitigation_strategies(self, risk_type):
        """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–Ω–∏–∂–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤"""
        strategies = {
            'bias_discrimination': [
                '–ê—É–¥–∏—Ç –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å',
                '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏',
                '–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ –∫–æ–º–∞–Ω–¥–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏'
            ],
            'privacy_breach': [
                '–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å',
                '–§–µ–¥–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ',
                '–ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö'
            ],
            'security_vulnerability': [
                'Adversarial training',
                '–†–µ–≥—É–ª—è—Ä–Ω—ã–µ security –∞—É–¥–∏—Ç—ã',
                '–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö'
            ]
        }
        
        return strategies.get(risk_type, [])
```

---

## üìã –ß–µ–∫-–ª–∏—Å—Ç —ç—Ç–∏—á–Ω–æ–≥–æ –ò–ò

```
‚úÖ –≠—Ç–∏—á–µ—Å–∫–∏–π —á–µ–∫-–ª–∏—Å—Ç:

üìä –î–∞–Ω–Ω—ã–µ:
‚îú‚îÄ‚îÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ –°–æ–≥–ª–∞—Å–∏–µ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
‚îî‚îÄ‚îÄ –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö

ü§ñ –ú–æ–¥–µ–ª—å:
‚îú‚îÄ‚îÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≥—Ä—É–ø–ø–∞—Ö
‚îú‚îÄ‚îÄ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥—Ä–∏—Ñ—Ç–∞
‚îî‚îÄ‚îÄ –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞—É–¥–∏—Ç

üöÄ –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ:
‚îú‚îÄ‚îÄ –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –Ω–∞–¥–∑–æ—Ä
‚îú‚îÄ‚îÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–º–µ–Ω—ã —Ä–µ—à–µ–Ω–∏–π
‚îú‚îÄ‚îÄ –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚îú‚îÄ‚îÄ –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –∏ –∂–∞–ª–æ–±—ã
‚îî‚îÄ‚îÄ –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏

üîí –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:
‚îú‚îÄ‚îÄ –ó–∞—â–∏—Ç–∞ –æ—Ç adversarial –∞—Ç–∞–∫
‚îú‚îÄ‚îÄ –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ –ö–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞
‚îú‚îÄ‚îÄ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ –ü–ª–∞–Ω —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã
```

---

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
- [[ai-in-production|AI –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ]] - –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
- [[large-language-models|LLM]] - —ç—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
- [[ai-fundamentals|–û—Å–Ω–æ–≤—ã AI]] - –ø—Ä–∏–Ω—Ü–∏–ø—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ò–ò

---

üí° **–°–æ–≤–µ—Ç:** –≠—Ç–∏–∫–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω—ã –∑–∞–∫–ª–∞–¥—ã–≤–∞—Ç—å—Å—è —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ò–ò —Å–∏—Å—Ç–µ–º—ã, –∞ –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è –≤ –∫–æ–Ω—Ü–µ! 
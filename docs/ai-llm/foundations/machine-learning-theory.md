# üìä –¢–µ–æ—Ä–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

## üßÆ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã

### –õ–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞
```
–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:
‚îú‚îÄ‚îÄ –í–µ–∫—Ç–æ—Ä—ã –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ –Ω–∏–º–∏
‚îú‚îÄ‚îÄ –ú–∞—Ç—Ä–∏—Ü—ã –∏ –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
‚îú‚îÄ‚îÄ –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–µ–∫—Ç–æ—Ä—ã
‚îú‚îÄ‚îÄ –°–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ (SVD)
‚îú‚îÄ‚îÄ –ù–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –º–∞—Ç—Ä–∏—Ü
‚îî‚îÄ‚îÄ –ü—Ä–æ–µ–∫—Ü–∏–∏ –∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è
```

#### –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
```python
import numpy as np

# –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
dot_product = np.dot(a, b)  # –∏–ª–∏ a @ b
print(f"Dot product: {dot_product}")

# –ù–æ—Ä–º–∞ –≤–µ–∫—Ç–æ—Ä–∞
norm_a = np.linalg.norm(a)
print(f"Norm of a: {norm_a}")

# –ö–æ—Å–∏–Ω—É—Å —É–≥–ª–∞ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏
cosine_similarity = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
print(f"Cosine similarity: {cosine_similarity}")
```

#### –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
```python
# –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
C = A @ B
print(f"Matrix multiplication:\n{C}")

# –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–µ–∫—Ç–æ—Ä—ã
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}")

# –°–∏–Ω–≥—É–ª—è—Ä–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ
U, s, Vt = np.linalg.svd(A)
print(f"SVD - U:\n{U}")
print(f"SVD - singular values: {s}")
print(f"SVD - Vt:\n{Vt}")
```

### –¢–µ–æ—Ä–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
```
–§—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è:
‚îú‚îÄ‚îÄ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
‚îú‚îÄ‚îÄ –°–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã
‚îú‚îÄ‚îÄ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
‚îú‚îÄ‚îÄ –£—Å–ª–æ–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
‚îú‚îÄ‚îÄ –¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞
‚îú‚îÄ‚îÄ –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–µ–ª—å–Ω–∞—è —Ç–µ–æ—Ä–µ–º–∞
‚îî‚îÄ‚îÄ –ú–∞—Ä–∫–æ–≤—Å–∫–∏–µ —Ü–µ–ø–∏
```

#### –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è
```python
import scipy.stats as stats

# –¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞: P(A|B) = P(B|A) * P(A) / P(B)
def bayes_theorem(prior, likelihood, evidence):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    """
    posterior = (likelihood * prior) / evidence
    return posterior

# –ü—Ä–∏–º–µ—Ä: –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
# P(–±–æ–ª–µ–∑–Ω—å) = 0.01 - –∞–ø—Ä–∏–æ—Ä–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
# P(—Ç–µ—Å—Ç+|–±–æ–ª–µ–∑–Ω—å) = 0.95 - —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç–∞
# P(—Ç–µ—Å—Ç+|–∑–¥–æ—Ä–æ–≤) = 0.05 - –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

prior_disease = 0.01
prior_healthy = 0.99
likelihood_positive_given_disease = 0.95
likelihood_positive_given_healthy = 0.05

# –ü–æ–ª–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
evidence = (likelihood_positive_given_disease * prior_disease + 
           likelihood_positive_given_healthy * prior_healthy)

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±–æ–ª–µ–∑–Ω–∏ –ø—Ä–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–º —Ç–µ—Å—Ç–µ
posterior = bayes_theorem(prior_disease, likelihood_positive_given_disease, evidence)
print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –±–æ–ª–µ–∑–Ω–∏ –ø—Ä–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–º —Ç–µ—Å—Ç–µ: {posterior:.3f}")
```

### –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥
```python
# –ú–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
from scipy import stats
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
np.random.seed(42)
true_mean = 5
true_std = 2
sample_size = 1000
data = np.random.normal(true_mean, true_std, sample_size)

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ (MLE)
mle_mean = np.mean(data)
mle_std = np.std(data, ddof=0)

# –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ (—Å –∞–ø—Ä–∏–æ—Ä–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º)
# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –∞–ø—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ
prior_mean = 0
prior_std = 10

# –ê–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ
posterior_precision = 1/prior_std**2 + sample_size/true_std**2
posterior_mean = (prior_mean/prior_std**2 + np.sum(data)/true_std**2) / posterior_precision
posterior_std = np.sqrt(1/posterior_precision)

print(f"–ò—Å—Ç–∏–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: Œº={true_mean}, œÉ={true_std}")
print(f"MLE –æ—Ü–µ–Ω–∫–∏: Œº={mle_mean:.3f}, œÉ={mle_std:.3f}")
print(f"–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞: Œº={posterior_mean:.3f}, œÉ={posterior_std:.3f}")
```

---

## üéØ –¢–∏–ø—ã –æ–±—É—á–µ–Ω–∏—è

### Supervised Learning (–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º)
```
–ó–∞–¥–∞—á–∏:
‚îú‚îÄ‚îÄ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
‚îú‚îÄ‚îÄ –†–µ–≥—Ä–µ—Å—Å–∏—è - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
‚îú‚îÄ‚îÄ –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ - —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤
‚îî‚îÄ‚îÄ Structured prediction - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä
```

#### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
X = np.linspace(0, 1, 100).reshape(-1, 1)
y = 1.5 * X.ravel() + 0.2 * np.random.normal(size=X.shape[0])

# –ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
lr = LinearRegression()
lr.fit(X, y)

# –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
poly_features = PolynomialFeatures(degree=3)
X_poly = poly_features.fit_transform(X)
poly_lr = LinearRegression()
poly_lr.fit(X_poly, y)

# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:
# y = w‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô
# –∏–ª–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π —Ñ–æ—Ä–º–µ: y = w^T x + b

print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã: {lr.coef_}")
print(f"Intercept: {lr.intercept_}")
print(f"R¬≤ score: {lr.score(X, y):.3f}")
```

#### –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                          n_informative=2, n_clusters_per_class=1, random_state=42)

# –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
log_reg = LogisticRegression()
log_reg.fit(X, y)

# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞:
# p(y=1|x) = 1 / (1 + exp(-(w^T x + b)))
# log-odds: log(p/(1-p)) = w^T x + b

def sigmoid(z):
    """–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    return 1 / (1 + np.exp(-z))

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
probabilities = log_reg.predict_proba(X)
print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {log_reg.score(X, y):.3f}")
```

### Unsupervised Learning (–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è)
```
–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏:
‚îú‚îÄ‚îÄ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
‚îú‚îÄ‚îÄ –ü–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ - —Å–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π - –ø–æ–∏—Å–∫ –≤—ã–±—Ä–æ—Å–æ–≤
‚îú‚îÄ‚îÄ –ê—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ - –ø–æ–∏—Å–∫ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π
‚îî‚îÄ‚îÄ –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
```

#### K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)

# K-means –∞–ª–≥–æ—Ä–∏—Ç–º
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X)
centroids = kmeans.cluster_centers_

# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º K-means:
# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å k —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ —Å–ª—É—á–∞–π–Ω–æ
# 2. –ü–æ–≤—Ç–æ—Ä—è—Ç—å –¥–æ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏:
#    a) –ù–∞–∑–Ω–∞—á–∏—Ç—å –∫–∞–∂–¥—É—é —Ç–æ—á–∫—É –±–ª–∏–∂–∞–π—à–µ–º—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É
#    b) –û–±–Ω–æ–≤–∏—Ç—å —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–µ–∫ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ

def kmeans_manual(X, k, max_iters=100):
    """–†—É—á–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è K-means"""
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]
    
    for _ in range(max_iters):
        # –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))
        labels = np.argmin(distances, axis=0)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤
        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    
    return labels, centroids

manual_labels, manual_centroids = kmeans_manual(X, 4)
```

#### Principal Component Analysis (PCA)
```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
iris = load_iris()
X = iris.data
y = iris.target

# PCA
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞ PCA:
# 1. –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: X_centered = X - mean(X)
# 2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã: C = (X_centered^T √ó X_centered) / (n-1)
# 3. –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –∏ –∑–Ω–∞—á–µ–Ω–∏—è: C √ó v = Œª √ó v
# 4. –ü—Ä–æ–µ–∫—Ü–∏—è: X_new = X_centered √ó V

print(f"–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è: {pca.explained_variance_ratio_}")
print(f"–°—É–º–º–∞—Ä–Ω–∞—è –æ–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è: {pca.explained_variance_ratio_.sum():.3f}")

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–≥–ª–∞–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
print(f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã PCA:\n{pca.components_}")
```

### Reinforcement Learning (–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º)
```
–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:
‚îú‚îÄ‚îÄ –ê–≥–µ–Ω—Ç, —Å—Ä–µ–¥–∞, –¥–µ–π—Å—Ç–≤–∏—è, —Å–æ—Å—Ç–æ—è–Ω–∏—è
‚îú‚îÄ‚îÄ –§—É–Ω–∫—Ü–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
‚îú‚îÄ‚îÄ –ü–æ–ª–∏—Ç–∏–∫–∞ (policy)
‚îú‚îÄ‚îÄ –§—É–Ω–∫—Ü–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ (value function)
‚îú‚îÄ‚îÄ Q-learning
‚îî‚îÄ‚îÄ Policy gradient methods
```

#### Q-Learning –∞–ª–≥–æ—Ä–∏—Ç–º
```python
import gym
import numpy as np

class QLearningAgent:
    def __init__(self, state_size, action_size, learning_rate=0.1, 
                 discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995):
        self.state_size = state_size
        self.action_size = action_size
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.q_table = np.zeros((state_size, action_size))
    
    def choose_action(self, state):
        """Epsilon-greedy policy"""
        if np.random.random() < self.epsilon:
            return np.random.choice(self.action_size)
        return np.argmax(self.q_table[state])
    
    def learn(self, state, action, reward, next_state, done):
        """Q-learning update rule"""
        if done:
            target = reward
        else:
            target = reward + self.gamma * np.max(self.q_table[next_state])
        
        # Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
        self.q_table[state, action] += self.lr * (target - self.q_table[state, action])
        
        if self.epsilon > 0.01:
            self.epsilon *= self.epsilon_decay

# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞ Q-learning:
# Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
# –≥–¥–µ:
# - Œ± - learning rate
# - Œ≥ - discount factor
# - r - reward
# - s,a - —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –¥–µ–π—Å—Ç–≤–∏–µ
# - s' - —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
```

---

## üìà –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
```python
import torch
import torch.nn as nn

# –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å

# 1. Mean Squared Error (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred)**2)

# 2. Cross-entropy (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
def cross_entropy_loss(y_true, y_pred):
    # y_true - one-hot encoded, y_pred - probabilities
    return -np.sum(y_true * np.log(y_pred + 1e-15))

# 3. Binary Cross-entropy
def binary_cross_entropy(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-15) + 
                   (1 - y_true) * np.log(1 - y_pred + 1e-15))

# 4. Hinge Loss (–¥–ª—è SVM)
def hinge_loss(y_true, y_pred):
    return np.maximum(0, 1 - y_true * y_pred)

# PyTorch implementations
mse = nn.MSELoss()
ce = nn.CrossEntropyLoss()
bce = nn.BCELoss()
```

### –ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
```python
# –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏ –µ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç—ã

class SGD:
    def __init__(self, learning_rate=0.01):
        self.lr = learning_rate
    
    def update(self, params, gradients):
        """–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫"""
        for param, grad in zip(params, gradients):
            param -= self.lr * grad

class Momentum:
    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocities = None
    
    def update(self, params, gradients):
        """SGD —Å –º–æ–º–µ–Ω—Ç–æ–º"""
        if self.velocities is None:
            self.velocities = [np.zeros_like(p) for p in params]
        
        for param, grad, velocity in zip(params, gradients, self.velocities):
            velocity[:] = self.momentum * velocity - self.lr * grad
            param += velocity

class Adam:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None  # First moment
        self.v = None  # Second moment
        self.t = 0     # Time step
    
    def update(self, params, gradients):
        """Adam optimizer"""
        if self.m is None:
            self.m = [np.zeros_like(p) for p in params]
            self.v = [np.zeros_like(p) for p in params]
        
        self.t += 1
        
        for param, grad, m, v in zip(params, gradients, self.m, self.v):
            # Update biased first moment estimate
            m[:] = self.beta1 * m + (1 - self.beta1) * grad
            
            # Update biased second raw moment estimate
            v[:] = self.beta2 * v + (1 - self.beta2) * grad**2
            
            # Compute bias-corrected moments
            m_hat = m / (1 - self.beta1**self.t)
            v_hat = v / (1 - self.beta2**self.t)
            
            # Update parameters
            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
```

---

## üîÑ –û–±–æ–±—â–µ–Ω–∏–µ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

### Bias-Variance Trade-off
```
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏:
‚îú‚îÄ‚îÄ Bias (—Å–º–µ—â–µ–Ω–∏–µ) - —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞
‚îú‚îÄ‚îÄ Variance (–¥–∏—Å–ø–µ—Ä—Å–∏—è) - —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
‚îú‚îÄ‚îÄ Irreducible error - –Ω–µ—É—Å—Ç—Ä–∞–Ω–∏–º—ã–π —à—É–º
‚îî‚îÄ‚îÄ Total error = Bias¬≤ + Variance + Noise
```

```python
# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è bias-variance trade-off
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

def bias_variance_decomposition(X_train, y_train, X_test, y_test, n_trials=100):
    """
    –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–∞ bias –∏ variance
    """
    predictions = []
    
    for _ in range(n_trials):
        # –°–æ–∑–¥–∞–Ω–∏–µ bootstrap –≤—ã–±–æ—Ä–∫–∏
        indices = np.random.choice(len(X_train), len(X_train), replace=True)
        X_boot = X_train[indices]
        y_boot = y_train[indices]
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = DecisionTreeRegressor(max_depth=10)
        model.fit(X_boot, y_boot)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        y_pred = model.predict(X_test)
        predictions.append(y_pred)
    
    predictions = np.array(predictions)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ bias –∏ variance
    mean_prediction = np.mean(predictions, axis=0)
    bias_squared = np.mean((mean_prediction - y_test)**2)
    variance = np.mean(np.var(predictions, axis=0))
    
    return bias_squared, variance

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
X = np.random.normal(0, 1, (1000, 5))
y = X[:, 0] + 0.5 * X[:, 1] + np.random.normal(0, 0.1, 1000)

X_train, X_test = X[:800], X[800:]
y_train, y_test = y[:800], y[800:]

bias_sq, variance = bias_variance_decomposition(X_train, y_train, X_test, y_test)
print(f"Bias¬≤: {bias_sq:.4f}")
print(f"Variance: {variance:.4f}")
```

### –¢–µ—Ö–Ω–∏–∫–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
```python
# L1 –∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Ridge regression (L2 regularization)
# Loss = MSE + Œ± * Œ£(w¬≤)
ridge = Ridge(alpha=1.0)

# Lasso regression (L1 regularization)
# Loss = MSE + Œ± * Œ£|w|
lasso = Lasso(alpha=1.0)

# Elastic Net (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è L1 –∏ L2)
# Loss = MSE + Œ±‚ÇÅ * Œ£|w| + Œ±‚ÇÇ * Œ£(w¬≤)
elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)

# Dropout (–¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π)
class Dropout:
    def __init__(self, drop_rate=0.5):
        self.drop_rate = drop_rate
        self.mask = None
    
    def forward(self, x, training=True):
        if training:
            self.mask = np.random.binomial(1, 1-self.drop_rate, x.shape) / (1-self.drop_rate)
            return x * self.mask
        return x
    
    def backward(self, grad_output):
        return grad_output * self.mask

# Batch Normalization
class BatchNorm:
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # Learnable parameters
        self.gamma = np.ones(num_features)
        self.beta = np.zeros(num_features)
        
        # Running statistics
        self.running_mean = np.zeros(num_features)
        self.running_var = np.ones(num_features)
    
    def forward(self, x, training=True):
        if training:
            batch_mean = np.mean(x, axis=0)
            batch_var = np.var(x, axis=0)
            
            # Update running statistics
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            
            # Normalize
            x_norm = (x - batch_mean) / np.sqrt(batch_var + self.eps)
        else:
            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)
        
        # Scale and shift
        return self.gamma * x_norm + self.beta
```

---

## üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π

### –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```python
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix)

def evaluate_classification(y_true, y_pred, y_prob=None):
    """–ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    
    print(f"Accuracy: {accuracy:.3f}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-score: {f1:.3f}")
    
    # AUC-ROC (–µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
    if y_prob is not None:
        auc = roc_auc_score(y_true, y_prob)
        print(f"AUC-ROC: {auc:.3f}")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    cm = confusion_matrix(y_true, y_pred)
    print(f"Confusion Matrix:\n{cm}")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm
    }

# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã:
# Precision = TP / (TP + FP)
# Recall = TP / (TP + FN)
# F1-score = 2 * (Precision * Recall) / (Precision + Recall)
# Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

### –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def evaluate_regression(y_true, y_pred):
    """–ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    print(f"MSE: {mse:.3f}")
    print(f"RMSE: {rmse:.3f}")
    print(f"MAE: {mae:.3f}")
    print(f"R¬≤: {r2:.3f}")
    print(f"MAPE: {mape:.2f}%")
    
    return {
        'mse': mse,
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'mape': mape
    }

# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª—ã:
# MSE = (1/n) * Œ£(y_true - y_pred)¬≤
# RMSE = ‚àöMSE
# MAE = (1/n) * Œ£|y_true - y_pred|
# R¬≤ = 1 - SS_res / SS_tot
# MAPE = (100/n) * Œ£|y_true - y_pred|/|y_true|
```

### Cross-Validation
```python
from sklearn.model_selection import cross_val_score, StratifiedKFold, TimeSeriesSplit

def comprehensive_cross_validation(model, X, y, cv_type='kfold', n_splits=5):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è"""
    
    if cv_type == 'stratified':
        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    elif cv_type == 'timeseries':
        cv = TimeSeriesSplit(n_splits=n_splits)
    else:  # standard k-fold
        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    # –†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']
    
    results = {}
    for metric in scoring:
        scores = cross_val_score(model, X, y, cv=cv, scoring=metric)
        results[metric] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std()
        }
        print(f"{metric}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
    
    return results
```

---

## üéØ –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã

### Ensemble Methods
```python
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# Bagging
bagging = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=100,
    random_state=42
)

# Random Forest (specialized bagging)
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Voting Classifier
voting_clf = VotingClassifier(
    estimators=[
        ('lr', LogisticRegression()),
        ('svm', SVC(probability=True)),
        ('rf', RandomForestClassifier())
    ],
    voting='soft'  # –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
)

# AdaBoost (Boosting)
from sklearn.ensemble import AdaBoostClassifier
ada_boost = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=100,
    learning_rate=1.0
)

# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
gb_clf = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3
)
```

### Feature Engineering
```python
from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler,
                                 PolynomialFeatures, OneHotEncoder)
from sklearn.feature_selection import SelectKBest, f_classif, RFE

class FeatureEngineer:
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
        self.selectors = {}
    
    def scale_features(self, X, method='standard'):
        """–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if method == 'standard':
            scaler = StandardScaler()
        elif method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'robust':
            scaler = RobustScaler()
        
        X_scaled = scaler.fit_transform(X)
        self.scalers[method] = scaler
        return X_scaled
    
    def create_polynomial_features(self, X, degree=2):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        poly = PolynomialFeatures(degree=degree, include_bias=False)
        X_poly = poly.fit_transform(X)
        self.poly_transformer = poly
        return X_poly
    
    def encode_categorical(self, X_cat):
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
        X_encoded = encoder.fit_transform(X_cat)
        self.categorical_encoder = encoder
        return X_encoded
    
    def select_features(self, X, y, method='univariate', k=10):
        """–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if method == 'univariate':
            selector = SelectKBest(score_func=f_classif, k=k)
        elif method == 'rfe':
            estimator = RandomForestClassifier()
            selector = RFE(estimator, n_features_to_select=k)
        
        X_selected = selector.fit_transform(X, y)
        self.feature_selector = selector
        return X_selected

# –ü—Ä–∏–º–µ—Ä –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
def preprocess_pipeline(X_numeric, X_categorical, y):
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    fe = FeatureEngineer()
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_num_scaled = fe.scale_features(X_numeric, 'standard')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_num_poly = fe.create_polynomial_features(X_num_scaled, degree=2)
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_cat_encoded = fe.encode_categorical(X_categorical)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_combined = np.hstack([X_num_poly, X_cat_encoded])
    
    # –û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_final = fe.select_features(X_combined, y, method='univariate', k=20)
    
    return X_final, fe
```

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
- [[deep-learning-foundations|–û—Å–Ω–æ–≤—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è]] - –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- [[neural-networks-architecture|–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π]] - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- [[ai-fundamentals|–û—Å–Ω–æ–≤—ã AI]] - –æ–±—â–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞
- [[../mathematics/programming-math|–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤]] - –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- **Scikit-learn** - –æ—Å–Ω–æ–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ ML –¥–ª—è Python
- **Pandas** - –∞–Ω–∞–ª–∏–∑ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
- **NumPy** - —á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
- **Matplotlib/Seaborn** - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- **Jupyter Notebooks** - –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞

---

üí° **–°–æ–≤–µ—Ç:** –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç solid –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –ù–∞—á–Ω–∏—Ç–µ —Å –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏, –∑–∞—Ç–µ–º –∏–∑—É—á–∞–π—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –í—Å–µ–≥–¥–∞ –ø–æ–º–Ω–∏—Ç–µ –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π! 
# üß† –£–≥–ª—É–±–ª–µ–Ω–Ω—ã–µ –æ—Å–Ω–æ–≤—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

## üî¨ –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

### –¢–µ–æ—Ä–∏—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏
```
–¢–µ–æ—Ä–µ–º–∞ –æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏:
‚îú‚îÄ‚îÄ –°–µ—Ç–∏ —Å –æ–¥–Ω–∏–º —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º –º–æ–≥—É—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –ª—é–±—É—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
‚îú‚îÄ‚îÄ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤
‚îú‚îÄ‚îÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
‚îî‚îÄ‚îÄ –ì–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏: –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
```

#### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
```python
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

class UniversalApproximator(nn.Module):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏"""
    def __init__(self, hidden_size=100):
        super().__init__()
        self.hidden = nn.Linear(1, hidden_size)
        self.output = nn.Linear(hidden_size, 1)
        self.activation = nn.ReLU()
    
    def forward(self, x):
        hidden = self.activation(self.hidden(x))
        return self.output(hidden)

# –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏
def target_function(x):
    return np.sin(2 * np.pi * x) * np.exp(-x)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
x_train = np.linspace(0, 2, 1000).reshape(-1, 1)
y_train = target_function(x_train.flatten())

# –û–±—É—á–µ–Ω–∏–µ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ç–æ—Ä–∞
model = UniversalApproximator(hidden_size=50)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

X_tensor = torch.FloatTensor(x_train)
y_tensor = torch.FloatTensor(y_train.reshape(-1, 1))

# –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
for epoch in range(1000):
    optimizer.zero_grad()
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    loss.backward()
    optimizer.step()
    
    if epoch % 200 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.6f}')
```

### –¢–µ–æ—Ä–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏
```
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
‚îú‚îÄ‚îÄ –≠–Ω—Ç—Ä–æ–ø–∏—è –∏ –≤–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
‚îú‚îÄ‚îÄ Information Bottleneck –ø—Ä–∏–Ω—Ü–∏–ø
‚îú‚îÄ‚îÄ –°–∂–∞—Ç–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
‚îú‚îÄ‚îÄ –ö–∞—Å–∫–∞–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
‚îî‚îÄ‚îÄ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Critical Learning)
```

#### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —É–∑–∫–∏–π –ø—Ä–æ—Ö–æ–¥
```python
class InformationBottleneck:
    """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Information Bottleneck –ø—Ä–∏–Ω—Ü–∏–ø–∞"""
    
    def __init__(self, beta=1.0):
        self.beta = beta  # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∂–∞—Ç–∏–µ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
    
    def mutual_information(self, X, Y):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ I(X;Y)"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é
        joint_entropy = self.entropy(torch.cat([X, Y], dim=1))
        marginal_x = self.entropy(X)
        marginal_y = self.entropy(Y)
        return marginal_x + marginal_y - joint_entropy
    
    def entropy(self, X):
        """–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è"""
        # –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ –≥–∞—É—Å—Å–æ–≤—Å–∫—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é
        cov_matrix = torch.cov(X.T)
        det = torch.det(cov_matrix + 1e-8 * torch.eye(X.shape[1]))
        return 0.5 * torch.log(det * (2 * np.pi * np.e))
    
    def ib_loss(self, X, T, Y):
        """
        Information Bottleneck Loss
        L = -I(T;Y) + Œ≤*I(X;T)
        –≥–¥–µ T - –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        """
        compression_term = self.mutual_information(X, T)
        prediction_term = self.mutual_information(T, Y)
        return -prediction_term + self.beta * compression_term

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
class IBNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, beta=1.0):
        super().__init__()
        self.encoder = nn.Linear(input_dim, hidden_dim)
        self.decoder = nn.Linear(hidden_dim, output_dim)
        self.ib = InformationBottleneck(beta)
        
    def forward(self, x):
        t = torch.relu(self.encoder(x))  # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        y = self.decoder(t)
        return y, t
    
    def compute_ib_loss(self, x, y_true):
        y_pred, t = self.forward(x)
        pred_loss = nn.MSELoss()(y_pred, y_true)
        ib_loss = self.ib.ib_loss(x, t, y_true)
        return pred_loss + 0.1 * ib_loss
```

---

## üèóÔ∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### Attention –º–µ—Ö–∞–Ω–∏–∑–º—ã –∏ –∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç—ã
```
–≠–≤–æ–ª—é—Ü–∏—è Attention:
‚îú‚îÄ‚îÄ Additive Attention (Bahdanau, 2014)
‚îú‚îÄ‚îÄ Multiplicative Attention (Luong, 2015)
‚îú‚îÄ‚îÄ Self-Attention (Transformer, 2017)
‚îú‚îÄ‚îÄ Multi-Head Attention
‚îú‚îÄ‚îÄ Sparse Attention (Local, Strided)
‚îú‚îÄ‚îÄ Linear Attention
‚îî‚îÄ‚îÄ Cross-Attention
```

#### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ Attention
```python
import math

class MultiHeadAttention(nn.Module):
    """–ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏"""
    
    def __init__(self, d_model, n_heads, attention_type='scaled_dot_product'):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.attention_type = attention_type
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        if attention_type == 'additive':
            self.W_a = nn.Linear(2 * self.d_k, self.d_k)
            self.v_a = nn.Linear(self.d_k, 1)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–µ scaled dot-product attention"""
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attn_weights, V)
        return context, attn_weights
    
    def additive_attention(self, Q, K, V, mask=None):
        """Additive (Bahdanau) attention"""
        seq_len_q, seq_len_k = Q.size(-2), K.size(-2)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è broadcast
        Q_expanded = Q.unsqueeze(-2).expand(-1, -1, -1, seq_len_k, -1)
        K_expanded = K.unsqueeze(-3).expand(-1, -1, seq_len_q, -1, -1)
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –∏ –ø—Ä–æ–µ–∫—Ü–∏—è
        combined = torch.cat([Q_expanded, K_expanded], dim=-1)
        energy = torch.tanh(self.W_a(combined))
        scores = self.v_a(energy).squeeze(-1)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attn_weights, V)
        return context, attn_weights
    
    def sparse_attention(self, Q, K, V, window_size=4):
        """–õ–æ–∫–∞–ª—å–Ω–æ–µ sparse attention"""
        seq_len = Q.size(-2)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è
        mask = torch.zeros(seq_len, seq_len)
        for i in range(seq_len):
            start = max(0, i - window_size // 2)
            end = min(seq_len, i + window_size // 2 + 1)
            mask[i, start:end] = 1
        
        return self.scaled_dot_product_attention(Q, K, V, mask)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏
        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # –í—ã–±–æ—Ä —Ç–∏–ø–∞ attention
        if self.attention_type == 'scaled_dot_product':
            attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        elif self.attention_type == 'additive':
            attn_output, attn_weights = self.additive_attention(Q, K, V, mask)
        elif self.attention_type == 'sparse':
            attn_output, attn_weights = self.sparse_attention(Q, K, V)
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è –≥–æ–ª–æ–≤
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        return self.W_o(attn_output), attn_weights
```

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–∏–ø—ã —Å–ª–æ–µ–≤
```python
class GatedLinearUnit(nn.Module):
    """Gated Linear Unit (GLU) - —É–ª—É—á—à–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è"""
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, 2 * output_dim)
        
    def forward(self, x):
        gated = self.linear(x)
        values, gates = gated.chunk(2, dim=-1)
        return values * torch.sigmoid(gates)

class SwiGLU(nn.Module):
    """SwiGLU activation (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ PaLM, LLaMA)"""
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.w1 = nn.Linear(input_dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, input_dim, bias=False)
        self.w3 = nn.Linear(input_dim, hidden_dim, bias=False)
        
    def forward(self, x):
        return self.w2(torch.silu(self.w1(x)) * self.w3(x))

class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
        
    def forward(self, x):
        rms = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.eps)
        return x / rms * self.weight

class ReZero(nn.Module):
    """ReZero: —É–ª—É—á—à–µ–Ω–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π"""
    def __init__(self, layer):
        super().__init__()
        self.layer = layer
        self.alpha = nn.Parameter(torch.zeros(1))
        
    def forward(self, x):
        return x + self.alpha * self.layer(x)
```

---

## üéØ –ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
```python
class AdamW(torch.optim.Optimizer):
    """AdamW —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π weight decay —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)
    
    def step(self, closure=None):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                
                # Weight decay (–Ω–µ –Ω–∞ momentum!)
                p.data.mul_(1 - group['lr'] * group['weight_decay'])
                
                grad = p.grad.data
                state = self.state[p]
                
                if len(state) == 0:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p.data)
                    state['exp_avg_sq'] = torch.zeros_like(p.data)
                
                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                beta1, beta2 = group['betas']
                state['step'] += 1
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–æ–≤
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                # Bias correction
                bias_correction1 = 1 - beta1 ** state['step']
                bias_correction2 = 1 - beta2 ** state['step']
                
                step_size = group['lr'] / bias_correction1
                bias_correction2_sqrt = math.sqrt(bias_correction2)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                p.data.addcdiv_(exp_avg, exp_avg_sq.sqrt() / bias_correction2_sqrt + group['eps'], 
                              value=-step_size)

class LAMB(torch.optim.Optimizer):
    """LAMB optimizer –¥–ª—è –±–æ–ª—å—à–∏—Ö batch sizes"""
    
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)
    
    def step(self, closure=None):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue
                
                grad = p.grad.data
                state = self.state[p]
                
                if len(state) == 0:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p.data)
                    state['exp_avg_sq'] = torch.zeros_like(p.data)
                
                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                beta1, beta2 = group['betas']
                state['step'] += 1
                
                # Momentum –∏ RMSprop
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                
                # Bias correction
                bias_correction1 = 1 - beta1 ** state['step']
                bias_correction2 = 1 - beta2 ** state['step']
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Adam
                adam_step = exp_avg / bias_correction1 / (exp_avg_sq / bias_correction2).sqrt().add_(group['eps'])
                
                # Weight decay
                adam_step.add_(p.data, alpha=group['weight_decay'])
                
                # Layer-wise –∞–¥–∞–ø—Ç–∞—Ü–∏—è
                adam_norm = adam_step.norm()
                weight_norm = p.data.norm()
                
                if weight_norm > 0 and adam_norm > 0:
                    trust_ratio = weight_norm / adam_norm
                    trust_ratio = min(trust_ratio, 10.0)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
                else:
                    trust_ratio = 1.0
                
                # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
                p.data.add_(adam_step, alpha=-group['lr'] * trust_ratio)
```

### –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
```python
class CosineAnnealingWarmup(torch.optim.lr_scheduler._LRScheduler):
    """–ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –æ—Ç–∂–∏–≥ —Å warm-up –ø–µ—Ä–∏–æ–¥–æ–º"""
    
    def __init__(self, optimizer, warmup_epochs, max_epochs, eta_min=0):
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.eta_min = eta_min
        super().__init__(optimizer)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # –õ–∏–Ω–µ–π–Ω—ã–π warm-up
            return [base_lr * self.last_epoch / self.warmup_epochs for base_lr in self.base_lrs]
        else:
            # –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –æ—Ç–∂–∏–≥
            progress = (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)
            return [self.eta_min + (base_lr - self.eta_min) * 
                   (1 + math.cos(math.pi * progress)) / 2 for base_lr in self.base_lrs]

class OneCycleLR(torch.optim.lr_scheduler._LRScheduler):
    """One Cycle Learning Rate Policy"""
    
    def __init__(self, optimizer, max_lr, total_steps, pct_start=0.3, anneal_strategy='cos'):
        self.max_lr = max_lr if isinstance(max_lr, list) else [max_lr] * len(optimizer.param_groups)
        self.total_steps = total_steps
        self.pct_start = pct_start
        self.anneal_strategy = anneal_strategy
        super().__init__(optimizer)
    
    def get_lr(self):
        lrs = []
        step_num = self.last_epoch
        
        for base_lr, max_lr in zip(self.base_lrs, self.max_lr):
            if step_num <= self.pct_start * self.total_steps:
                # –§–∞–∑–∞ —Ä–æ—Å—Ç–∞
                pct = step_num / (self.pct_start * self.total_steps)
                lr = base_lr + (max_lr - base_lr) * pct
            else:
                # –§–∞–∑–∞ —Å–ø–∞–¥–∞
                pct = (step_num - self.pct_start * self.total_steps) / ((1 - self.pct_start) * self.total_steps)
                if self.anneal_strategy == 'cos':
                    lr = base_lr + (max_lr - base_lr) * (1 + math.cos(math.pi * pct)) / 2
                else:  # linear
                    lr = max_lr - (max_lr - base_lr) * pct
            
            lrs.append(lr)
        
        return lrs
```

---

## üõ°Ô∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è

### Stochastic Depth –∏ DropPath
```python
class StochasticDepth(nn.Module):
    """Stochastic Depth –¥–ª—è ResNet –∏ DenseNet"""
    
    def __init__(self, p=0.5, mode='row'):
        super().__init__()
        self.p = p
        self.mode = mode
    
    def forward(self, x, residual):
        if not self.training:
            return x + residual
        
        batch_size = x.size(0)
        
        if self.mode == 'row':
            # –°–ª—É—á–∞–π–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ü–µ–ª—ã–µ –æ–±—Ä–∞–∑—Ü—ã
            mask = torch.rand(batch_size, 1, 1, 1, device=x.device) > self.p
        else:
            # Bernoulli –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            mask = torch.rand_like(x) > self.p
        
        return x + residual * mask.float() / (1 - self.p)

class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) –¥–ª—è Vision Transformers"""
    
    def __init__(self, drop_prob=0.0):
        super().__init__()
        self.drop_prob = drop_prob
        
    def forward(self, x):
        if not self.training or self.drop_prob == 0.0:
            return x
        
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        
        return x.div(keep_prob) * random_tensor

class SpectrualNormalization(nn.Module):
    """Spectral Normalization –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è GANs"""
    
    def __init__(self, layer, n_power_iterations=1):
        super().__init__()
        self.layer = layer
        self.n_power_iterations = n_power_iterations
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è u –∏ v –≤–µ–∫—Ç–æ—Ä–æ–≤
        weight = layer.weight
        height = weight.size(0)
        width = weight.view(height, -1).size(1)
        
        self.register_buffer('u', torch.randn(height))
        self.register_buffer('v', torch.randn(width))
        
        self.u.data = F.normalize(self.u.data)
        self.v.data = F.normalize(self.v.data)
    
    def power_iteration(self, weight):
        """Power iteration –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–π –Ω–æ—Ä–º—ã"""
        weight_mat = weight.view(weight.size(0), -1)
        
        for _ in range(self.n_power_iterations):
            # v = normalize(W^T * u)
            self.v.data = F.normalize(torch.mv(weight_mat.t(), self.u.data))
            # u = normalize(W * v)
            self.u.data = F.normalize(torch.mv(weight_mat, self.v.data))
        
        # œÉ = u^T * W * v
        sigma = torch.dot(self.u.data, torch.mv(weight_mat, self.v.data))
        return sigma
    
    def forward(self, *args, **kwargs):
        if self.training:
            sigma = self.power_iteration(self.layer.weight)
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
            self.layer.weight.data = self.layer.weight.data / sigma
        
        return self.layer(*args, **kwargs)
```

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
```python
class LayerScale(nn.Module):
    """Layer Scale –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö Transformers"""
    
    def __init__(self, dim, init_values=1e-5):
        super().__init__()
        self.gamma = nn.Parameter(init_values * torch.ones(dim))
        
    def forward(self, x):
        return self.gamma * x

class GEGLU(nn.Module):
    """GEGLU activation –∏–∑ GLU variants improve Transformer"""
    
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim * 2)
        
    def forward(self, x):
        x_proj = self.proj(x)
        x1, x2 = x_proj.chunk(2, dim=-1)
        return x1 * F.gelu(x2)

class AdaptiveLayerNorm(nn.Module):
    """Adaptive Layer Normalization —Å –æ–±—É—á–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    
    def __init__(self, num_features, condition_dim):
        super().__init__()
        self.num_features = num_features
        self.ln = nn.LayerNorm(num_features, affine=False)
        
        # –£—Å–ª–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.scale_net = nn.Linear(condition_dim, num_features)
        self.shift_net = nn.Linear(condition_dim, num_features)
        
    def forward(self, x, condition):
        normalized = self.ln(x)
        
        scale = self.scale_net(condition)
        shift = self.shift_net(condition)
        
        return normalized * (1 + scale) + shift
```

---

## üî¨ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è

### Curriculum Learning –∏ Self-Paced Learning
```python
class CurriculumScheduler:
    """–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è Curriculum Learning"""
    
    def __init__(self, dataset, difficulty_fn, initial_pace=0.1):
        self.dataset = dataset
        self.difficulty_fn = difficulty_fn
        self.pace = initial_pace
        self.current_subset_size = int(len(dataset) * initial_pace)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        difficulties = [difficulty_fn(sample) for sample in dataset]
        self.sorted_indices = sorted(range(len(dataset)), key=lambda i: difficulties[i])
    
    def get_current_subset(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–µ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        current_indices = self.sorted_indices[:self.current_subset_size]
        return torch.utils.data.Subset(self.dataset, current_indices)
    
    def update_pace(self, epoch, total_epochs):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–º–ø–∞ –æ–±—É—á–µ–Ω–∏—è"""
        # –õ–∏–Ω–µ–π–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        progress = epoch / total_epochs
        self.pace = 0.1 + 0.9 * progress
        self.current_subset_size = min(len(self.dataset), 
                                     int(len(self.dataset) * self.pace))

class SelfPacedLearning:
    """Self-Paced Learning implementation"""
    
    def __init__(self, initial_lambda=1.0, lambda_increase_rate=1.1):
        self.lam = initial_lambda
        self.lambda_increase_rate = lambda_increase_rate
        
    def compute_weights(self, losses):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è –æ–±—Ä–∞–∑—Ü–æ–≤"""
        weights = torch.zeros_like(losses)
        weights[losses < self.lam] = 1.0
        return weights
    
    def spl_loss(self, losses, weights):
        """Self-paced learning loss"""
        return torch.sum(weights * losses) - self.lam * torch.sum(weights)
    
    def update_lambda(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ Œª"""
        self.lam *= self.lambda_increase_rate

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º —Ü–∏–∫–ª–µ
def train_with_curriculum(model, dataset, curriculum_scheduler, epochs=100):
    optimizer = torch.optim.Adam(model.parameters())
    
    for epoch in range(epochs):
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        current_dataset = curriculum_scheduler.get_current_subset()
        dataloader = torch.utils.data.DataLoader(current_dataset, batch_size=32)
        
        for batch in dataloader:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—É—â–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ
            optimizer.zero_grad()
            loss = model.compute_loss(batch)
            loss.backward()
            optimizer.step()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–º–ø–∞
        curriculum_scheduler.update_pace(epoch, epochs)
```

### Meta-Learning –∏ MAML
```python
class MAML(nn.Module):
    """Model-Agnostic Meta-Learning"""
    
    def __init__(self, model, inner_lr=0.01, meta_lr=0.001):
        super().__init__()
        self.model = model
        self.inner_lr = inner_lr
        self.meta_optimizer = torch.optim.Adam(self.model.parameters(), lr=meta_lr)
        
    def inner_update(self, task_data, task_labels):
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏"""
        # –ö–æ–ø–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        fast_weights = [p.clone() for p in self.model.parameters()]
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —à–∞–≥ –¥–ª—è –∑–∞–¥–∞—á–∏
        outputs = self.model(task_data)
        loss = F.cross_entropy(outputs, task_labels)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        grads = torch.autograd.grad(loss, self.model.parameters(), create_graph=True)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞
        fast_weights = [w - self.inner_lr * g for w, g in zip(fast_weights, grads)]
        
        return fast_weights
    
    def meta_update(self, meta_batch):
        """–ú–µ—Ç–∞-–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        meta_loss = 0.0
        
        for task in meta_batch:
            support_x, support_y, query_x, query_y = task
            
            # –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
            fast_weights = self.inner_update(support_x, support_y)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –Ω–∞ query set —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
            query_outputs = self.forward_with_params(query_x, fast_weights)
            meta_loss += F.cross_entropy(query_outputs, query_y)
        
        # –ú–µ—Ç–∞-–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —à–∞–≥
        meta_loss /= len(meta_batch)
        self.meta_optimizer.zero_grad()
        meta_loss.backward()
        self.meta_optimizer.step()
        
        return meta_loss.item()
    
    def forward_with_params(self, x, params):
        """Forward pass —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
        # –ü—Ä–∏–º–µ—Ä –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π —Å–µ—Ç–∏
        for i, (layer, param) in enumerate(zip(self.model.modules(), params)):
            if isinstance(layer, nn.Linear):
                x = F.linear(x, param)
                if i < len(params) - 1:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
                    x = F.relu(x)
        return x

class PrototypicalNetwork(nn.Module):
    """Prototypical Networks –¥–ª—è few-shot learning"""
    
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder
        
    def compute_prototypes(self, support_set, support_labels, n_ways):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞"""
        embeddings = self.encoder(support_set)
        prototypes = []
        
        for class_idx in range(n_ways):
            class_mask = (support_labels == class_idx)
            class_embeddings = embeddings[class_mask]
            prototype = class_embeddings.mean(dim=0)
            prototypes.append(prototype)
        
        return torch.stack(prototypes)
    
    def forward(self, support_set, support_labels, query_set, n_ways):
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤
        prototypes = self.compute_prototypes(support_set, support_labels, n_ways)
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è query set
        query_embeddings = self.encoder(query_set)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤
        distances = torch.cdist(query_embeddings, prototypes)
        
        # –õ–æ–≥–∏—Ç—ã –∫–∞–∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        logits = -distances
        
        return logits
```

---

## üìä –ê–Ω–∞–ª–∏–∑ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

### Gradient-based –º–µ—Ç–æ–¥—ã –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
```python
class GradCAM:
    """Gradient-weighted Class Activation Mapping"""
    
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è hooks
        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_backward_hook(self.save_gradient)
    
    def save_activation(self, module, input, output):
        self.activations = output
    
    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]
    
    def generate_cam(self, input_image, class_idx):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è CAM –∫–∞—Ä—Ç—ã"""
        # Forward pass
        output = self.model(input_image)
        
        # Backward pass –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        self.model.zero_grad()
        class_score = output[0, class_idx]
        class_score.backward()
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
        weights = torch.mean(self.gradients, dim=[2, 3])
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–π
        cam = torch.zeros(self.activations.shape[2:])
        for i, w in enumerate(weights[0]):
            cam += w * self.activations[0, i]
        
        # ReLU –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        cam = F.relu(cam)
        cam = (cam - cam.min()) / (cam.max() - cam.min())
        
        return cam

class IntegratedGradients:
    """Integrated Gradients –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    
    def __init__(self, model):
        self.model = model
        
    def compute_gradients(self, inputs, target_class):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
        inputs.requires_grad_(True)
        outputs = self.model(inputs)
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ —Ü–µ–ª–µ–≤–æ–º—É –∫–ª–∞—Å—Å—É
        target_score = outputs[0, target_class]
        gradients = torch.autograd.grad(target_score, inputs)[0]
        
        return gradients
    
    def integrated_gradients(self, input_image, baseline=None, target_class=0, steps=50):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Integrated Gradients"""
        if baseline is None:
            baseline = torch.zeros_like(input_image)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—É—Ç–∏ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏
        alphas = torch.linspace(0, 1, steps)
        integrated_grads = torch.zeros_like(input_image)
        
        for alpha in alphas:
            # –ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            interpolated = baseline + alpha * (input_image - baseline)
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
            grads = self.compute_gradients(interpolated, target_class)
            integrated_grads += grads
        
        # –°—Ä–µ–¥–Ω–µ–µ –∏ —É–º–Ω–æ–∂–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω–æ—Å—Ç—å
        integrated_grads /= steps
        integrated_grads *= (input_image - baseline)
        
        return integrated_grads

class SaliencyMaps:
    """–†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã saliency maps"""
    
    @staticmethod
    def vanilla_gradient(model, input_image, target_class):
        """–ü—Ä–æ—Å—Ç—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã"""
        input_image.requires_grad_(True)
        output = model(input_image)
        target_score = output[0, target_class]
        saliency = torch.autograd.grad(target_score, input_image)[0]
        return saliency.abs()
    
    @staticmethod
    def smooth_grad(model, input_image, target_class, noise_level=0.1, n_samples=50):
        """SmoothGrad - —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–º –≤—Ö–æ–¥–∞–º"""
        saliency_sum = torch.zeros_like(input_image)
        
        for _ in range(n_samples):
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞
            noise = torch.randn_like(input_image) * noise_level
            noisy_input = input_image + noise
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
            saliency = SaliencyMaps.vanilla_gradient(model, noisy_input, target_class)
            saliency_sum += saliency
        
        return saliency_sum / n_samples
    
    @staticmethod
    def guided_backprop(model, input_image, target_class):
        """Guided Backpropagation"""
        def relu_hook(module, grad_in, grad_out):
            # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ReLU –¥–ª—è guided backprop
            return (torch.clamp(grad_in[0], min=0),)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è hooks –¥–ª—è –≤—Å–µ—Ö ReLU —Å–ª–æ–µ–≤
        handles = []
        for module in model.modules():
            if isinstance(module, nn.ReLU):
                handles.append(module.register_backward_hook(relu_hook))
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        saliency = SaliencyMaps.vanilla_gradient(model, input_image, target_class)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ hooks
        for handle in handles:
            handle.remove()
        
        return saliency
```

---

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –∏ —Ä–µ—Å—É—Ä—Å—ã

### –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
- [[neural-networks-architecture|–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π]] - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
- [[machine-learning-theory|–¢–µ–æ—Ä–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è]] - –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã
- [[large-language-models|–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏]] - –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ NLP
- [[computer-vision|–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ]] - –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ CV
- [[ai-in-production|AI –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ]] - —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª–µ–π

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
```
–ö–Ω–∏–≥–∏ –∏ –∫—É—Ä—Å—ã:
‚îú‚îÄ‚îÄ Deep Learning (Goodfellow, Bengio, Courville)
‚îú‚îÄ‚îÄ Pattern Recognition and Machine Learning (Bishop)
‚îú‚îÄ‚îÄ CS231n: Convolutional Neural Networks (Stanford)
‚îú‚îÄ‚îÄ CS224n: Natural Language Processing (Stanford)
‚îú‚îÄ‚îÄ Fast.ai Practical Deep Learning
‚îî‚îÄ‚îÄ Deep Learning Specialization (Coursera)

–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏:
‚îú‚îÄ‚îÄ NeurIPS (Neural Information Processing Systems)
‚îú‚îÄ‚îÄ ICML (International Conference on Machine Learning)
‚îú‚îÄ‚îÄ ICLR (International Conference on Learning Representations)
‚îú‚îÄ‚îÄ CVPR (Computer Vision and Pattern Recognition)
‚îî‚îÄ‚îÄ EMNLP/ACL (Natural Language Processing)
```

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
```python
# –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

# –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import transformers
import timm  # PyTorch Image Models
import torchvision
import albumentations  # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑
import wandb
import tensorboard
import captum  # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
import optuna
import ray.tune
import hydra
```

---

üí° **–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ "–±–æ–ª—å—à–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", –∞ —Å–ª–æ–∂–Ω–∞—è –æ–±–ª–∞—Å—Ç—å —Å–æ —Å–≤–æ–∏–º–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ—Å–Ω–æ–≤–∞–º–∏, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö AI —Å–∏—Å—Ç–µ–º! üß†üöÄ 
# üßÆ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ç–µ–æ—Ä–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

## üî¨ –¢–µ–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è (Learning Theory)

### PAC Learning Framework
```
Probably Approximately Correct (PAC) Learning:
‚îú‚îÄ‚îÄ Concept class C
‚îú‚îÄ‚îÄ Hypothesis space H  
‚îú‚îÄ‚îÄ Sample complexity m(Œµ, Œ¥)
‚îú‚îÄ‚îÄ VC Dimension
‚îú‚îÄ‚îÄ Rademacher Complexity
‚îî‚îÄ‚îÄ Generalization bounds
```

#### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã PAC Learning
```python
import numpy as np
import torch
import matplotlib.pyplot as plt
from scipy import stats

class PACLearningAnalysis:
    """–ê–Ω–∞–ª–∏–∑ PAC Learning —Å–≤–æ–π—Å—Ç–≤"""
    
    @staticmethod
    def vc_dimension_linear_classifier(d):
        """
        VC dimension –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ R^d
        VC(H) = d + 1 –¥–ª—è –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –≤ R^d
        """
        return d + 1
    
    @staticmethod
    def sample_complexity_bound(vc_dim, epsilon, delta):
        """
        Sample complexity bound:
        m ‚â• (8/Œµ) * [VC(H) * log(13/Œµ) + log(4/Œ¥)]
        """
        term1 = vc_dim * np.log(13 / epsilon)
        term2 = np.log(4 / delta)
        sample_complexity = (8 / epsilon) * (term1 + term2)
        
        return int(np.ceil(sample_complexity))
    
    @staticmethod
    def rademacher_complexity_estimate(model, data_loader, num_samples=1000):
        """
        –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ Rademacher complexity:
        R_m(H) = E_œÉ[sup_{h‚ààH} (1/m) Œ£ œÉ_i h(x_i)]
        """
        complexities = []
        
        for _ in range(num_samples):
            # –°–ª—É—á–∞–π–Ω—ã–µ Rademacher –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
            sigma = torch.randint(0, 2, (len(data_loader.dataset),)) * 2 - 1
            
            batch_complexities = []
            idx = 0
            
            for batch_x, _ in data_loader:
                batch_size = batch_x.size(0)
                batch_sigma = sigma[idx:idx + batch_size].float()
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
                with torch.no_grad():
                    outputs = model(batch_x)
                    if outputs.dim() > 1:
                        outputs = outputs.mean(dim=1)  # –î–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –≤—ã—Ö–æ–¥–∞
                
                # Rademacher correlation
                correlation = torch.mean(batch_sigma * outputs)
                batch_complexities.append(correlation.item())
                
                idx += batch_size
            
            complexities.append(np.mean(batch_complexities))
        
        return np.mean(complexities), np.std(complexities)
    
    @staticmethod
    def generalization_bound(train_error, rademacher_complexity, m, delta=0.05):
        """
        Generalization bound —á–µ—Ä–µ–∑ Rademacher complexity:
        R(h) ‚â§ RÃÇ(h) + 2R_m(H) + sqrt(log(2/Œ¥) / (2m))
        """
        confidence_term = np.sqrt(np.log(2 / delta) / (2 * m))
        bound = train_error + 2 * rademacher_complexity + confidence_term
        
        return bound

class BayesianLearningTheory:
    """–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    
    @staticmethod
    def bayesian_posterior(prior_mean, prior_cov, likelihood_precision, data_mean, n_samples):
        """
        –ë–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–≥–æ —Å–ª—É—á–∞—è:
        p(Œ∏|D) ‚àù p(D|Œ∏) * p(Œ∏)
        """
        # Precision (–æ–±—Ä–∞—Ç–Ω–∞—è –∫–æ–≤–∞—Ä–∏–∞—Ü–∏—è)
        prior_precision = torch.inverse(prior_cov)
        
        # Posterior precision
        posterior_precision = prior_precision + n_samples * likelihood_precision * torch.eye(prior_mean.size(0))
        
        # Posterior covariance
        posterior_cov = torch.inverse(posterior_precision)
        
        # Posterior mean
        posterior_mean = posterior_cov @ (prior_precision @ prior_mean + 
                                        n_samples * likelihood_precision * data_mean)
        
        return posterior_mean, posterior_cov
    
    @staticmethod
    def model_evidence(data, prior_params, likelihood_params):
        """
        Model evidence (marginal likelihood):
        p(D|M) = ‚à´ p(D|Œ∏,M) p(Œ∏|M) dŒ∏
        """
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–≥–æ —Å–ª—É—á–∞—è
        log_evidence = 0
        
        # Prior contribution
        log_evidence += stats.multivariate_normal.logpdf(
            prior_params['mean'], 
            np.zeros_like(prior_params['mean']), 
            prior_params['cov']
        )
        
        # Likelihood contribution
        for x in data:
            log_evidence += stats.norm.logpdf(x, 
                                            likelihood_params['mean'], 
                                            likelihood_params['std'])
        
        return log_evidence
    
    @staticmethod
    def bayes_factor(evidence1, evidence2):
        """
        Bayes Factor –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π:
        BF = p(D|M1) / p(D|M2)
        """
        return np.exp(evidence1 - evidence2)

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–µ–æ—Ä–∏—è –≤ ML
class InformationTheoreticLearning:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
    
    @staticmethod
    def mutual_information_neural_estimation(x, y, critic_network, num_samples=1000):
        """
        MINE: Mutual Information Neural Estimation
        I(X;Y) = sup_T E_P[T(x,y)] - log E_Q[exp(T(x,y))]
        –≥–¥–µ Q - product marginal
        """
        # Joint samples
        joint_samples = torch.cat([x, y], dim=1)
        
        # Marginal samples (shuffle y)
        y_shuffled = y[torch.randperm(y.size(0))]
        marginal_samples = torch.cat([x, y_shuffled], dim=1)
        
        # Critic scores
        joint_scores = critic_network(joint_samples)
        marginal_scores = critic_network(marginal_samples)
        
        # MINE estimator
        mi_estimate = torch.mean(joint_scores) - torch.log(torch.mean(torch.exp(marginal_scores)))
        
        return mi_estimate
    
    @staticmethod
    def entropy_estimation(samples, k=3):
        """
        –û—Ü–µ–Ω–∫–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏ —á–µ—Ä–µ–∑ k-NN:
        H(X) ‚âà œà(N) - œà(k) + log(c_d) + (d/N) Œ£ log(œÅ_k(i))
        """
        n, d = samples.shape
        
        # k-NN distances
        from sklearn.neighbors import NearestNeighbors
        nbrs = NearestNeighbors(n_neighbors=k+1).fit(samples)
        distances, _ = nbrs.kneighbors(samples)
        
        # k-th nearest neighbor distances (excluding self)
        knn_distances = distances[:, k]
        
        # Volume constant for d-dimensional unit ball
        c_d = np.pi**(d/2) / np.math.gamma(d/2 + 1)
        
        # Digamma function
        from scipy.special import digamma
        
        # Entropy estimate
        entropy = digamma(n) - digamma(k) + np.log(c_d) + (d/n) * np.sum(np.log(knn_distances + 1e-10))
        
        return entropy
    
    @staticmethod
    def conditional_entropy(x, y, k=3):
        """
        Conditional entropy H(Y|X) —á–µ—Ä–µ–∑ k-NN
        """
        joint_entropy = InformationTheoreticLearning.entropy_estimation(
            np.column_stack([x, y]), k=k
        )
        x_entropy = InformationTheoreticLearning.entropy_estimation(x, k=k)
        
        conditional_entropy = joint_entropy - x_entropy
        return conditional_entropy
    
    @staticmethod
    def information_bottleneck_objective(x, y, z, beta=1.0):
        """
        Information Bottleneck:
        L = I(Z;Y) - Œ≤ * I(X;Z)
        """
        # Mutual informations (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
        i_z_y = np.corrcoef(z.flatten(), y.flatten())[0, 1]**2  # Squared correlation
        i_x_z = np.corrcoef(x.flatten(), z.flatten())[0, 1]**2
        
        objective = i_z_y - beta * i_x_z
        return objective, i_z_y, i_x_z

# –¢–µ–æ—Ä–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è ML
class OptimizationTheory:
    """–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ ML"""
    
    @staticmethod
    def convergence_analysis_sgd(gradient_norms, learning_rates, lipschitz_constant):
        """
        –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ SGD:
        –î–ª—è —Å–∏–ª—å–Ω–æ –≤—ã–ø—É–∫–ª—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π: E[f(x_t) - f*] ‚â§ (1-ŒºŒ∑)^t * [f(x_0) - f*]
        """
        convergence_rates = []
        
        for t, (grad_norm, lr) in enumerate(zip(gradient_norms, learning_rates)):
            # –£—Å–ª–æ–≤–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏: lr < 2/L
            if lr >= 2 / lipschitz_constant:
                print(f"Warning: Learning rate {lr} too large at step {t}")
            
            # Theoretical convergence rate (simplified)
            rate = (1 - lr * 0.1)**t  # Assuming strong convexity parameter Œº = 0.1
            convergence_rates.append(rate)
        
        return convergence_rates
    
    @staticmethod
    def adam_convergence_bound(gradients, beta1=0.9, beta2=0.999, epsilon=1e-8):
        """
        –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ Adam:
        Convergence bound –¥–ª—è non-convex case
        """
        t = len(gradients)
        
        # Bias correction terms
        beta1_t = beta1**t
        beta2_t = beta2**t
        
        # Second moment bound
        v_max = max([torch.norm(g)**2 for g in gradients])
        
        # Theoretical bound (simplified)
        bound = np.sqrt(v_max) / (1 - beta1_t) * np.sqrt(1 - beta2_t)
        
        return bound
    
    @staticmethod
    def escape_from_saddle_points(hessian_eigenvalues, noise_scale):
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–±–µ–≥–∞–Ω–∏—è —Å–µ–¥–ª–æ–≤—ã—Ö —Ç–æ—á–µ–∫:
        Noisy SGD –º–æ–∂–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —Å–µ–¥–ª–æ–≤—ã—Ö —Ç–æ—á–µ–∫ –ø—Ä–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–º —à—É–º–µ
        """
        min_eigenvalue = min(hessian_eigenvalues)
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π –∏–∑–±–µ–≥–∞–Ω–∏—è —Å–µ–¥–ª–æ–≤–æ–π —Ç–æ—á–∫–∏
        escape_condition = noise_scale > abs(min_eigenvalue)
        
        # –í—Ä–µ–º—è escape (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ)
        if escape_condition:
            escape_time = 1 / (noise_scale - abs(min_eigenvalue))
        else:
            escape_time = float('inf')
        
        return escape_condition, escape_time

# Curse of Dimensionality
class DimensionalityAnalysis:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∫–ª—è—Ç–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
    
    @staticmethod
    def volume_concentration(dimensions, num_samples=10000):
        """
        –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –æ–±—ä–µ–º–∞ –≤ –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö:
        –í d-–º–µ—Ä–Ω–æ–º —à–∞—Ä–µ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –æ–±—ä–µ–º–∞ —Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ —É –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏
        """
        results = {}
        
        for d in dimensions:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ—á–µ–∫ –≤ –µ–¥–∏–Ω–∏—á–Ω–æ–º —à–∞—Ä–µ
            points = np.random.randn(num_samples, d)
            norms = np.linalg.norm(points, axis=1)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –µ–¥–∏–Ω–∏—á–Ω–æ–º—É —à–∞—Ä—É
            points = points / norms[:, np.newaxis]
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –æ—Ç —Ü–µ–Ω—Ç—Ä–∞
            distances = np.linalg.norm(points, axis=1)
            
            # –î–æ–ª—è –æ–±—ä–µ–º–∞ –≤ –≤–Ω–µ—à–Ω–µ–π –æ–±–æ–ª–æ—á–∫–µ (r > 0.9)
            outer_shell_fraction = np.mean(distances > 0.9)
            
            results[d] = {
                'mean_distance': np.mean(distances),
                'std_distance': np.std(distances),
                'outer_shell_fraction': outer_shell_fraction
            }
        
        return results
    
    @staticmethod
    def nearest_neighbor_distance_analysis(dimensions, n_points=1000):
        """
        –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –≤ –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö
        """
        results = {}
        
        for d in dimensions:
            # –°–ª—É—á–∞–π–Ω—ã–µ —Ç–æ—á–∫–∏ –≤ –≥–∏–ø–µ—Ä–∫—É–±–µ [0,1]^d
            points = np.random.uniform(0, 1, (n_points, d))
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
            from sklearn.neighbors import NearestNeighbors
            nbrs = NearestNeighbors(n_neighbors=2).fit(points)
            distances, _ = nbrs.kneighbors(points)
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞ (–∏—Å–∫–ª—é—á–∞—è —Å–∞–º—É —Ç–æ—á–∫—É)
            nn_distances = distances[:, 1]
            
            results[d] = {
                'mean_nn_distance': np.mean(nn_distances),
                'std_nn_distance': np.std(nn_distances),
                'min_nn_distance': np.min(nn_distances),
                'max_nn_distance': np.max(nn_distances)
            }
        
        return results
    
    @staticmethod
    def intrinsic_dimensionality_mle(data, k=10):
        """
        Maximum Likelihood –æ—Ü–µ–Ω–∫–∞ intrinsic dimensionality:
        dÃÇ = (k-1) / Œ£ log(r_k / r_1)
        """
        from sklearn.neighbors import NearestNeighbors
        
        n, ambient_dim = data.shape
        
        # k nearest neighbors
        nbrs = NearestNeighbors(n_neighbors=k+1).fit(data)
        distances, _ = nbrs.kneighbors(data)
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º—É —Ç–æ—á–∫—É (distance = 0)
        distances = distances[:, 1:]
        
        # MLE estimator
        ratios = distances[:, -1] / distances[:, 0]  # r_k / r_1
        log_ratios = np.log(ratios + 1e-10)
        
        intrinsic_dim = (k - 1) / np.mean(log_ratios)
        
        return intrinsic_dim

# –¢–µ–æ—Ä–∏—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π
class DeepLearningTheory:
    """–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    
    @staticmethod
    def neural_tangent_kernel(model, x1, x2):
        """
        Neural Tangent Kernel (NTK):
        K(x1, x2) = E[‚àÇf(x1)/‚àÇŒ∏ ¬∑ ‚àÇf(x2)/‚àÇŒ∏]
        """
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        def compute_gradients(x):
            output = model(x.unsqueeze(0))
            gradients = torch.autograd.grad(output.sum(), model.parameters(), 
                                          create_graph=True, retain_graph=True)
            return torch.cat([g.flatten() for g in gradients])
        
        grad1 = compute_gradients(x1)
        grad2 = compute_gradients(x2)
        
        # –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        ntk_value = torch.dot(grad1, grad2)
        
        return ntk_value.item()
    
    @staticmethod
    def effective_dimension(model, data_loader, threshold=0.99):
        """
        Effective dimension –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏:
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π NTK, –æ–±—ä—è—Å–Ω—è—é—â–∏—Ö threshold –¥–∏—Å–ø–µ—Ä—Å–∏–∏
        """
        ntk_matrix = []
        data_points = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        for batch_x, _ in data_loader:
            for x in batch_x[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                data_points.append(x)
            if len(data_points) >= 100:
                break
        
        # –í—ã—á–∏—Å–ª—è–µ–º NTK –º–∞—Ç—Ä–∏—Ü—É
        n = len(data_points)
        ntk_matrix = torch.zeros(n, n)
        
        for i in range(n):
            for j in range(i, n):
                ntk_val = DeepLearningTheory.neural_tangent_kernel(
                    model, data_points[i], data_points[j]
                )
                ntk_matrix[i, j] = ntk_val
                ntk_matrix[j, i] = ntk_val
        
        # –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        eigenvalues, _ = torch.linalg.eigh(ntk_matrix)
        eigenvalues = torch.sort(eigenvalues, descending=True)[0]
        
        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        total_variance = eigenvalues.sum()
        cumulative_variance = torch.cumsum(eigenvalues, dim=0)
        effective_dim = torch.argmax((cumulative_variance / total_variance >= threshold).float()) + 1
        
        return effective_dim.item(), eigenvalues
    
    @staticmethod
    def lottery_ticket_analysis(model, sparsity_levels):
        """
        –ê–Ω–∞–ª–∏–∑ Lottery Ticket Hypothesis:
        –ü–æ–∏—Å–∫ –ø–æ–¥—Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ
        """
        original_params = {}
        for name, param in model.named_parameters():
            original_params[name] = param.clone()
        
        results = {}
        
        for sparsity in sparsity_levels:
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏
            mask = {}
            for name, param in model.named_parameters():
                # Magnitude-based pruning
                flat_param = param.abs().flatten()
                threshold = torch.quantile(flat_param, sparsity)
                mask[name] = (param.abs() > threshold).float()
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å–∫–∏
            masked_params = 0
            total_params = 0
            
            for name, param in model.named_parameters():
                param.data *= mask[name]
                masked_params += mask[name].sum().item()
                total_params += param.numel()
            
            actual_sparsity = 1 - (masked_params / total_params)
            
            results[sparsity] = {
                'actual_sparsity': actual_sparsity,
                'remaining_params': masked_params,
                'mask': mask
            }
        
        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        for name, param in model.named_parameters():
            param.data = original_params[name]
        
        return results
    
    @staticmethod
    def generalization_gap_analysis(train_losses, val_losses, model_complexity):
        """
        –ê–Ω–∞–ª–∏–∑ generalization gap:
        Gap = Training Error - Validation Error
        """
        gap = np.array(val_losses) - np.array(train_losses)
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å complexity
        correlation = np.corrcoef(gap, model_complexity)[0, 1]
        
        # –¢—Ä–µ–Ω–¥ over time
        trend_coef = np.polyfit(range(len(gap)), gap, 1)[0]
        
        return {
            'mean_gap': np.mean(gap),
            'std_gap': np.std(gap),
            'max_gap': np.max(gap),
            'complexity_correlation': correlation,
            'trend_coefficient': trend_coef
        }

# –†–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∏ adversarial –ø—Ä–∏–º–µ—Ä—ã
class RobustnessTheory:
    """–¢–µ–æ—Ä–∏—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
    
    @staticmethod
    def lipschitz_constant_estimation(model, input_dim, num_samples=1000):
        """
        –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –õ–∏–ø—à–∏—Ü–∞:
        L = max |f(x1) - f(x2)| / |x1 - x2|
        """
        lipschitz_estimates = []
        
        for _ in range(num_samples):
            # –°–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ä—ã —Ç–æ—á–µ–∫
            x1 = torch.randn(1, input_dim)
            x2 = torch.randn(1, input_dim)
            
            with torch.no_grad():
                y1 = model(x1)
                y2 = model(x2)
            
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è
            input_dist = torch.norm(x1 - x2)
            output_dist = torch.norm(y1 - y2)
            
            if input_dist > 1e-6:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                lipschitz_est = (output_dist / input_dist).item()
                lipschitz_estimates.append(lipschitz_est)
        
        return max(lipschitz_estimates), np.mean(lipschitz_estimates)
    
    @staticmethod
    def adversarial_robustness_certificate(model, x, epsilon, norm='l2'):
        """
        –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç adversarial robustness:
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –≤ Œµ-–æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏
        """
        x.requires_grad_(True)
        output = model(x)
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –≤—Ö–æ–¥—É
        grad = torch.autograd.grad(output.sum(), x, create_graph=True)[0]
        
        if norm == 'l2':
            # L2 norm robustness bound
            grad_norm = torch.norm(grad, p=2)
            certificate = epsilon * grad_norm
        elif norm == 'linf':
            # L‚àû norm robustness bound
            grad_norm = torch.norm(grad, p=float('inf'))
            certificate = epsilon * grad_norm
        else:
            raise ValueError(f"Unsupported norm: {norm}")
        
        return certificate.item(), grad_norm.item()
    
    @staticmethod
    def certified_defense_analysis(model, test_loader, epsilon=0.1):
        """
        –ê–Ω–∞–ª–∏–∑ certified defense –º–µ—Ç–æ–¥–æ–≤
        """
        certificates = []
        predictions = []
        
        for batch_x, batch_y in test_loader:
            for x, y in zip(batch_x, batch_y):
                cert, _ = RobustnessTheory.adversarial_robustness_certificate(
                    model, x.unsqueeze(0), epsilon
                )
                
                with torch.no_grad():
                    pred = model(x.unsqueeze(0)).argmax(dim=1)
                
                certificates.append(cert)
                predictions.append((pred == y).item())
        
        # Certified accuracy
        certified_correct = sum(p and c < 1.0 for p, c in zip(predictions, certificates))
        certified_accuracy = certified_correct / len(predictions)
        
        return {
            'certified_accuracy': certified_accuracy,
            'mean_certificate': np.mean(certificates),
            'certificate_distribution': certificates
        }
```

---

## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è

### Concentration Inequalities
```python
class ConcentrationInequalities:
    """–ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    @staticmethod
    def hoeffding_bound(n, a, b, delta):
        """
        –ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –•—ë—Ñ—Ñ–¥–∏–Ω–≥–∞:
        P(|XÃÑ - E[X]| ‚â• t) ‚â§ 2exp(-2nt¬≤/(b-a)¬≤)
        """
        # Bound –Ω–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π probability
        t = np.sqrt(-np.log(delta/2) * (b - a)**2 / (2 * n))
        return t
    
    @staticmethod
    def mcdiarmid_bound(n, c_values, delta):
        """
        –ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ú–∞–∫–î–∏–∞—Ä–º–∏–¥–∞:
        P(|f(X) - E[f(X)]| ‚â• t) ‚â§ 2exp(-2t¬≤/Œ£c_i¬≤)
        """
        sum_c_squared = sum(c**2 for c in c_values)
        t = np.sqrt(-np.log(delta/2) * sum_c_squared / 2)
        return t
    
    @staticmethod
    def bennett_inequality(variance, bound, n, delta):
        """
        –ù–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ë–µ–Ω–Ω–µ—Ç—Ç–∞ (–¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ–ª–∏—á–∏–Ω):
        –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ —á–µ–º –•—ë—Ñ—Ñ–¥–∏–Ω–≥–∞ –ø—Ä–∏ –º–∞–ª–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏
        """
        def h(u):
            return (1 + u) * np.log(1 + u) - u
        
        # –†–µ—à–∞–µ–º —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è t
        from scipy.optimize import fsolve
        
        def equation(t):
            return variance * h(bound * t / variance) - t**2 / n + np.log(delta / 2)
        
        try:
            t = fsolve(equation, 0.1)[0]
            return max(0, t)
        except:
            return float('inf')

# –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
class MultipleTestingCorrection:
    """–ö–æ—Ä—Ä–µ–∫—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ ML"""
    
    @staticmethod
    def bonferroni_correction(p_values, alpha=0.05):
        """
        –ü–æ–ø—Ä–∞–≤–∫–∞ –ë–æ–Ω—Ñ–µ—Ä—Ä–æ–Ω–∏:
        Adjusted Œ± = Œ± / m
        """
        m = len(p_values)
        adjusted_alpha = alpha / m
        significant = [p <= adjusted_alpha for p in p_values]
        
        return significant, adjusted_alpha
    
    @staticmethod
    def benjamini_hochberg_fdr(p_values, alpha=0.05):
        """
        –ö–æ–Ω—Ç—Ä–æ–ª—å False Discovery Rate (Benjamini-Hochberg):
        –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª—å—à–∏–π k: P(k) ‚â§ (k/m) * Œ±
        """
        m = len(p_values)
        sorted_indices = np.argsort(p_values)
        sorted_p_values = np.array(p_values)[sorted_indices]
        
        # BH –∫—Ä–∏—Ç–µ—Ä–∏–π
        bh_thresholds = [(i + 1) / m * alpha for i in range(m)]
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª—å—à–∏–π –∏–Ω–¥–µ–∫—Å —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏–π –∫—Ä–∏—Ç–µ—Ä–∏—é
        significant_idx = -1
        for i in range(m - 1, -1, -1):
            if sorted_p_values[i] <= bh_thresholds[i]:
                significant_idx = i
                break
        
        # –û—Ç–º–µ—á–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã
        significant = [False] * m
        if significant_idx >= 0:
            for i in range(significant_idx + 1):
                original_idx = sorted_indices[i]
                significant[original_idx] = True
        
        return significant, significant_idx
    
    @staticmethod
    def storey_fdr_estimation(p_values, lambda_threshold=0.5):
        """
        –û—Ü–µ–Ω–∫–∞ Storey –¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ FDR:
        œÄÃÇ‚ÇÄ = #{p_i > Œª} / [(1-Œª)m]
        """
        m = len(p_values)
        num_above_threshold = sum(1 for p in p_values if p > lambda_threshold)
        
        pi_0_estimate = num_above_threshold / ((1 - lambda_threshold) * m)
        pi_0_estimate = min(pi_0_estimate, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–≤–µ—Ä—Ö—É
        
        return pi_0_estimate
```

---

## üéØ –ö–∞—É–∑–∞–ª—å–Ω–æ—Å—Ç—å –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏

### Causal Inference
```python
class CausalInference:
    """–ö–∞—É–∑–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏"""
    
    @staticmethod
    def average_treatment_effect(y, treatment, x_confounders):
        """
        Average Treatment Effect (ATE):
        ATE = E[Y(1) - Y(0)]
        """
        from sklearn.linear_model import LinearRegression
        
        # Propensity score (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∏—Ç—å treatment)
        propensity_model = LinearRegression()
        propensity_model.fit(x_confounders, treatment)
        propensity_scores = propensity_model.predict_proba(x_confounders)[:, 1]
        
        # Inverse Propensity Weighting (IPW)
        treated_mask = treatment == 1
        control_mask = treatment == 0
        
        # ATE —á–µ—Ä–µ–∑ IPW
        ate_treated = np.mean(y[treated_mask] / propensity_scores[treated_mask])
        ate_control = np.mean(y[control_mask] / (1 - propensity_scores[control_mask]))
        
        ate = ate_treated - ate_control
        
        return ate, propensity_scores
    
    @staticmethod
    def backdoor_adjustment(y, treatment, confounders, confounder_values):
        """
        Backdoor adjustment –¥–ª—è –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞:
        P(Y|do(T=t)) = Œ£_c P(Y|T=t,C=c) P(C=c)
        """
        from sklearn.preprocessing import LabelEncoder
        from collections import Counter
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        unique_confounders = np.unique(confounders, axis=0)
        confounder_probs = {}
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ P(C=c)
        for conf in unique_confounders:
            count = np.sum(np.all(confounders == conf, axis=1))
            prob = count / len(confounders)
            confounder_probs[tuple(conf)] = prob
        
        # Adjustment formula
        adjusted_effect = 0
        
        for conf_val, prob in confounder_probs.items():
            # P(Y|T=1,C=c)
            mask_treated = (treatment == 1) & np.all(confounders == conf_val, axis=1)
            if np.any(mask_treated):
                y_treated_given_c = np.mean(y[mask_treated])
            else:
                y_treated_given_c = 0
            
            # P(Y|T=0,C=c)
            mask_control = (treatment == 0) & np.all(confounders == conf_val, axis=1)
            if np.any(mask_control):
                y_control_given_c = np.mean(y[mask_control])
            else:
                y_control_given_c = 0
            
            # Weighted contribution
            adjusted_effect += (y_treated_given_c - y_control_given_c) * prob
        
        return adjusted_effect
    
    @staticmethod
    def instrumental_variable_analysis(y, treatment, instrument, controls=None):
        """
        Instrumental Variable (IV) –∞–Ω–∞–ª–∏–∑:
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –∫–∞—É–∑–∞–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        """
        from sklearn.linear_model import LinearRegression
        
        # Two-Stage Least Squares (2SLS)
        
        # First stage: Treatment ~ Instrument + Controls
        if controls is not None:
            first_stage_x = np.column_stack([instrument.reshape(-1, 1), controls])
        else:
            first_stage_x = instrument.reshape(-1, 1)
        
        first_stage = LinearRegression()
        first_stage.fit(first_stage_x, treatment)
        treatment_predicted = first_stage.predict(first_stage_x)
        
        # Second stage: Y ~ Predicted_Treatment + Controls
        if controls is not None:
            second_stage_x = np.column_stack([treatment_predicted.reshape(-1, 1), controls])
        else:
            second_stage_x = treatment_predicted.reshape(-1, 1)
        
        second_stage = LinearRegression()
        second_stage.fit(second_stage_x, y)
        
        # IV estimate (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏ treatment)
        iv_estimate = second_stage.coef_[0]
        
        # First stage F-statistic (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–ª—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞)
        residuals_first = treatment - treatment_predicted
        mse_first = np.mean(residuals_first**2)
        
        # Simplified F-statistic
        f_stat = np.var(treatment_predicted) / mse_first if mse_first > 0 else 0
        
        return iv_estimate, f_stat, first_stage, second_stage

class CausalGraphs:
    """–†–∞–±–æ—Ç–∞ —Å –∫–∞—É–∑–∞–ª—å–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∞–º–∏"""
    
    @staticmethod
    def d_separation_test(graph, x, y, conditioning_set):
        """
        –¢–µ—Å—Ç d-separation –≤ –∫–∞—É–∑–∞–ª—å–Ω–æ–º –≥—Ä–∞—Ñ–µ:
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å–ª–æ–≤–Ω—É—é –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        """
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (causalgraphicalmodels, networkx)
        
        def has_path_blocked(path, conditioning_set):
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ª–∏ –ø—É—Ç—å conditioning set"""
            for i in range(1, len(path) - 1):
                node = path[i]
                prev_node = path[i-1]
                next_node = path[i+1]
                
                # Collider pattern: prev -> node <- next
                if graph.get(prev_node, {}).get(node) and graph.get(next_node, {}).get(node):
                    if node not in conditioning_set:
                        return True  # Blocked by collider
                
                # Chain/fork: prev -> node -> next or prev <- node -> next
                else:
                    if node in conditioning_set:
                        return True  # Blocked by conditioning
            
            return False
        
        # –ù–∞–π—Ç–∏ –≤—Å–µ –ø—É—Ç–∏ –º–µ–∂–¥—É x –∏ y (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        def find_paths(start, end, path=[]):
            path = path + [start]
            if start == end:
                return [path]
            
            paths = []
            if start in graph:
                for node in graph[start]:
                    if node not in path:  # –ò–∑–±–µ–≥–∞–µ–º —Ü–∏–∫–ª–æ–≤
                        newpaths = find_paths(node, end, path)
                        paths.extend(newpaths)
            return paths
        
        paths = find_paths(x, y)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ø—É—Ç–∏
        all_paths_blocked = True
        for path in paths:
            if not has_path_blocked(path, conditioning_set):
                all_paths_blocked = False
                break
        
        return all_paths_blocked  # True –µ—Å–ª–∏ x ‚ä• y | conditioning_set
    
    @staticmethod
    def identify_confounders(graph, treatment, outcome):
        """
        –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–Ω—Ñ–∞—É–Ω–¥–µ—Ä–æ–≤ –ø–æ –∫–∞—É–∑–∞–ª—å–Ω–æ–º—É –≥—Ä–∞—Ñ—É
        """
        confounders = set()
        
        # –ö–æ–Ω—Ñ–∞—É–Ω–¥–µ—Ä - –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è, –≤–ª–∏—è—é—â–∞—è –Ω–∞ treatment –∏ outcome
        for node in graph:
            influences_treatment = treatment in graph.get(node, {})
            influences_outcome = outcome in graph.get(node, {})
            
            if influences_treatment and influences_outcome:
                confounders.add(node)
        
        return list(confounders)
```

---

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã
- [[machine-learning-theory|–û—Å–Ω–æ–≤—ã —Ç–µ–æ—Ä–∏–∏ ML]] - –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
- [[deep-learning-foundations|–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ]] - –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏
- [[neural-networks-architecture|–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã]] - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏
- [[generative-ai-theory|–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏]] - —Ç–µ–æ—Ä–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- [[ai-ethics-safety|–≠—Ç–∏–∫–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å]] - —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
- [[prompt-engineering|Prompt Engineering]] - –∫–∞—É–∑–∞–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è

---

üéì **–ö–ª—é—á–µ–≤–∞—è –º—ã—Å–ª—å:** –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ç–µ–æ—Ä–∏—è ML –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è! üî¨‚ú® 
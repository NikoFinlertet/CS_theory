# Neural Networks & Deep Learning

> **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ**
>
> –û—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞ –¥–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä

## üß† –û—Å–Ω–æ–≤—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_circles
from sklearn.preprocessing import StandardScaler

# ==================== –ë–ê–ó–û–í–´–ô –ü–ï–†–¶–ï–ü–¢–†–û–ù ====================
class Perceptron:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å - –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω
    
    –ú–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å —Ç–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–µ –∑–∞–¥–∞—á–∏
    """
    
    def __init__(self, learning_rate=0.1, max_epochs=100):
        self.learning_rate = learning_rate
        self.max_epochs = max_epochs
        self.weights = None
        self.bias = None
        self.errors = []
    
    def activation_function(self, x):
        """
        –°—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        
        –í—ã—Ö–æ–¥: 1 –µ—Å–ª–∏ x >= 0, –∏–Ω–∞—á–µ 0
        """
        return np.where(x >= 0, 1, 0)
    
    def fit(self, X, y):
        """
        –û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
        
        –ü—Ä–∞–≤–∏–ª–æ –æ–±—É—á–µ–Ω–∏—è: w = w + Œ± * (y - ≈∑) * x
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        n_features = X.shape[1]
        self.weights = np.random.normal(0, 0.1, n_features)
        self.bias = 0
        
        for epoch in range(self.max_epochs):
            epoch_errors = 0
            
            for i in range(len(X)):
                # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                linear_output = np.dot(X[i], self.weights) + self.bias
                prediction = self.activation_function(linear_output)
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏
                error = y[i] - prediction
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ bias
                if error != 0:
                    self.weights += self.learning_rate * error * X[i]
                    self.bias += self.learning_rate * error
                    epoch_errors += 1
            
            self.errors.append(epoch_errors)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –æ—à–∏–±–æ–∫, –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ
            if epoch_errors == 0:
                break
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        linear_output = np.dot(X, self.weights) + self.bias
        return self.activation_function(linear_output)

# ==================== –ú–ù–û–ì–û–°–õ–û–ô–ù–´–ô –ü–ï–†–¶–ï–ü–¢–†–û–ù ====================
class MLP:
    """
    –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω —Å –æ–±—Ä–∞—Ç–Ω—ã–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—à–∏–±–∫–∏
    
    –ú–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–¥–∞—á–∏
    """
    
    def __init__(self, layer_sizes, learning_rate=0.1, max_epochs=1000):
        self.layer_sizes = layer_sizes
        self.learning_rate = learning_rate
        self.max_epochs = max_epochs
        self.weights = []
        self.biases = []
        self.initialize_weights()
    
    def initialize_weights(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º Xavier
        
        –ü–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º/–≤–∑—Ä—ã–≤–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        """
        for i in range(len(self.layer_sizes) - 1):
            # Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            limit = np.sqrt(6 / (self.layer_sizes[i] + self.layer_sizes[i + 1]))
            weight = np.random.uniform(-limit, limit, 
                                     (self.layer_sizes[i], self.layer_sizes[i + 1]))
            bias = np.zeros(self.layer_sizes[i + 1])
            
            self.weights.append(weight)
            self.biases.append(bias)
    
    def sigmoid(self, x):
        """
        –°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        
        f(x) = 1 / (1 + e^(-x))
        –í—ã—Ö–æ–¥: (0, 1)
        """
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è
        x = np.clip(x, -500, 500)
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x):
        """
        –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Å–∏–≥–º–æ–∏–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        
        f'(x) = f(x) * (1 - f(x))
        """
        return x * (1 - x)
    
    def relu(self, x):
        """
        ReLU —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        
        f(x) = max(0, x)
        –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        """
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """
        –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è ReLU
        
        f'(x) = 1 –µ—Å–ª–∏ x > 0, –∏–Ω–∞—á–µ 0
        """
        return np.where(x > 0, 1, 0)
    
    def forward_propagation(self, X):
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        
        –í—ã—á–∏—Å–ª—è–µ—Ç –≤—ã—Ö–æ–¥ —Å–µ—Ç–∏ –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        activations = [X]
        
        for i in range(len(self.weights)):
            # –õ–∏–Ω–µ–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            
            # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
            if i == len(self.weights) - 1:  # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
                a = self.sigmoid(z)
            else:  # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏
                a = self.relu(z)
            
            activations.append(a)
        
        return activations
    
    def backward_propagation(self, X, y, activations):
        """
        –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏
        
        –í—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤
        """
        m = X.shape[0]  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        weight_gradients = [np.zeros_like(w) for w in self.weights]
        bias_gradients = [np.zeros_like(b) for b in self.biases]
        
        # –û—à–∏–±–∫–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
        output_error = activations[-1] - y
        
        # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        errors = [output_error]
        
        for i in range(len(self.weights) - 1, -1, -1):
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç –≤–µ—Å–æ–≤
            weight_gradients[i] = np.dot(activations[i].T, errors[-1]) / m
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç bias
            bias_gradients[i] = np.mean(errors[-1], axis=0)
            
            # –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª–æ—è
            if i > 0:
                if i == len(self.weights) - 1:  # –û—Ç –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
                    delta = errors[-1] * self.sigmoid_derivative(activations[i + 1])
                else:  # –û—Ç —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
                    delta = errors[-1] * self.relu_derivative(activations[i + 1])
                
                error = np.dot(delta, self.weights[i].T)
                errors.append(error)
        
        return weight_gradients, bias_gradients
    
    def fit(self, X, y, validation_data=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ y –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if len(y.shape) == 1:
            y = y.reshape(-1, 1)
        
        train_losses = []
        val_losses = []
        
        for epoch in range(self.max_epochs):
            # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            activations = self.forward_propagation(X)
            
            # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            weight_gradients, bias_gradients = self.backward_propagation(X, y, activations)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * weight_gradients[i]
                self.biases[i] -= self.learning_rate * bias_gradients[i]
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
            train_loss = self.compute_loss(y, activations[-1])
            train_losses.append(train_loss)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏
            if validation_data:
                X_val, y_val = validation_data
                val_predictions = self.predict(X_val)
                val_loss = self.compute_loss(y_val.reshape(-1, 1), val_predictions)
                val_losses.append(val_loss)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            if epoch % 100 == 0:
                print(f"Epoch {epoch}, Train Loss: {train_loss:.4f}")
        
        return train_losses, val_losses
    
    def compute_loss(self, y_true, y_pred):
        """
        –ë–∏–Ω–∞—Ä–Ω–∞—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è
        
        Loss = -1/m * Œ£(y*log(≈∑) + (1-y)*log(1-≈∑))
        """
        # –ò–∑–±–µ–≥–∞–µ–º log(0)
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        activations = self.forward_propagation(X)
        return activations[-1]

# ==================== –§–£–ù–ö–¶–ò–ò –ê–ö–¢–ò–í–ê–¶–ò–ò ====================
class ActivationFunctions:
    """
    –†–∞–∑–ª–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏ –∏—Ö —Å–≤–æ–π—Å—Ç–≤–∞
    """
    
    @staticmethod
    def sigmoid(x):
        """
        –°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
        
        –ü–ª—é—Å—ã: –≥–ª–∞–¥–∫–∞—è, –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è
        –ú–∏–Ω—É—Å—ã: –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        """
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    @staticmethod
    def tanh(x):
        """
        –ì–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π —Ç–∞–Ω–≥–µ–Ω—Å
        
        –ü–ª—é—Å—ã: –≤—ã—Ö–æ–¥ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1], —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞
        –ú–∏–Ω—É—Å—ã: –≤—Å–µ –µ—â–µ –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        """
        return np.tanh(x)
    
    @staticmethod
    def relu(x):
        """
        Rectified Linear Unit
        
        –ü–ª—é—Å—ã: –ø—Ä–æ—Å—Ç–æ—Ç–∞, —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        –ú–∏–Ω—É—Å—ã: –º–æ–∂–µ—Ç "—É–º–µ—Ä–µ—Ç—å" (dead ReLU problem)
        """
        return np.maximum(0, x)
    
    @staticmethod
    def leaky_relu(x, alpha=0.01):
        """
        Leaky ReLU
        
        –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "–º–µ—Ä—Ç–≤—ã—Ö" –Ω–µ–π—Ä–æ–Ω–æ–≤
        """
        return np.where(x > 0, x, alpha * x)
    
    @staticmethod
    def elu(x, alpha=1.0):
        """
        Exponential Linear Unit
        
        –ü–ª—é—Å—ã: –≥–ª–∞–¥–∫–∞—è, –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        """
        return np.where(x > 0, x, alpha * (np.exp(x) - 1))
    
    @staticmethod
    def swish(x):
        """
        Swish —Ñ—É–Ω–∫—Ü–∏—è (Self-Gated)
        
        f(x) = x * sigmoid(x)
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö
        """
        return x * ActivationFunctions.sigmoid(x)
    
    @staticmethod
    def softmax(x):
        """
        Softmax –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª–æ–≥–∏—Ç—ã –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        """
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# ==================== –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–Ø –í –ù–ï–ô–†–û–ù–ù–´–• –°–ï–¢–Ø–• ====================
class Regularization:
    """
    –ú–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
    """
    
    @staticmethod
    def dropout(X, dropout_rate=0.5, training=True):
        """
        Dropout —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        
        –°–ª—É—á–∞–π–Ω–æ "–≤—ã–∫–ª—é—á–∞–µ—Ç" –Ω–µ–π—Ä–æ–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        """
        if not training:
            return X
        
        mask = np.random.binomial(1, 1 - dropout_rate, X.shape)
        return X * mask / (1 - dropout_rate)
    
    @staticmethod
    def batch_normalization(X, gamma=1, beta=0, eps=1e-8):
        """
        Batch Normalization
        
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤—Ö–æ–¥—ã –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è
        –£—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∏ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å
        """
        mean = np.mean(X, axis=0)
        var = np.var(X, axis=0)
        
        X_normalized = (X - mean) / np.sqrt(var + eps)
        return gamma * X_normalized + beta, mean, var
    
    @staticmethod
    def layer_normalization(X, gamma=1, beta=0, eps=1e-8):
        """
        Layer Normalization
        
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–Ω–µ –ø–æ –±–∞—Ç—á—É)
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è RNN –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
        """
        mean = np.mean(X, axis=1, keepdims=True)
        var = np.var(X, axis=1, keepdims=True)
        
        X_normalized = (X - mean) / np.sqrt(var + eps)
        return gamma * X_normalized + beta

# ==================== –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–´ ====================
class Optimizers:
    """
    –†–∞–∑–ª–∏—á–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
    
    class SGD:
        """
        –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
        """
        def __init__(self, learning_rate=0.01, momentum=0.0):
            self.learning_rate = learning_rate
            self.momentum = momentum
            self.velocities = {}
        
        def update(self, params, gradients, param_name):
            if param_name not in self.velocities:
                self.velocities[param_name] = np.zeros_like(params)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å –º–æ–º–µ–Ω—Ç–æ–º
            self.velocities[param_name] = (self.momentum * self.velocities[param_name] + 
                                         self.learning_rate * gradients)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            return params - self.velocities[param_name]
    
    class Adam:
        """
        Adaptive Moment Estimation
        
        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∏–¥–µ–∏ momentum –∏ RMSprop
        """
        def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
            self.learning_rate = learning_rate
            self.beta1 = beta1
            self.beta2 = beta2
            self.epsilon = epsilon
            self.m = {}  # –ü–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç
            self.v = {}  # –í—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç
            self.t = 0   # –í—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥
        
        def update(self, params, gradients, param_name):
            self.t += 1
            
            if param_name not in self.m:
                self.m[param_name] = np.zeros_like(params)
                self.v[param_name] = np.zeros_like(params)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–æ–≤
            self.m[param_name] = self.beta1 * self.m[param_name] + (1 - self.beta1) * gradients
            self.v[param_name] = self.beta2 * self.v[param_name] + (1 - self.beta2) * gradients**2
            
            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è
            m_corrected = self.m[param_name] / (1 - self.beta1**self.t)
            v_corrected = self.v[param_name] / (1 - self.beta2**self.t)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            return params - self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)
    
    class RMSprop:
        """
        Root Mean Square Propagation
        
        –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        """
        def __init__(self, learning_rate=0.001, decay_rate=0.9, epsilon=1e-8):
            self.learning_rate = learning_rate
            self.decay_rate = decay_rate
            self.epsilon = epsilon
            self.cache = {}
        
        def update(self, params, gradients, param_name):
            if param_name not in self.cache:
                self.cache[param_name] = np.zeros_like(params)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—ç—à–∞
            self.cache[param_name] = (self.decay_rate * self.cache[param_name] + 
                                    (1 - self.decay_rate) * gradients**2)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            return params - self.learning_rate * gradients / (np.sqrt(self.cache[param_name]) + self.epsilon)

# ==================== –°–í–ï–†–¢–û–ß–ù–´–ï –ù–ï–ô–†–û–ù–ù–´–ï –°–ï–¢–ò ====================
class ConvolutionalLayer:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è
    """
    
    def __init__(self, num_filters, filter_size, stride=1, padding=0):
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.stride = stride
        self.padding = padding
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤
        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.1
        self.biases = np.zeros(num_filters)
    
    def convolution_2d(self, image, kernel):
        """
        2D —Å–≤–µ—Ä—Ç–∫–∞
        
        –û—Å–Ω–æ–≤–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è CNN
        """
        # –†–∞–∑–º–µ—Ä—ã
        img_height, img_width = image.shape
        kernel_height, kernel_width = kernel.shape
        
        # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        out_height = (img_height - kernel_height) // self.stride + 1
        out_width = (img_width - kernel_width) // self.stride + 1
        
        # –í—ã—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        output = np.zeros((out_height, out_width))
        
        # –°–≤–µ—Ä—Ç–∫–∞
        for i in range(0, out_height * self.stride, self.stride):
            for j in range(0, out_width * self.stride, self.stride):
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–∫–Ω–∞
                window = image[i:i+kernel_height, j:j+kernel_width]
                
                # –≠–ª–µ–º–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∏ —Å—É–º–º–∞
                output[i//self.stride, j//self.stride] = np.sum(window * kernel)
        
        return output
    
    def forward(self, input_image):
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π
        """
        batch_size, height, width = input_image.shape
        
        # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        out_height = (height - self.filter_size) // self.stride + 1
        out_width = (width - self.filter_size) // self.stride + 1
        
        # –í—ã—Ö–æ–¥–Ω—ã–µ feature maps
        output = np.zeros((batch_size, self.num_filters, out_height, out_width))
        
        for b in range(batch_size):
            for f in range(self.num_filters):
                # –°–≤–µ—Ä—Ç–∫–∞ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º
                conv_result = self.convolution_2d(input_image[b], self.filters[f])
                output[b, f] = conv_result + self.biases[f]
        
        return output

class PoolingLayer:
    """
    –°–ª–æ–π —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (pooling)
    """
    
    def __init__(self, pool_size=2, stride=2, mode='max'):
        self.pool_size = pool_size
        self.stride = stride
        self.mode = mode
    
    def forward(self, input_tensor):
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ pooling —Å–ª–æ–π
        """
        batch_size, channels, height, width = input_tensor.shape
        
        # –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
        out_height = (height - self.pool_size) // self.stride + 1
        out_width = (width - self.pool_size) // self.stride + 1
        
        output = np.zeros((batch_size, channels, out_height, out_width))
        
        for b in range(batch_size):
            for c in range(channels):
                for i in range(out_height):
                    for j in range(out_width):
                        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–∫–Ω–∞
                        start_i = i * self.stride
                        start_j = j * self.stride
                        end_i = start_i + self.pool_size
                        end_j = start_j + self.pool_size
                        
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–∫–Ω–∞
                        window = input_tensor[b, c, start_i:end_i, start_j:end_j]
                        
                        # Pooling –æ–ø–µ—Ä–∞—Ü–∏—è
                        if self.mode == 'max':
                            output[b, c, i, j] = np.max(window)
                        elif self.mode == 'avg':
                            output[b, c, i, j] = np.mean(window)
        
        return output

# ==================== –†–ï–ö–£–†–†–ï–ù–¢–ù–´–ï –ù–ï–ô–†–û–ù–ù–´–ï –°–ï–¢–ò ====================
class SimpleRNN:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
    """
    
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self.Wxh = np.random.randn(input_size, hidden_size) * 0.1
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.1
        self.Why = np.random.randn(hidden_size, output_size) * 0.1
        
        # Bias
        self.bh = np.zeros((1, hidden_size))
        self.by = np.zeros((1, output_size))
    
    def forward(self, inputs):
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ RNN
        
        inputs: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        sequence_length = len(inputs)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        h = np.zeros((1, self.hidden_size))
        
        # –•—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π
        hidden_states = []
        outputs = []
        
        for t in range(sequence_length):
            # –¢–µ–∫—É—â–∏–π –≤—Ö–æ–¥
            x = inputs[t].reshape(1, -1)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            h = np.tanh(np.dot(x, self.Wxh) + np.dot(h, self.Whh) + self.bh)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞
            y = np.dot(h, self.Why) + self.by
            
            hidden_states.append(h.copy())
            outputs.append(y.copy())
        
        return outputs, hidden_states

class LSTM:
    """
    Long Short-Term Memory
    
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤ RNN
    """
    
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # –í–µ—Å–∞ –¥–ª—è forget gate
        self.Wf = np.random.randn(input_size + hidden_size, hidden_size) * 0.1
        self.bf = np.zeros((1, hidden_size))
        
        # –í–µ—Å–∞ –¥–ª—è input gate
        self.Wi = np.random.randn(input_size + hidden_size, hidden_size) * 0.1
        self.bi = np.zeros((1, hidden_size))
        
        # –í–µ—Å–∞ –¥–ª—è candidate values
        self.Wc = np.random.randn(input_size + hidden_size, hidden_size) * 0.1
        self.bc = np.zeros((1, hidden_size))
        
        # –í–µ—Å–∞ –¥–ª—è output gate
        self.Wo = np.random.randn(input_size + hidden_size, hidden_size) * 0.1
        self.bo = np.zeros((1, hidden_size))
    
    def sigmoid(self, x):
        """–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def forward_step(self, x, h_prev, c_prev):
        """
        –û–¥–∏–Ω —à–∞–≥ LSTM
        
        x: —Ç–µ–∫—É—â–∏–π –≤—Ö–æ–¥
        h_prev: –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        c_prev: –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏
        """
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Ö–æ–¥–∞ –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        combined = np.concatenate([x, h_prev], axis=1)
        
        # Forget gate: —Ä–µ—à–∞–µ—Ç, —á—Ç–æ –∑–∞–±—ã—Ç—å –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è —è—á–µ–π–∫–∏
        f_gate = self.sigmoid(np.dot(combined, self.Wf) + self.bf)
        
        # Input gate: —Ä–µ—à–∞–µ—Ç, –∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
        i_gate = self.sigmoid(np.dot(combined, self.Wi) + self.bi)
        
        # Candidate values: –Ω–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        c_candidate = np.tanh(np.dot(combined, self.Wc) + self.bc)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —è—á–µ–π–∫–∏
        c_new = f_gate * c_prev + i_gate * c_candidate
        
        # Output gate: —Ä–µ—à–∞–µ—Ç, —á—Ç–æ –≤—ã–≤–µ—Å—Ç–∏
        o_gate = self.sigmoid(np.dot(combined, self.Wo) + self.bo)
        
        # –ù–æ–≤–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        h_new = o_gate * np.tanh(c_new)
        
        return h_new, c_new
    
    def forward(self, inputs):
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ LSTM
        """
        sequence_length = len(inputs)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π
        h = np.zeros((1, self.hidden_size))
        c = np.zeros((1, self.hidden_size))
        
        outputs = []
        
        for t in range(sequence_length):
            x = inputs[t].reshape(1, -1)
            h, c = self.forward_step(x, h, c)
            outputs.append(h.copy())
        
        return outputs

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ú–ï–†–´ ====================
def neural_network_examples():
    """
    –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π
    """
    
    # –ü—Ä–∏–º–µ—Ä 1: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å MLP
    print("=== MLP –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ===")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–µ–ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
    X, y = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ MLP
    mlp = MLP(layer_sizes=[2, 10, 8, 1], learning_rate=0.1, max_epochs=1000)
    train_losses, _ = mlp.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = mlp.predict(X_test)
    binary_predictions = (predictions > 0.5).astype(int)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    accuracy = np.mean(binary_predictions.flatten() == y_test)
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å MLP: {accuracy:.3f}")
    
    # –ü—Ä–∏–º–µ—Ä 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
    print("\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ===")
    
    x = np.linspace(-5, 5, 100)
    activations = ActivationFunctions()
    
    functions = {
        'Sigmoid': activations.sigmoid(x),
        'Tanh': activations.tanh(x),
        'ReLU': activations.relu(x),
        'Leaky ReLU': activations.leaky_relu(x),
        'Swish': activations.swish(x)
    }
    
    plt.figure(figsize=(12, 8))
    for name, values in functions.items():
        plt.plot(x, values, label=name, linewidth=2)
    
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # –ü—Ä–∏–º–µ—Ä 3: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    print("\n=== –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è ===")
    
    # –ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    X_small, y_small = make_classification(n_samples=100, n_features=2, n_redundant=0, 
                                          n_informative=2, n_clusters_per_class=1, random_state=42)
    
    X_small_scaled = StandardScaler().fit_transform(X_small)
    
    # –ë–æ–ª—å—à–∞—è —Å–µ—Ç—å –Ω–∞ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    large_mlp = MLP(layer_sizes=[2, 50, 50, 1], learning_rate=0.01, max_epochs=2000)
    
    # –û–±—É—á–µ–Ω–∏–µ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    X_train_small, X_val_small, y_train_small, y_val_small = train_test_split(
        X_small_scaled, y_small, test_size=0.3, random_state=42
    )
    
    train_losses, val_losses = large_mlp.fit(X_train_small, y_train_small, 
                                           validation_data=(X_val_small, y_val_small))
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss', color='blue')
    plt.plot(val_losses, label='Validation Loss', color='red')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è: –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    return {
        'mlp_accuracy': accuracy,
        'train_losses': train_losses,
        'val_losses': val_losses
    }

# ==================== –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ ====================
deep_learning_architectures = {
    "Feedforward Networks": {
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è",
        "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –Ω–µ—Ç —Ü–∏–∫–ª–æ–≤",
        "–ü—Ä–∏–º–µ—Ä—ã": "MLP, Deep Neural Networks"
    },
    
    "Convolutional Networks": {
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–õ–æ–∫–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤",
        "–ü—Ä–∏–º–µ—Ä—ã": "LeNet, AlexNet, VGG, ResNet"
    },
    
    "Recurrent Networks": {
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, NLP",
        "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–ü–∞–º—è—Ç—å, —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–≤—è–∑–∏",
        "–ü—Ä–∏–º–µ—Ä—ã": "RNN, LSTM, GRU"
    },
    
    "Attention Mechanisms": {
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥, –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞",
        "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö",
        "–ü—Ä–∏–º–µ—Ä—ã": "Transformer, BERT, GPT"
    },
    
    "Generative Models": {
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, —Å—Ç–∏–ª—å-—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä",
        "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–û–±—É—á–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π",
        "–ü—Ä–∏–º–µ—Ä—ã": "GAN, VAE, Flow-based models"
    }
}

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –°–û–í–ï–¢–´ ====================
deep_learning_best_practices = {
    "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö": [
        "–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
        "–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è",
        "–ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test",
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤"
    ],
    
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏": [
        "–ù–∞—á–∏–Ω–∞—Ç—å —Å –ø—Ä–æ—Å—Ç–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã",
        "–î–æ–±–∞–≤–ª—è—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π (skip connections)",
        "–ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"
    ],
    
    "–û–±—É—á–µ–Ω–∏–µ": [
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (Adam, AdamW)",
        "–ü—Ä–∏–º–µ–Ω—è—Ç—å —Ç–µ—Ö–Ω–∏–∫–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (dropout, batch norm)",
        "–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –≤–µ—Å–∞",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å early stopping"
    ],
    
    "–û—Ç–ª–∞–¥–∫–∞": [
        "–ü—Ä–æ–≤–µ—Ä—è—Ç—å –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ",
        "–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è",
        "–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏",
        "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    ]
}
```

---

*–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ - —ç—Ç–æ –ø–æ–ø—ã—Ç–∫–∞ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–∑–≥–∞, –Ω–æ —Å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é* üß†‚ö° 
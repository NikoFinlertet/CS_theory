# –†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

> **–ö–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ**
>
> –û—Ç –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

## üåê –í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (Hash Tables)

```python
# ==================== LRU CACHE –î–õ–Ø –í–ï–ë-–ü–†–ò–õ–û–ñ–ï–ù–ò–Ø ====================
# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ API –∑–∞–ø—Ä–æ—Å–æ–≤
from collections import OrderedDict
import time

class LRUCache:
    """
    LRU Cache –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ API
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤: Redis, Memcached, –±—Ä–∞—É–∑–µ—Ä–∞—Ö
    """
    def __init__(self, capacity):
        self.capacity = capacity
        self.cache = OrderedDict()
        self.timestamps = {}
    
    def get(self, key):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞ - O(1)"""
        if key in self.cache:
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∫–æ–Ω–µ—Ü (most recently used)
            self.cache.move_to_end(key)
            return self.cache[key]
        return None
    
    def put(self, key, value, ttl=300):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∫—ç—à - O(1)"""
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            if len(self.cache) >= self.capacity:
                # –£–¥–∞–ª—è–µ–º LRU —ç–ª–µ–º–µ–Ω—Ç
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
                del self.timestamps[oldest_key]
        
        self.cache[key] = value
        self.timestamps[key] = time.time() + ttl

# ==================== –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –í DJANGO/FLASK ====================
class APICache:
    """–ö—ç—à –¥–ª—è API endpoints"""
    
    def __init__(self):
        self.cache = LRUCache(1000)
    
    def cached_api_call(self, endpoint, params):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ API –≤—ã–∑–æ–≤–æ–≤"""
        cache_key = f"{endpoint}:{hash(str(params))}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_result = self.cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π API –≤—ã–∑–æ–≤
        result = self.make_api_call(endpoint, params)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.cache.put(cache_key, result, ttl=300)
        return result
    
    def make_api_call(self, endpoint, params):
        """–ò–º–∏—Ç–∞—Ü–∏—è API –≤—ã–∑–æ–≤–∞"""
        # –ó–¥–µ—Å—å –±—ã–ª –±—ã —Ä–µ–∞–ª—å–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å
        return f"Result for {endpoint} with {params}"

# ==================== RATE LIMITING ====================
class RateLimiter:
    """
    Rate limiting –¥–ª—è API
    –ê–ª–≥–æ—Ä–∏—Ç–º: Sliding Window
    """
    def __init__(self, max_requests=100, window_seconds=60):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests = {}  # user_id -> [timestamps]
    
    def is_allowed(self, user_id):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ rate limit - O(k) –≥–¥–µ k - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –æ–∫–Ω–µ"""
        now = time.time()
        
        if user_id not in self.requests:
            self.requests[user_id] = []
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        user_requests = self.requests[user_id]
        cutoff_time = now - self.window_seconds
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –≤ —Ç–µ–∫—É—â–µ–º –æ–∫–Ω–µ
        self.requests[user_id] = [
            timestamp for timestamp in user_requests 
            if timestamp > cutoff_time
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
        if len(self.requests[user_id]) >= self.max_requests:
            return False
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
        self.requests[user_id].append(now)
        return True

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Flask
"""
@app.route('/api/data')
def get_data():
    user_id = request.headers.get('User-ID')
    
    if not rate_limiter.is_allowed(user_id):
        return {'error': 'Rate limit exceeded'}, 429
    
    return api_cache.cached_api_call('/data', request.args)
"""
```

### –ü–æ–∏—Å–∫ –∏ –∞–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç (Trie)

```python
# ==================== TRIE –î–õ–Ø –ê–í–¢–û–ö–û–ú–ü–õ–ò–¢–ê ====================
class TrieNode:
    """–£–∑–µ–ª –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞"""
    def __init__(self):
        self.children = {}
        self.is_end_word = False
        self.frequency = 0  # –î–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

class AutocompleteService:
    """
    –°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç–∞
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤: Google Search, IDE, e-commerce –ø–æ–∏—Å–∫
    """
    def __init__(self):
        self.root = TrieNode()
    
    def add_word(self, word, frequency=1):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –≤ –∏–Ω–¥–µ–∫—Å - O(m) –≥–¥–µ m - –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞"""
        node = self.root
        for char in word.lower():
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        
        node.is_end_word = True
        node.frequency += frequency
    
    def search_suggestions(self, prefix, max_results=10):
        """–ü–æ–∏—Å–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ prefix - O(p + n) –≥–¥–µ p - –¥–ª–∏–Ω–∞ prefix"""
        node = self.root
        
        # –ù–∞—Ö–æ–¥–∏–º —É–∑–µ–ª —Å prefix
        for char in prefix.lower():
            if char not in node.children:
                return []
            node = node.children[char]
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ —Å –¥–∞–Ω–Ω—ã–º prefix
        suggestions = []
        self._collect_words(node, prefix, suggestions)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        suggestions.sort(key=lambda x: x[1], reverse=True)
        
        return [word for word, freq in suggestions[:max_results]]
    
    def _collect_words(self, node, prefix, suggestions):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π —Å–±–æ—Ä —Å–ª–æ–≤"""
        if node.is_end_word:
            suggestions.append((prefix, node.frequency))
        
        for char, child_node in node.children.items():
            self._collect_words(child_node, prefix + char, suggestions)

# ==================== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° ELASTICSEARCH ====================
class SearchService:
    """–°–µ—Ä–≤–∏—Å –ø–æ–∏—Å–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π fuzzy matching"""
    
    def __init__(self):
        self.trie = AutocompleteService()
        self.load_popular_searches()
    
    def load_popular_searches(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        popular_searches = [
            ("python tutorial", 1000),
            ("javascript framework", 800),
            ("react components", 600),
            ("python web scraping", 500),
            ("javascript async", 400)
        ]
        
        for query, frequency in popular_searches:
            self.trie.add_word(query, frequency)
    
    def get_suggestions(self, query, max_suggestions=5):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        return self.trie.search_suggestions(query, max_suggestions)
    
    def fuzzy_search(self, query, max_distance=2):
        """–ü–æ–∏—Å–∫ —Å —É—á–µ—Ç–æ–º –æ–ø–µ—á–∞—Ç–æ–∫ (edit distance)"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –¥–ª—è fuzzy search
        # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Elasticsearch, Solr
        pass

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ React/Vue
"""
// Frontend –∫–æ–¥
const SearchComponent = () => {
  const [query, setQuery] = useState('');
  const [suggestions, setSuggestions] = useState([]);
  
  const handleInputChange = async (value) => {
    setQuery(value);
    
    if (value.length > 2) {
      const response = await fetch(`/api/suggestions?q=${value}`);
      const data = await response.json();
      setSuggestions(data.suggestions);
    }
  };
  
  return (
    <div>
      <input 
        value={query}
        onChange={(e) => handleInputChange(e.target.value)}
        placeholder="–ü–æ–∏—Å–∫..."
      />
      <ul>
        {suggestions.map(suggestion => (
          <li key={suggestion}>{suggestion}</li>
        ))}
      </ul>
    </div>
  );
};
"""
```

---

## üóÑÔ∏è –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

### –ò–Ω–¥–µ–∫—Å—ã (B-–¥–µ—Ä–µ–≤—å—è)

```python
# ==================== B-TREE INDEX –°–ò–ú–£–õ–Ø–¶–ò–Ø ====================
class BTreeNode:
    """–£–∑–µ–ª B-–¥–µ—Ä–µ–≤–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    def __init__(self, is_leaf=False):
        self.keys = []
        self.values = []
        self.children = []
        self.is_leaf = is_leaf
        self.max_keys = 3  # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏

class DatabaseIndex:
    """
    –°–∏–º—É–ª—è—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤: MySQL, PostgreSQL, MongoDB
    """
    def __init__(self):
        self.root = BTreeNode(is_leaf=True)
    
    def search(self, key):
        """–ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É - O(log n)"""
        return self._search_node(self.root, key)
    
    def _search_node(self, node, key):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —É–∑–ª–µ"""
        i = 0
        while i < len(node.keys) and key > node.keys[i]:
            i += 1
        
        if i < len(node.keys) and key == node.keys[i]:
            return node.values[i]
        
        if node.is_leaf:
            return None
        
        return self._search_node(node.children[i], key)
    
    def insert(self, key, value):
        """–í—Å—Ç–∞–≤–∫–∞ –≤ –∏–Ω–¥–µ–∫—Å - O(log n)"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ split/merge —É–∑–ª–æ–≤
        pass

# ==================== QUERY OPTIMIZER ====================
class QueryOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∞
    """
    def __init__(self):
        self.table_stats = {}
        self.index_stats = {}
    
    def analyze_query(self, sql_query):
        """–ê–Ω–∞–ª–∏–∑ SQL –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü–∞—Ä—Å–∏–Ω–≥ WHERE —É—Å–ª–æ–≤–∏–π
        conditions = self.parse_where_conditions(sql_query)
        
        # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        best_index = self.choose_best_index(conditions)
        
        # –û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        cost = self.estimate_cost(conditions, best_index)
        
        return {
            'plan': 'index_scan' if best_index else 'table_scan',
            'index': best_index,
            'estimated_cost': cost
        }
    
    def parse_where_conditions(self, sql):
        """–ü–∞—Ä—Å–∏–Ω–≥ WHERE —É—Å–ª–æ–≤–∏–π"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–æ–∂–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
        return []
    
    def choose_best_index(self, conditions):
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        # –ê–Ω–∞–ª–∏–∑ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        return None
    
    def estimate_cost(self, conditions, index):
        """–û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        # –ú–æ–¥–µ–ª—å —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        # - I/O –æ–ø–µ—Ä–∞—Ü–∏–π
        # - CPU –≤—Ä–µ–º—è
        # - –ü–∞–º—è—Ç—å
        return 100
```

### –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤

```python
# ==================== QUERY CACHE ====================
import hashlib
import json

class QueryCache:
    """
    –ö—ç—à –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤: Redis, Memcached, Application Layer
    """
    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_ttl = 300  # 5 –º–∏–Ω—É—Ç
    
    def get_cached_query(self, sql, params):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –∫—ç—à–∞"""
        cache_key = self._generate_cache_key(sql, params)
        
        cached_result = self.redis.get(cache_key)
        if cached_result:
            return json.loads(cached_result)
        
        return None
    
    def cache_query_result(self, sql, params, result, ttl=None):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        cache_key = self._generate_cache_key(sql, params)
        ttl = ttl or self.default_ttl
        
        serialized_result = json.dumps(result, default=str)
        self.redis.setex(cache_key, ttl, serialized_result)
    
    def _generate_cache_key(self, sql, params):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        query_signature = f"{sql}:{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(query_signature.encode()).hexdigest()
    
    def invalidate_cache(self, pattern):
        """–ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        keys = self.redis.keys(f"*{pattern}*")
        if keys:
            self.redis.delete(*keys)

# ==================== DATABASE WRAPPER ====================
class CachedDatabase:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, db_connection, query_cache):
        self.db = db_connection
        self.cache = query_cache
    
    def execute_query(self, sql, params=None, use_cache=True):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        params = params or {}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è SELECT –∑–∞–ø—Ä–æ—Å–æ–≤
        if use_cache and sql.strip().upper().startswith('SELECT'):
            cached_result = self.cache.get_cached_query(sql, params)
            if cached_result:
                return cached_result
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –±–∞–∑–µ
        result = self._execute_raw_query(sql, params)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç SELECT –∑–∞–ø—Ä–æ—Å–æ–≤
        if use_cache and sql.strip().upper().startswith('SELECT'):
            self.cache.cache_query_result(sql, params, result)
        
        # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –¥–ª—è –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if self._is_modifying_query(sql):
            table_name = self._extract_table_name(sql)
            self.cache.invalidate_cache(table_name)
        
        return result
    
    def _execute_raw_query(self, sql, params):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ –±–∞–∑–µ"""
        # –ó–¥–µ—Å—å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π SQL –∑–∞–ø—Ä–æ—Å
        pass
    
    def _is_modifying_query(self, sql):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å"""
        modifying_keywords = ['INSERT', 'UPDATE', 'DELETE', 'CREATE', 'DROP', 'ALTER']
        return any(sql.strip().upper().startswith(keyword) for keyword in modifying_keywords)
    
    def _extract_table_name(self, sql):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ SQL"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π SQL –ø–∞—Ä—Å–µ—Ä
        return 'table'
```

---

## üöÄ –°–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö

### MapReduce –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö

```python
# ==================== MAPREDUCE FRAMEWORK ====================
from multiprocessing import Pool
from collections import defaultdict
import json

class MapReduceFramework:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è MapReduce
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤: Hadoop, Apache Spark
    """
    def __init__(self, num_workers=4):
        self.num_workers = num_workers
    
    def run_job(self, data, mapper, reducer):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ MapReduce job"""
        # –§–∞–∑–∞ Map
        mapped_data = self._map_phase(data, mapper)
        
        # –§–∞–∑–∞ Shuffle & Sort
        shuffled_data = self._shuffle_phase(mapped_data)
        
        # –§–∞–∑–∞ Reduce
        result = self._reduce_phase(shuffled_data, reducer)
        
        return result
    
    def _map_phase(self, data, mapper):
        """Map —Ñ–∞–∑–∞ - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        with Pool(self.num_workers) as pool:
            chunk_size = len(data) // self.num_workers
            chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]
            
            mapped_results = pool.map(mapper, chunks)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_mapped = []
        for result in mapped_results:
            all_mapped.extend(result)
        
        return all_mapped
    
    def _shuffle_phase(self, mapped_data):
        """Shuffle —Ñ–∞–∑–∞ - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª—é—á–∞–º"""
        shuffled = defaultdict(list)
        
        for key, value in mapped_data:
            shuffled[key].append(value)
        
        return shuffled
    
    def _reduce_phase(self, shuffled_data, reducer):
        """Reduce —Ñ–∞–∑–∞ - –∞–≥—Ä–µ–≥–∞—Ü–∏—è"""
        results = {}
        
        for key, values in shuffled_data.items():
            results[key] = reducer(key, values)
        
        return results

# ==================== –ü–†–ò–ú–ï–†: –ü–û–î–°–ß–ï–¢ –°–õ–û–í ====================
def word_count_mapper(text_chunk):
    """Mapper –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å–ª–æ–≤"""
    result = []
    for line in text_chunk:
        words = line.strip().split()
        for word in words:
            result.append((word.lower(), 1))
    return result

def word_count_reducer(key, values):
    """Reducer –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å–ª–æ–≤"""
    return sum(values)

# ==================== –ê–ù–ê–õ–ò–ó –õ–û–ì–û–í ====================
class LogAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤ —Å–µ—Ä–≤–µ—Ä–∞
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ—Ä–∞–±–∞–π—Ç—ã –ª–æ–≥–æ–≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ
    """
    def __init__(self):
        self.mapreduce = MapReduceFramework(num_workers=8)
    
    def analyze_access_logs(self, log_files):
        """–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –¥–æ—Å—Ç—É–ø–∞"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–≥–∏
        logs = self._load_logs(log_files)
        
        # –ü–æ–¥—Å—á–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ IP
        ip_counts = self.mapreduce.run_job(
            logs, 
            self._ip_mapper, 
            self._count_reducer
        )
        
        # –ü–æ–¥—Å—á–µ—Ç –æ—à–∏–±–æ–∫ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        error_timeline = self.mapreduce.run_job(
            logs,
            self._error_mapper,
            self._count_reducer
        )
        
        return {
            'top_ips': self._get_top_n(ip_counts, 10),
            'error_timeline': error_timeline
        }
    
    def _load_logs(self, log_files):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–æ–≤"""
        logs = []
        for file_path in log_files:
            with open(file_path, 'r') as f:
                logs.extend(f.readlines())
        return logs
    
    def _ip_mapper(self, log_chunk):
        """Mapper –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è IP –∞–¥—Ä–µ—Å–æ–≤"""
        result = []
        for line in log_chunk:
            # –ü–∞—Ä—Å–∏–Ω–≥ Apache/Nginx –ª–æ–≥–∞
            parts = line.split()
            if len(parts) > 0:
                ip = parts[0]
                result.append((ip, 1))
        return result
    
    def _error_mapper(self, log_chunk):
        """Mapper –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫"""
        result = []
        for line in log_chunk:
            parts = line.split()
            if len(parts) > 8:
                status_code = parts[8]
                timestamp = parts[3][1:12]  # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å
                
                if status_code.startswith('4') or status_code.startswith('5'):
                    result.append((timestamp, 1))
        return result
    
    def _count_reducer(self, key, values):
        """Reducer –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞"""
        return sum(values)
    
    def _get_top_n(self, data, n):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø N —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        return sorted(data.items(), key=lambda x: x[1], reverse=True)[:n]

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
"""
analyzer = LogAnalyzer()
log_files = ['/var/log/nginx/access.log.1', '/var/log/nginx/access.log.2']
results = analyzer.analyze_access_logs(log_files)

print("–¢–æ–ø IP –∞–¥—Ä–µ—Å–æ–≤:")
for ip, count in results['top_ips']:
    print(f"{ip}: {count} –∑–∞–ø—Ä–æ—Å–æ–≤")
"""
```

---

## ü§ñ Machine Learning

### –ê–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

```python
# ==================== COLLABORATIVE FILTERING ====================
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class RecommendationSystem:
    """
    –°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤: Netflix, Spotify, Amazon, YouTube
    """
    def __init__(self):
        self.user_item_matrix = None
        self.user_similarity = None
        self.item_similarity = None
    
    def fit(self, user_item_matrix):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        self.user_item_matrix = user_item_matrix
        
        # –í—ã—á–∏—Å–ª—è–µ–º similarity –º–∞—Ç—Ä–∏—Ü—ã
        self.user_similarity = cosine_similarity(user_item_matrix)
        self.item_similarity = cosine_similarity(user_item_matrix.T)
    
    def recommend_items(self, user_id, n_recommendations=10):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # Collaborative Filtering –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        user_scores = self._calculate_user_based_scores(user_id)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ
        user_items = self.user_item_matrix[user_id]
        unseen_items = np.where(user_items == 0)[0]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        recommendations = sorted(
            [(item, user_scores[item]) for item in unseen_items],
            key=lambda x: x[1],
            reverse=True
        )
        
        return recommendations[:n_recommendations]
    
    def _calculate_user_based_scores(self, user_id):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ scores –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        similar_users = self.user_similarity[user_id]
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ ratings –æ—Ç –ø–æ—Ö–æ–∂–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        scores = np.zeros(self.user_item_matrix.shape[1])
        
        for item_id in range(len(scores)):
            numerator = 0
            denominator = 0
            
            for other_user_id in range(len(similar_users)):
                if other_user_id != user_id:
                    similarity = similar_users[other_user_id]
                    rating = self.user_item_matrix[other_user_id, item_id]
                    
                    if rating > 0:  # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ü–µ–Ω–∏–ª —ç—Ç–æ—Ç item
                        numerator += similarity * rating
                        denominator += abs(similarity)
            
            scores[item_id] = numerator / denominator if denominator > 0 else 0
        
        return scores

# ==================== CONTENT-BASED FILTERING ====================
from sklearn.feature_extraction.text import TfidfVectorizer

class ContentBasedRecommender:
    """
    –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π, —Ñ–∏–ª—å–º–æ–≤, –º—É–∑—ã–∫–∏
    """
    def __init__(self):
        self.tfidf = TfidfVectorizer(max_features=10000, stop_words='english')
        self.item_features = None
        self.item_similarity = None
    
    def fit(self, items_data):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏—è—Ö items"""
        # items_data = [{'id': 1, 'title': '...', 'description': '...', 'genre': '...'}]
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ item
        item_texts = []
        for item in items_data:
            text = f"{item['title']} {item['description']} {item['genre']}"
            item_texts.append(text)
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
        self.item_features = self.tfidf.fit_transform(item_texts)
        
        # –í—ã—á–∏—Å–ª—è–µ–º similarity –º–µ–∂–¥—É items
        self.item_similarity = cosine_similarity(self.item_features)
    
    def recommend_similar_items(self, item_id, n_recommendations=10):
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ—Ö–æ–∂–∏—Ö items"""
        # –ü–æ–ª—É—á–∞–µ–º similarity scores –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ item
        similarity_scores = self.item_similarity[item_id]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity
        similar_items = sorted(
            enumerate(similarity_scores),
            key=lambda x: x[1],
            reverse=True
        )[1:]  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º item
        
        return similar_items[:n_recommendations]

# ==================== HYBRID RECOMMENDER ====================
class HybridRecommender:
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç collaborative –∏ content-based –ø–æ–¥—Ö–æ–¥—ã
    """
    def __init__(self, collaborative_weight=0.7, content_weight=0.3):
        self.collaborative = RecommendationSystem()
        self.content_based = ContentBasedRecommender()
        self.collaborative_weight = collaborative_weight
        self.content_weight = content_weight
    
    def fit(self, user_item_matrix, items_data):
        """–û–±—É—á–µ–Ω–∏–µ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        self.collaborative.fit(user_item_matrix)
        self.content_based.fit(items_data)
    
    def recommend(self, user_id, n_recommendations=10):
        """–ì–∏–±—Ä–∏–¥–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –æ–±–µ–∏—Ö —Å–∏—Å—Ç–µ–º
        collab_recs = self.collaborative.recommend_items(user_id, n_recommendations * 2)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º scores
        hybrid_scores = {}
        
        for item_id, collab_score in collab_recs:
            # –ü–æ–ª—É—á–∞–µ–º content-based score
            content_score = self._get_content_score(user_id, item_id)
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è
            hybrid_score = (
                self.collaborative_weight * collab_score +
                self.content_weight * content_score
            )
            
            hybrid_scores[item_id] = hybrid_score
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≥–∏–±—Ä–∏–¥–Ω–æ–º—É score
        recommendations = sorted(
            hybrid_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return recommendations[:n_recommendations]
    
    def _get_content_score(self, user_id, item_id):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ content-based score"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        return 0.5
```

---

## üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏

### –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```python
# ==================== STREAM PROCESSING ====================
import heapq
from collections import deque
import time

class StreamProcessor:
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤: Apache Kafka, Apache Storm, AWS Kinesis
    """
    def __init__(self, window_size=60):
        self.window_size = window_size  # —Å–µ–∫—É–Ω–¥—ã
        self.data_buffer = deque()
        self.metrics = {}
    
    def process_event(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
        current_time = time.time()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –≤ –±—É—Ñ–µ—Ä
        self.data_buffer.append({
            'event': event,
            'timestamp': current_time
        })
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Å–æ–±—ã—Ç–∏—è
        self._clean_old_events(current_time)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self._update_metrics()
    
    def _clean_old_events(self, current_time):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –≤–Ω–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞"""
        cutoff_time = current_time - self.window_size
        
        while self.data_buffer and self.data_buffer[0]['timestamp'] < cutoff_time:
            self.data_buffer.popleft()
    
    def _update_metrics(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
        if not self.data_buffer:
            return
        
        # –ü–æ–¥—Å—á–µ—Ç —Å–æ–±—ã—Ç–∏–π –ø–æ —Ç–∏–ø–∞–º
        event_counts = {}
        total_events = 0
        
        for item in self.data_buffer:
            event_type = item['event'].get('type', 'unknown')
            event_counts[event_type] = event_counts.get(event_type, 0) + 1
            total_events += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.metrics.update({
            'total_events': total_events,
            'events_per_type': event_counts,
            'events_per_second': total_events / self.window_size
        })
    
    def get_metrics(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        return self.metrics

# ==================== REAL-TIME ANALYTICS ====================
class RealTimeAnalytics:
    """
    –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    –î–ª—è –¥–∞—à–±–æ—Ä–¥–æ–≤, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞, alerting
    """
    def __init__(self):
        self.processors = {}
        self.alerts = []
    
    def add_stream(self, stream_name, processor):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.processors[stream_name] = processor
    
    def process_log_entry(self, log_entry):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø–∏—Å–∏ –ª–æ–≥–∞"""
        # –ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–∞
        parsed_log = self._parse_log(log_entry)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞—Ö
        for stream_name, processor in self.processors.items():
            if self._should_process(stream_name, parsed_log):
                processor.process_event(parsed_log)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–ª–µ—Ä—Ç—ã
                self._check_alerts(stream_name, processor)
    
    def _parse_log(self, log_entry):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        parts = log_entry.split()
        
        return {
            'type': 'http_request',
            'ip': parts[0] if len(parts) > 0 else 'unknown',
            'status_code': parts[8] if len(parts) > 8 else '200',
            'response_time': float(parts[10]) if len(parts) > 10 else 0,
            'timestamp': time.time()
        }
    
    def _should_process(self, stream_name, event):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –¥–æ–ª–∂–µ–Ω –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–æ–±—ã—Ç–∏–µ"""
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É —Å–æ–±—ã—Ç–∏—è
        if stream_name == 'error_stream':
            return event.get('status_code', '200').startswith(('4', '5'))
        elif stream_name == 'performance_stream':
            return event.get('response_time', 0) > 0
        
        return True
    
    def _check_alerts(self, stream_name, processor):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤"""
        metrics = processor.get_metrics()
        
        # –ê–ª–µ—Ä—Ç –Ω–∞ –≤—ã—Å–æ–∫–∏–π RPS
        if metrics.get('events_per_second', 0) > 1000:
            self.alerts.append({
                'type': 'high_rps',
                'stream': stream_name,
                'value': metrics['events_per_second'],
                'timestamp': time.time()
            })
        
        # –ê–ª–µ—Ä—Ç –Ω–∞ –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫
        if stream_name == 'error_stream':
            error_rate = metrics.get('events_per_second', 0)
            if error_rate > 10:  # –ë–æ–ª–µ–µ 10 –æ—à–∏–±–æ–∫ –≤ —Å–µ–∫—É–Ω–¥—É
                self.alerts.append({
                    'type': 'high_error_rate',
                    'stream': stream_name,
                    'value': error_rate,
                    'timestamp': time.time()
                })
    
    def get_dashboard_data(self):
        """–î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
        dashboard = {}
        
        for stream_name, processor in self.processors.items():
            dashboard[stream_name] = processor.get_metrics()
        
        dashboard['alerts'] = self.alerts[-10:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∞–ª–µ—Ä—Ç–æ–≤
        
        return dashboard

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
"""
analytics = RealTimeAnalytics()

# –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Ç–æ–∫–∏
analytics.add_stream('error_stream', StreamProcessor(window_size=60))
analytics.add_stream('performance_stream', StreamProcessor(window_size=300))

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
log_entry = "192.168.1.1 - - [25/Dec/2023:10:00:00 +0000] GET /api/users 500 1234 0.5"
analytics.process_log_entry(log_entry)

# –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞
dashboard_data = analytics.get_dashboard_data()
"""
```

---

## üåü –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã

```python
# ==================== GUIDE –ü–û –í–´–ë–û–†–£ –ê–õ–ì–û–†–ò–¢–ú–û–í ====================

algorithm_guide = {
    "–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞": {
        "–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ": ["LRU Cache", "Hash Tables"],
        "–ê–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç": ["Trie", "Suffix Array"],
        "Rate Limiting": ["Sliding Window", "Token Bucket"],
        "–ü–æ–∏—Å–∫": ["Elasticsearch", "Inverted Index"]
    },
    
    "–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö": {
        "–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ": ["B-Trees", "Hash Index"],
        "–ó–∞–ø—Ä–æ—Å—ã": ["Query Optimization", "Join Algorithms"],
        "–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ": ["Redis", "Memcached"]
    },
    
    "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ": {
        "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": ["Collaborative Filtering", "Content-Based"],
        "–ü–æ–∏—Å–∫": ["Vector Search", "Approximate Nearest Neighbor"],
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞": ["TF-IDF", "Word Embeddings"]
    },
    
    "–ê–Ω–∞–ª–∏—Ç–∏–∫–∞": {
        "–ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ": ["MapReduce", "Spark"],
        "–ü–æ—Ç–æ–∫–∏": ["Stream Processing", "Time Series"],
        "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥": ["Sliding Window", "Exponential Smoothing"]
    }
}

def choose_algorithm(domain, problem_type):
    """–í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏"""
    if domain in algorithm_guide:
        return algorithm_guide[domain].get(problem_type, ["Generic Solution"])
    return ["Research needed"]

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
print("–î–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –∞–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç–∞:")
print(choose_algorithm("–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "–ê–≤—Ç–æ–∫–æ–º–ø–ª–∏—Ç"))
```

---

*–ê–ª–≥–æ—Ä–∏—Ç–º—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ ‚Äî —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–¥, –∞ —Ä–µ—à–µ–Ω–∏—è –±–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á* üöÄüíº 
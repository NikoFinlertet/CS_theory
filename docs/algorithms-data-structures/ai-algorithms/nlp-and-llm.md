# Natural Language Processing & Large Language Models

> **–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏**
>
> –û—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM

## üî§ –û—Å–Ω–æ–≤—ã NLP

### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

```python
import re
import numpy as np
from collections import Counter, defaultdict
import math
from typing import List, Dict, Tuple, Optional

# ==================== –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø ====================
class TextPreprocessor:
    """
    –ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    """
    
    def __init__(self):
        self.stop_words = {
            'english': {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'},
            'russian': {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–∫', '–æ—Ç', '–¥–ª—è', '–∑–∞', '–ø—Ä–∏', '–Ω–µ', '—á—Ç–æ', '–∫–∞–∫', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–æ', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–±—ã—Ç—å', '–∏–ª–∏', '–Ω–æ', '–¥–∞', '–Ω–µ—Ç', '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '–≤–æ—Ç', '–µ—â–µ', '—É–∂–µ', '—Ç—É—Ç', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—Ç–æ', '—á—Ç–æ', '—á–µ–º', '–ø—Ä–æ', '–ø–æ–¥', '–Ω–∞–¥', '–±–µ–∑', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–æ–∫–æ–ª–æ', '–ø–æ—Å–ª–µ', '–ø–µ—Ä–µ–¥', '–≤–æ', '—Å–æ', '–∏–∑', '–¥–æ', '–∂–µ', '–ª–∏', '–±—ã', '—Ç–æ–ª—å–∫–æ', '–¥–∞–∂–µ', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ', '–µ—Å–ª–∏', '—á—Ç–æ–±—ã', '–ø–æ—Ç–æ–º—É', '–ø–æ—ç—Ç–æ–º—É', '—Ö–æ—Ç—è', '–Ω–µ—Å–º–æ—Ç—Ä—è'}
        }
    
    def tokenize(self, text: str) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (—Å–ª–æ–≤–∞, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
        """
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤
        text = re.sub(r'<[^>]+>', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ email
        text = re.sub(r'\S+@\S+', '', text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ —Å–ª–æ–≤–∞–º –∏ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        tokens = re.findall(r'\b\w+\b|[^\w\s]', text)
        
        return tokens
    
    def remove_stopwords(self, tokens: List[str], language: str = 'english') -> List[str]:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        
        –£–±–∏—Ä–∞–µ—Ç —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –Ω–æ –º–∞–ª–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞
        """
        stop_words = self.stop_words.get(language, set())
        return [token for token in tokens if token not in stop_words]
    
    def stem_words(self, tokens: List[str]) -> List[str]:
        """
        –°—Ç–µ–º–º–∏–Ω–≥ - –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤ –∫ –∫–æ—Ä–Ω—é
        
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–µ–º–º–∏–Ω–≥–∞
        """
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Å—Ç–µ–º–º–∏–Ω–≥–∞ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
        stemming_rules = [
            (r'ing$', ''),
            (r'ed$', ''),
            (r'er$', ''),
            (r'est$', ''),
            (r's$', ''),
            (r'ly$', ''),
            (r'tion$', ''),
            (r'ness$', ''),
            (r'ment$', ''),
        ]
        
        stemmed = []
        for token in tokens:
            for pattern, replacement in stemming_rules:
                if re.search(pattern, token) and len(token) > 3:
                    token = re.sub(pattern, replacement, token)
                    break
            stemmed.append(token)
        
        return stemmed
    
    def clean_text(self, text: str, language: str = 'english', 
                   remove_stops: bool = True, stem: bool = True) -> List[str]:
        """
        –ü–æ–ª–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = self.tokenize(text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        if remove_stops:
            tokens = self.remove_stopwords(tokens, language)
        
        # –°—Ç–µ–º–º–∏–Ω–≥
        if stem:
            tokens = self.stem_words(tokens)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
        tokens = [token for token in tokens if len(token) > 2]
        
        return tokens

# ==================== N-–ì–†–ê–ú–ú–´ ====================
class NGramLanguageModel:
    """
    –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ n-–≥—Ä–∞–º–º
    
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö n-1 —Å–ª–æ–≤
    """
    
    def __init__(self, n: int = 3):
        self.n = n
        self.ngrams = defaultdict(Counter)
        self.vocab = set()
    
    def train(self, texts: List[str]):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤
        """
        preprocessor = TextPreprocessor()
        
        for text in texts:
            tokens = preprocessor.clean_text(text)
            self.vocab.update(tokens)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞
            padded_tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']
            
            # –°–æ–∑–¥–∞–µ–º n-–≥—Ä–∞–º–º—ã
            for i in range(len(padded_tokens) - self.n + 1):
                ngram = tuple(padded_tokens[i:i + self.n])
                context = ngram[:-1]
                next_word = ngram[-1]
                
                self.ngrams[context][next_word] += 1
    
    def get_probability(self, context: Tuple[str, ...], next_word: str) -> float:
        """
        –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        
        P(w_n | w_{n-1}, w_{n-2}, ..., w_1) = Count(w_1...w_n) / Count(w_1...w_{n-1})
        """
        if context not in self.ngrams:
            return 0.0
        
        context_count = sum(self.ngrams[context].values())
        word_count = self.ngrams[context][next_word]
        
        return word_count / context_count if context_count > 0 else 0.0
    
    def get_probability_with_smoothing(self, context: Tuple[str, ...], next_word: str) -> float:
        """
        –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å –ª–∞–ø–ª–∞—Å—Å–æ–≤—ã–º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ–º
        
        –î–æ–±–∞–≤–ª—è–µ—Ç 1 –∫ –∫–∞–∂–¥–æ–º—É —Å—á–µ—Ç—á–∏–∫—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        """
        if context not in self.ngrams:
            return 1.0 / (len(self.vocab) + 1)
        
        context_count = sum(self.ngrams[context].values())
        word_count = self.ngrams[context][next_word]
        vocab_size = len(self.vocab)
        
        return (word_count + 1) / (context_count + vocab_size)
    
    def generate_text(self, context: Tuple[str, ...], max_length: int = 50) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏
        """
        generated = list(context)
        
        for _ in range(max_length):
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞
            if context in self.ngrams:
                candidates = list(self.ngrams[context].keys())
                probabilities = [self.get_probability_with_smoothing(context, word) 
                               for word in candidates]
                
                # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–æ —Å–ª—É—á–∞–π–Ω–æ —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é
                if probabilities:
                    next_word = np.random.choice(candidates, p=probabilities)
                    
                    if next_word == '<END>':
                        break
                    
                    generated.append(next_word)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    context = tuple(generated[-(self.n-1):])
                else:
                    break
            else:
                break
        
        return ' '.join(generated[self.n-1:])  # –£–±–∏—Ä–∞–µ–º START —Ç–æ–∫–µ–Ω—ã
    
    def calculate_perplexity(self, test_texts: List[str]) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏ - –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        
        Perplexity = 2^(-log_2(P(text)))
        –ß–µ–º –º–µ–Ω—å—à–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è, —Ç–µ–º –ª—É—á—à–µ –º–æ–¥–µ–ª—å
        """
        preprocessor = TextPreprocessor()
        total_log_prob = 0.0
        total_words = 0
        
        for text in test_texts:
            tokens = preprocessor.clean_text(text)
            padded_tokens = ['<START>'] * (self.n - 1) + tokens + ['<END>']
            
            for i in range(len(padded_tokens) - self.n + 1):
                ngram = tuple(padded_tokens[i:i + self.n])
                context = ngram[:-1]
                next_word = ngram[-1]
                
                prob = self.get_probability_with_smoothing(context, next_word)
                if prob > 0:
                    total_log_prob += math.log2(prob)
                    total_words += 1
        
        avg_log_prob = total_log_prob / total_words if total_words > 0 else 0
        perplexity = 2 ** (-avg_log_prob)
        
        return perplexity

# ==================== TF-IDF ====================
class TFIDFVectorizer:
    """
    Term Frequency - Inverse Document Frequency
    
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
    """
    
    def __init__(self, max_features: Optional[int] = None):
        self.max_features = max_features
        self.vocabulary = {}
        self.idf_values = {}
        self.preprocessor = TextPreprocessor()
    
    def fit(self, documents: List[str]):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        processed_docs = [self.preprocessor.clean_text(doc) for doc in documents]
        
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        doc_frequencies = Counter()
        all_words = set()
        
        for doc_tokens in processed_docs:
            unique_words = set(doc_tokens)
            for word in unique_words:
                doc_frequencies[word] += 1
                all_words.add(word)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        if self.max_features:
            # –ë–µ—Ä–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
            most_common = doc_frequencies.most_common(self.max_features)
            self.vocabulary = {word: idx for idx, (word, _) in enumerate(most_common)}
        else:
            self.vocabulary = {word: idx for idx, word in enumerate(all_words)}
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IDF
        total_docs = len(documents)
        for word in self.vocabulary:
            df = doc_frequencies[word]
            # IDF = log(N / df) –≥–¥–µ N - –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self.idf_values[word] = math.log(total_docs / df)
    
    def transform(self, documents: List[str]) -> np.ndarray:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä—ã
        """
        tfidf_matrix = np.zeros((len(documents), len(self.vocabulary)))
        
        for doc_idx, document in enumerate(documents):
            tokens = self.preprocessor.clean_text(document)
            
            # –ü–æ–¥—Å—á–µ—Ç TF (term frequency)
            tf_counter = Counter(tokens)
            total_terms = len(tokens)
            
            for word, count in tf_counter.items():
                if word in self.vocabulary:
                    word_idx = self.vocabulary[word]
                    
                    # TF = count / total_terms_in_doc
                    tf = count / total_terms
                    
                    # IDF –¥–ª—è —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞
                    idf = self.idf_values[word]
                    
                    # TF-IDF = TF * IDF
                    tfidf_matrix[doc_idx, word_idx] = tf * idf
        
        return tfidf_matrix
    
    def fit_transform(self, documents: List[str]) -> np.ndarray:
        """
        –û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –æ–¥–Ω–æ–º –º–µ—Ç–æ–¥–µ
        """
        self.fit(documents)
        return self.transform(documents)

# ==================== WORD2VEC (–£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø) ====================
class Word2Vec:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Word2Vec
    
    –û–±—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤
    """
    
    def __init__(self, vector_size: int = 100, window: int = 5, 
                 min_count: int = 1, learning_rate: float = 0.025):
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.learning_rate = learning_rate
        self.vocab = {}
        self.word_vectors = {}
        self.preprocessor = TextPreprocessor()
    
    def build_vocab(self, documents: List[str]):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        """
        word_counts = Counter()
        
        for document in documents:
            tokens = self.preprocessor.clean_text(document)
            word_counts.update(tokens)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞ –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
        filtered_words = {word: count for word, count in word_counts.items() 
                         if count >= self.min_count}
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        self.vocab = {word: idx for idx, word in enumerate(filtered_words.keys())}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤
        vocab_size = len(self.vocab)
        for word in self.vocab:
            self.word_vectors[word] = np.random.uniform(-0.5, 0.5, self.vector_size)
    
    def generate_training_data(self, documents: List[str]) -> List[Tuple[str, str]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ)
        """
        training_data = []
        
        for document in documents:
            tokens = self.preprocessor.clean_text(document)
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è
            tokens = [token for token in tokens if token in self.vocab]
            
            for i, target_word in enumerate(tokens):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                start = max(0, i - self.window)
                end = min(len(tokens), i + self.window + 1)
                
                for j in range(start, end):
                    if i != j:  # –ù–µ –±–µ—Ä–µ–º —Å–∞–º–æ —Å–ª–æ–≤–æ
                        context_word = tokens[j]
                        training_data.append((target_word, context_word))
        
        return training_data
    
    def sigmoid(self, x: float) -> float:
        """–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def train(self, documents: List[str], epochs: int = 5):
        """
        –û–±—É—á–µ–Ω–∏–µ Word2Vec (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Skip-gram)
        """
        self.build_vocab(documents)
        training_data = self.generate_training_data(documents)
        
        for epoch in range(epochs):
            total_loss = 0
            
            for target_word, context_word in training_data:
                # –í–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤
                target_vector = self.word_vectors[target_word]
                context_vector = self.word_vectors[context_word]
                
                # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                dot_product = np.dot(target_vector, context_vector)
                prediction = self.sigmoid(dot_product)
                
                # –û—à–∏–±–∫–∞ (—Ü–µ–ª—å = 1 –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞)
                error = 1 - prediction
                total_loss += -math.log(prediction + 1e-10)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
                gradient = error * self.learning_rate
                self.word_vectors[target_word] += gradient * context_vector
                self.word_vectors[context_word] += gradient * target_vector
            
            if epoch % 1 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}")
    
    def get_similar_words(self, word: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        """
        if word not in self.word_vectors:
            return []
        
        word_vector = self.word_vectors[word]
        similarities = []
        
        for other_word, other_vector in self.word_vectors.items():
            if other_word != word:
                # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                cosine_sim = np.dot(word_vector, other_vector) / (
                    np.linalg.norm(word_vector) * np.linalg.norm(other_vector)
                )
                similarities.append((other_word, cosine_sim))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

# ==================== ATTENTION MECHANISM ====================
class AttentionMechanism:
    """
    –ë–∞–∑–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
    
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, hidden_size: int):
        self.hidden_size = hidden_size
        # –í–µ—Å–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è attention scores
        self.W_attention = np.random.randn(hidden_size, hidden_size) * 0.1
        self.v_attention = np.random.randn(hidden_size, 1) * 0.1
    
    def compute_attention_scores(self, hidden_states: np.ndarray) -> np.ndarray:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ attention scores
        
        hidden_states: [seq_len, hidden_size]
        """
        # –í—ã—á–∏—Å–ª—è–µ–º —ç–Ω–µ—Ä–≥–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        energy = np.tanh(np.dot(hidden_states, self.W_attention))
        
        # –í—ã—á–∏—Å–ª—è–µ–º attention scores
        scores = np.dot(energy, self.v_attention).flatten()
        
        return scores
    
    def apply_attention(self, hidden_states: np.ndarray, 
                       attention_scores: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ attention –∫ —Å–∫—Ä—ã—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏—è–º
        """
        # Softmax –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ scores
        attention_weights = self.softmax(attention_scores)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        context_vector = np.dot(attention_weights, hidden_states)
        
        return context_vector, attention_weights
    
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """Softmax —Ñ—É–Ω–∫—Ü–∏—è"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

# ==================== TRANSFORMER (–£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø) ====================
class MultiHeadAttention:
    """
    Multi-Head Attention –∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Transformer
    """
    
    def __init__(self, d_model: int, n_heads: int):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # –í–µ—Å–∞ –¥–ª—è Query, Key, Value
        self.W_q = np.random.randn(d_model, d_model) * 0.1
        self.W_k = np.random.randn(d_model, d_model) * 0.1
        self.W_v = np.random.randn(d_model, d_model) * 0.1
        self.W_o = np.random.randn(d_model, d_model) * 0.1
    
    def scaled_dot_product_attention(self, Q: np.ndarray, K: np.ndarray, 
                                   V: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Scaled Dot-Product Attention
        
        Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V
        """
        # –í—ã—á–∏—Å–ª—è–µ–º attention scores
        scores = np.dot(Q, K.T) / np.sqrt(self.d_k)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if mask is not None:
            scores = np.where(mask == 0, -np.inf, scores)
        
        # Softmax
        attention_weights = self.softmax(scores)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º attention –∫ values
        output = np.dot(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> np.ndarray:
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Multi-Head Attention
        """
        batch_size, seq_len, d_model = x.shape
        
        # –í—ã—á–∏—Å–ª—è–µ–º Q, K, V
        Q = np.dot(x, self.W_q)
        K = np.dot(x, self.W_k)
        V = np.dot(x, self.W_v)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ heads
        Q = Q.reshape(batch_size, seq_len, self.n_heads, self.d_k).transpose(0, 2, 1, 3)
        K = K.reshape(batch_size, seq_len, self.n_heads, self.d_k).transpose(0, 2, 1, 3)
        V = V.reshape(batch_size, seq_len, self.n_heads, self.d_k).transpose(0, 2, 1, 3)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ head
        attention_outputs = []
        for i in range(self.n_heads):
            head_output, _ = self.scaled_dot_product_attention(
                Q[:, i], K[:, i], V[:, i], mask
            )
            attention_outputs.append(head_output)
        
        # Concatenate heads
        concat_output = np.concatenate(attention_outputs, axis=-1)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        output = np.dot(concat_output, self.W_o)
        
        return output
    
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """Softmax —Ñ—É–Ω–∫—Ü–∏—è"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# ==================== BERT-–ü–û–î–û–ë–ù–ê–Ø –ú–û–î–ï–õ–¨ ====================
class SimpleBERT:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è BERT –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤
    """
    
    def __init__(self, vocab_size: int, hidden_size: int, max_seq_len: int, n_heads: int = 8):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.max_seq_len = max_seq_len
        self.n_heads = n_heads
        
        # Embedding —Å–ª–æ–∏
        self.token_embeddings = np.random.randn(vocab_size, hidden_size) * 0.1
        self.position_embeddings = np.random.randn(max_seq_len, hidden_size) * 0.1
        
        # Multi-Head Attention
        self.attention = MultiHeadAttention(hidden_size, n_heads)
        
        # Feed-Forward Network
        self.ff_W1 = np.random.randn(hidden_size, hidden_size * 4) * 0.1
        self.ff_W2 = np.random.randn(hidden_size * 4, hidden_size) * 0.1
        self.ff_b1 = np.zeros(hidden_size * 4)
        self.ff_b2 = np.zeros(hidden_size)
    
    def get_embeddings(self, token_ids: np.ndarray) -> np.ndarray:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ embeddings –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤
        """
        seq_len = len(token_ids)
        
        # Token embeddings
        token_emb = self.token_embeddings[token_ids]
        
        # Position embeddings
        position_emb = self.position_embeddings[:seq_len]
        
        # –°—É–º–º–∞ embeddings
        embeddings = token_emb + position_emb
        
        return embeddings
    
    def feed_forward(self, x: np.ndarray) -> np.ndarray:
        """
        Feed-Forward Network
        """
        # –ü–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π + ReLU
        hidden = np.dot(x, self.ff_W1) + self.ff_b1
        hidden = np.maximum(0, hidden)  # ReLU
        
        # –í—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        output = np.dot(hidden, self.ff_W2) + self.ff_b2
        
        return output
    
    def forward(self, token_ids: np.ndarray) -> np.ndarray:
        """
        –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ BERT
        """
        # –ü–æ–ª—É—á–∞–µ–º embeddings
        embeddings = self.get_embeddings(token_ids)
        embeddings = embeddings.reshape(1, len(token_ids), self.hidden_size)
        
        # Multi-Head Attention
        attention_output = self.attention.forward(embeddings)
        
        # Residual connection + Layer Norm (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        attention_output = attention_output + embeddings
        
        # Feed-Forward Network
        ff_output = self.feed_forward(attention_output)
        
        # Residual connection + Layer Norm (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        output = ff_output + attention_output
        
        return output.squeeze(0)

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ú–ï–†–´ ====================
def nlp_examples():
    """
    –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã NLP —Ç–µ—Ö–Ω–∏–∫
    """
    
    # –ü—Ä–∏–º–µ—Ä 1: N-gram —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å
    print("=== N-gram —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å ===")
    
    # –û–±—É—á–∞—é—â–∏–µ —Ç–µ–∫—Å—Ç—ã
    training_texts = [
        "The quick brown fox jumps over the lazy dog",
        "A quick brown dog runs in the park",
        "The lazy cat sleeps on the couch",
        "Dogs and cats are pets",
        "The brown fox runs quickly"
    ]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    ngram_model = NGramLanguageModel(n=3)
    ngram_model.train(training_texts)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    context = ('quick', 'brown')
    generated = ngram_model.generate_text(context, max_length=10)
    print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")
    print(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {generated}")
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏
    test_texts = ["The quick brown fox runs"]
    perplexity = ngram_model.calculate_perplexity(test_texts)
    print(f"–ü–µ—Ä–ø–ª–µ–∫—Å–∏—è: {perplexity:.2f}")
    
    # –ü—Ä–∏–º–µ—Ä 2: TF-IDF
    print("\n=== TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è ===")
    
    documents = [
        "The cat sat on the mat",
        "The dog ran in the park",
        "Cats and dogs are pets",
        "I love my pet cat",
        "The park has many dogs"
    ]
    
    tfidf = TFIDFVectorizer(max_features=10)
    tfidf_matrix = tfidf.fit_transform(documents)
    
    print(f"–°–ª–æ–≤–∞—Ä—å: {list(tfidf.vocabulary.keys())}")
    print(f"–§–æ—Ä–º–∞ TF-IDF –º–∞—Ç—Ä–∏—Ü—ã: {tfidf_matrix.shape}")
    print(f"–ü–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç (TF-IDF): {tfidf_matrix[0]}")
    
    # –ü—Ä–∏–º–µ—Ä 3: Word2Vec
    print("\n=== Word2Vec –æ–±—É—á–µ–Ω–∏–µ ===")
    
    w2v = Word2Vec(vector_size=50, window=3, min_count=1, learning_rate=0.025)
    w2v.train(documents, epochs=10)
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤
    if 'cat' in w2v.word_vectors:
        similar_words = w2v.get_similar_words('cat', top_k=3)
        print(f"–°–ª–æ–≤–∞, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ 'cat': {similar_words}")
    
    # –ü—Ä–∏–º–µ—Ä 4: Attention –º–µ—Ö–∞–Ω–∏–∑–º
    print("\n=== Attention –º–µ—Ö–∞–Ω–∏–∑–º ===")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    hidden_states = np.random.randn(5, 8)  # 5 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤, 8 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    attention = AttentionMechanism(hidden_size=8)
    scores = attention.compute_attention_scores(hidden_states)
    context, weights = attention.apply_attention(hidden_states, scores)
    
    print(f"Attention –≤–µ—Å–∞: {weights}")
    print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä: {context}")
    
    return {
        'ngram_perplexity': perplexity,
        'tfidf_matrix': tfidf_matrix,
        'word_vectors': w2v.word_vectors,
        'attention_weights': weights
    }

# ==================== LLM –ö–û–ù–¶–ï–ü–¶–ò–ò ====================
class LLMConcepts:
    """
    –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    """
    
    @staticmethod
    def tokenization_strategies():
        """
        –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è LLM
        """
        return {
            "Word-level": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ —Å–ª–æ–≤–∞–º",
                "–ü–ª—é—Å—ã": "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è",
                "–ú–∏–Ω—É—Å—ã": "–ë–æ–ª—å—à–æ–π —Å–ª–æ–≤–∞—Ä—å, –ø—Ä–æ–±–ª–µ–º—ã —Å OOV"
            },
            
            "Subword (BPE)": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "Byte-Pair Encoding",
                "–ü–ª—é—Å—ã": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–æ–º —Å–ª–æ–≤–∞—Ä—è –∏ –ø–æ–∫—Ä—ã—Ç–∏–µ–º",
                "–ú–∏–Ω—É—Å—ã": "–°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏"
            },
            
            "Character-level": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º",
                "–ü–ª—é—Å—ã": "–ù–µ—Ç –ø—Ä–æ–±–ª–µ–º —Å OOV",
                "–ú–∏–Ω—É—Å—ã": "–î–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
            },
            
            "SentencePiece": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è",
                "–ü–ª—é—Å—ã": "–†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º–∏ —è–∑—ã–∫–∞–º–∏",
                "–ú–∏–Ω—É—Å—ã": "–¢—Ä–µ–±—É–µ—Ç –æ–±—É—á–µ–Ω–∏—è"
            }
        }
    
    @staticmethod
    def training_paradigms():
        """
        –ü–∞—Ä–∞–¥–∏–≥–º—ã –æ–±—É—á–µ–Ω–∏—è LLM
        """
        return {
            "Autoregressive": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞",
                "–ü—Ä–∏–º–µ—Ä—ã": "GPT, PaLM, LLaMA",
                "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–û–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è"
            },
            
            "Masked Language Modeling": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤",
                "–ü—Ä–∏–º–µ—Ä—ã": "BERT, RoBERTa, DeBERTa",
                "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"
            },
            
            "Encoder-Decoder": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
                "–ü—Ä–∏–º–µ—Ä—ã": "T5, BART, mT5",
                "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á"
            },
            
            "Instruction Following": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º",
                "–ü—Ä–∏–º–µ—Ä—ã": "InstructGPT, ChatGPT, Claude",
                "–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "Alignment —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏"
            }
        }
    
    @staticmethod
    def scaling_laws():
        """
        –ó–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è LLM
        """
        return {
            "Kaplan et al.": {
                "–§–æ—Ä–º—É–ª–∞": "Performance ‚àù N^Œ± (–≥–¥–µ N - –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)",
                "–í—ã–≤–æ–¥": "–ë–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ",
                "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è": "–ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö"
            },
            
            "Chinchilla": {
                "–§–æ—Ä–º—É–ª–∞": "Optimal: Parameters ‚àù Data^0.5",
                "–í—ã–≤–æ–¥": "–í–∞–∂–µ–Ω –±–∞–ª–∞–Ω—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –¥–∞–Ω–Ω—ã—Ö",
                "–ü—Ä–∞–∫—Ç–∏–∫–∞": "–ú–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö"
            },
            
            "Emergent Abilities": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–°–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø–æ—è–≤–ª—è—é—Ç—Å—è –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —Ä–∞–∑–º–µ—Ä–µ",
                "–ü—Ä–∏–º–µ—Ä—ã": "In-context learning, chain-of-thought",
                "–ü–æ—Ä–æ–≥": "–û–±—ã—á–Ω–æ >100B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
            }
        }

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –°–û–í–ï–¢–´ ====================
nlp_best_practices = {
    "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞": [
        "–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode",
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏",
        "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –¥–æ–º–µ–Ω–∞",
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"
    ],
    
    "–†–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏": [
        "–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤",
        "–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö",
        "–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–∞—Ö",
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"
    ],
    
    "–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏": [
        "–£—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö",
        "–ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏",
        "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏",
        "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º"
    ],
    
    "–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞": [
        "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏",
        "–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫",
        "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫—Ä–∞–µ–≤—ã—Ö —Å–ª—É—á–∞—è—Ö",
        "–ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞"
    ]
}

# ==================== –°–û–í–†–ï–ú–ï–ù–ù–´–ï –ê–†–•–ò–¢–ï–ö–¢–£–†–´ ====================
modern_architectures = {
    "Transformer": {
        "–ì–æ–¥": 2017,
        "–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è": "Attention is All You Need",
        "–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞": "–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è, –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏",
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è": "–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥, –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"
    },
    
    "BERT": {
        "–ì–æ–¥": 2018,
        "–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è": "Bidirectional Encoder Representations",
        "–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è": "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, NER, Q&A"
    },
    
    "GPT": {
        "–ì–æ–¥": 2018-2023,
        "–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è": "Generative Pre-training",
        "–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞": "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞, few-shot learning",
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è": "–ß–∞—Ç-–±–æ—Ç—ã, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞, —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ"
    },
    
    "T5": {
        "–ì–æ–¥": 2019,
        "–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è": "Text-to-Text Transfer Transformer",
        "–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á",
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è": "–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è, –ø–µ—Ä–µ–≤–æ–¥, Q&A"
    }
}
```

---

*NLP –ø—Ä–µ–≤—Ä–∞—Ç–∏–ª–æ—Å—å –∏–∑ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–µ–π* ü§ñüìù 
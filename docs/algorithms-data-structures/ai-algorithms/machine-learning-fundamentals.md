# Machine Learning Fundamentals

> **–û—Å–Ω–æ–≤—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤**
>
> –û—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –¥–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

## üìä –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã

### –õ–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# ==================== –í–ï–ö–¢–û–†–ù–´–ï –û–ü–ï–†–ê–¶–ò–ò ====================
class LinearAlgebraML:
    """
    –û—Å–Ω–æ–≤—ã –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã –¥–ª—è ML
    """
    
    def vector_operations(self):
        """
        –ë–∞–∑–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        
        –í–µ–∫—Ç–æ—Ä—ã - –æ—Å–Ω–æ–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ ML
        """
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
        a = np.array([1, 2, 3])
        b = np.array([4, 5, 6])
        
        # –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ - –º–µ—Ä–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
        dot_product = np.dot(a, b)  # a¬∑b = 1*4 + 2*5 + 3*6 = 32
        
        # –ù–æ—Ä–º–∞ –≤–µ–∫—Ç–æ—Ä–∞ - –¥–ª–∏–Ω–∞/–≤–µ–ª–∏—á–∏–Ω–∞
        norm_a = np.linalg.norm(a)  # ||a|| = ‚àö(1¬≤ + 2¬≤ + 3¬≤) = ‚àö14
        
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ - —É–≥–æ–ª –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏
        cosine_similarity = dot_product / (np.linalg.norm(a) * np.linalg.norm(b))
        
        # –ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏
        euclidean_distance = np.linalg.norm(a - b)
        
        return {
            'dot_product': dot_product,
            'norm_a': norm_a,
            'cosine_similarity': cosine_similarity,
            'euclidean_distance': euclidean_distance
        }
    
    def matrix_operations(self):
        """
        –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ ML
        
        –ú–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        # –ú–∞—Ç—Ä–∏—Ü–∞ –¥–∞–Ω–Ω—ã—Ö: —Å—Ç—Ä–æ–∫–∏ = –æ–±—Ä–∞–∑—Ü—ã, —Å—Ç–æ–ª–±—Ü—ã = –ø—Ä–∏–∑–Ω–∞–∫–∏
        X = np.array([[1, 2], [3, 4], [5, 6]])  # 3 –æ–±—Ä–∞–∑—Ü–∞, 2 –ø—Ä–∏–∑–Ω–∞–∫–∞
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ - –ø–æ–≤–æ—Ä–æ—Ç –º–∞—Ç—Ä–∏—Ü—ã
        X_T = X.T  # –¢–µ–ø–µ—Ä—å 2 —Å—Ç—Ä–æ–∫–∏, 3 —Å—Ç–æ–ª–±—Ü–∞
        
        # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
        result = np.dot(X_T, X)  # (2x3) √ó (3x2) = (2x2)
        
        # –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –∏ –∑–Ω–∞—á–µ–Ω–∏—è
        eigenvalues, eigenvectors = np.linalg.eig(result)
        
        return {
            'original_shape': X.shape,
            'transposed_shape': X_T.shape,
            'matrix_product': result,
            'eigenvalues': eigenvalues,
            'eigenvectors': eigenvectors
        }

# ==================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –í–ï–†–û–Ø–¢–ù–û–°–¢–¨ ====================
class StatisticsForML:
    """
    –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–ª—è ML
    """
    
    def descriptive_statistics(self, data):
        """
        –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        
        –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        """
        # –ú–µ—Ä—ã —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏
        mean = np.mean(data)           # –°—Ä–µ–¥–Ω–µ–µ - —Ü–µ–Ω—Ç—Ä —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        median = np.median(data)       # –ú–µ–¥–∏–∞–Ω–∞ - —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
        mode = np.bincount(data).argmax()  # –ú–æ–¥–∞ - —Å–∞–º–æ–µ —á–∞—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        
        # –ú–µ—Ä—ã —Ä–∞–∑–±—Ä–æ—Å–∞
        variance = np.var(data)        # –î–∏—Å–ø–µ—Ä—Å–∏—è - —Ä–∞–∑–±—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö
        std_dev = np.std(data)         # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
        range_val = np.max(data) - np.min(data)  # –†–∞–∑–º–∞—Ö
        
        # –ö–≤–∞–Ω—Ç–∏–ª–∏
        q1 = np.percentile(data, 25)   # 1-–π –∫–≤–∞—Ä—Ç–∏–ª—å
        q3 = np.percentile(data, 75)   # 3-–π –∫–≤–∞—Ä—Ç–∏–ª—å
        iqr = q3 - q1                  # –ú–µ–∂–∫–≤–∞—Ä—Ç–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–∞—Ö
        
        return {
            'mean': mean, 'median': median, 'mode': mode,
            'variance': variance, 'std_dev': std_dev, 'range': range_val,
            'q1': q1, 'q3': q3, 'iqr': iqr
        }
    
    def probability_distributions(self):
        """
        –û—Å–Ω–æ–≤–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        """
        # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ - –æ—Å–Ω–æ–≤–∞ –º–Ω–æ–≥–∏—Ö ML –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
        normal_data = np.random.normal(loc=0, scale=1, size=1000)
        
        # –ë–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ - –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        binomial_data = np.random.binomial(n=10, p=0.5, size=1000)
        
        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ - –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
        uniform_data = np.random.uniform(low=0, high=1, size=1000)
        
        return {
            'normal': normal_data,
            'binomial': binomial_data,
            'uniform': uniform_data
        }
    
    def hypothesis_testing(self, sample1, sample2):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑
        
        –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∏–π
        """
        from scipy import stats
        
        # t-—Ç–µ—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–∏—Ö
        t_stat, p_value = stats.ttest_ind(sample1, sample2)
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        alpha = 0.05  # –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        is_significant = p_value < alpha
        
        return {
            't_statistic': t_stat,
            'p_value': p_value,
            'is_significant': is_significant,
            'confidence_level': 1 - alpha
        }

# ==================== –û–°–ù–û–í–ù–´–ï –ê–õ–ì–û–†–ò–¢–ú–´ ML ====================
class SupervisedLearning:
    """
    –ê–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º
    """
    
    def __init__(self):
        self.models = {}
    
    def linear_regression_from_scratch(self, X, y):
        """
        –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –Ω—É–ª—è
        
        –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à—É—é –ø—Ä—è–º—É—é –ª–∏–Ω–∏—é —á–µ—Ä–µ–∑ –¥–∞–Ω–Ω—ã–µ
        –§–æ—Ä–º—É–ª–∞: y = w‚ÇÄ + w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô
        """
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É –µ–¥–∏–Ω–∏—Ü –¥–ª—è —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —á–ª–µ–Ω–∞
        X_with_bias = np.column_stack([np.ones(len(X)), X])
        
        # –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ: w = (X^T * X)^(-1) * X^T * y
        weights = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        def predict(X_new):
            X_new_with_bias = np.column_stack([np.ones(len(X_new)), X_new])
            return X_new_with_bias @ weights
        
        return weights, predict
    
    def gradient_descent_regression(self, X, y, learning_rate=0.01, epochs=1000):
        """
        –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º
        
        –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è - –æ—Å–Ω–æ–≤–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        m, n = X.shape
        weights = np.zeros(n + 1)  # +1 –¥–ª—è bias
        X_with_bias = np.column_stack([np.ones(m), X])
        
        cost_history = []
        
        for epoch in range(epochs):
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            predictions = X_with_bias @ weights
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (MSE)
            cost = np.mean((predictions - y) ** 2)
            cost_history.append(cost)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            gradients = (2 / m) * X_with_bias.T @ (predictions - y)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            weights -= learning_rate * gradients
        
        return weights, cost_history
    
    def logistic_regression_from_scratch(self, X, y, learning_rate=0.01, epochs=1000):
        """
        –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –Ω—É–ª—è
        
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–≥–º–æ–∏–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        """
        # –°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
        def sigmoid(z):
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è
            z = np.clip(z, -500, 500)
            return 1 / (1 + np.exp(-z))
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        m, n = X.shape
        weights = np.zeros(n + 1)
        X_with_bias = np.column_stack([np.ones(m), X])
        
        cost_history = []
        
        for epoch in range(epochs):
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            linear_pred = X_with_bias @ weights
            predictions = sigmoid(linear_pred)
            
            # –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
            epsilon = 1e-15  # –ò–∑–±–µ–≥–∞–µ–º log(0)
            predictions = np.clip(predictions, epsilon, 1 - epsilon)
            cost = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))
            cost_history.append(cost)
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
            gradients = (1 / m) * X_with_bias.T @ (predictions - y)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
            weights -= learning_rate * gradients
        
        def predict_proba(X_new):
            X_new_with_bias = np.column_stack([np.ones(len(X_new)), X_new])
            return sigmoid(X_new_with_bias @ weights)
        
        def predict(X_new):
            return (predict_proba(X_new) >= 0.5).astype(int)
        
        return weights, predict, predict_proba, cost_history

# ==================== –ú–ï–¢–†–ò–ö–ò –ò –û–¶–ï–ù–ö–ê ====================
class ModelEvaluation:
    """
    –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
    """
    
    def regression_metrics(self, y_true, y_pred):
        """
        –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        """
        # –°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
        mae = np.mean(np.abs(y_true - y_pred))
        
        # –°—Ä–µ–¥–Ω—è—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞
        mse = np.mean((y_true - y_pred) ** 2)
        
        # –ö–æ—Ä–µ–Ω—å –∏–∑ —Å—Ä–µ–¥–Ω–µ–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –æ—à–∏–±–∫–∏
        rmse = np.sqrt(mse)
        
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏ R¬≤
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        r2 = 1 - (ss_res / ss_tot)
        
        return {
            'MAE': mae,
            'MSE': mse,
            'RMSE': rmse,
            'R¬≤': r2
        }
    
    def classification_metrics(self, y_true, y_pred):
        """
        –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_true, y_pred)
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_true, y_pred)
        
        # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if len(np.unique(y_true)) == 2:
            tn, fp, fn, tp = cm.ravel()
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'specificity': specificity,
                'f1_score': f1_score,
                'confusion_matrix': cm
            }
        
        return {
            'accuracy': accuracy,
            'confusion_matrix': cm
        }
    
    def cross_validation(self, X, y, model_class, k_folds=5):
        """
        K-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        
        –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        """
        from sklearn.model_selection import KFold
        
        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        fold_scores = []
        
        for train_idx, val_idx in kf.split(X):
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model = model_class()
            model.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
            y_pred = model.predict(X_val)
            score = accuracy_score(y_val, y_pred)
            fold_scores.append(score)
        
        return {
            'mean_score': np.mean(fold_scores),
            'std_score': np.std(fold_scores),
            'fold_scores': fold_scores
        }

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ô –ü–†–ò–ú–ï–† ====================
def ml_pipeline_example():
    """
    –ü–æ–ª–Ω—ã–π –ø—Ä–∏–º–µ—Ä ML –ø–∞–π–ø–ª–∞–π–Ω–∞
    """
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                              n_informative=2, n_clusters_per_class=1, random_state=42)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    sl = SupervisedLearning()
    weights, predict, predict_proba, cost_history = sl.logistic_regression_from_scratch(
        X_train, y_train, learning_rate=0.1, epochs=1000
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = predict(X_test)
    y_pred_proba = predict_proba(X_test)
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    evaluator = ModelEvaluation()
    metrics = evaluator.classification_metrics(y_test, y_pred)
    
    print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:")
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.3f}")
    print(f"Precision: {metrics['precision']:.3f}")
    print(f"Recall: {metrics['recall']:.3f}")
    print(f"F1-score: {metrics['f1_score']:.3f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏—è
    def plot_decision_boundary(X, y, predict_func):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞–Ω–∏—Ü—ã —Ä–µ—à–µ–Ω–∏—è
        """
        h = 0.02
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                            np.arange(y_min, y_max, h))
        
        mesh_points = np.c_[xx.ravel(), yy.ravel()]
        Z = predict_func(mesh_points)
        Z = Z.reshape(xx.shape)
        
        plt.figure(figsize=(10, 8))
        plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)
        scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')
        plt.colorbar(scatter)
        plt.title('–ì—Ä–∞–Ω–∏—Ü–∞ —Ä–µ—à–µ–Ω–∏—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏')
        plt.xlabel('–ü—Ä–∏–∑–Ω–∞–∫ 1')
        plt.ylabel('–ü—Ä–∏–∑–Ω–∞–∫ 2')
        plt.show()
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    plot_decision_boundary(X_test, y_test, predict)
    
    return {
        'weights': weights,
        'metrics': metrics,
        'cost_history': cost_history
    }

# ==================== FEATURE ENGINEERING ====================
class FeatureEngineering:
    """
    –¢–µ—Ö–Ω–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    
    def feature_scaling(self, X):
        """
        –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        –í–∞–∂–Ω–æ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫ –º–∞—Å—à—Ç–∞–±—É
        """
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è (Z-score): Œº=0, œÉ=1
        standardized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (Min-Max): [0, 1]
        normalized = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))
        
        return {
            'standardized': standardized,
            'normalized': normalized
        }
    
    def polynomial_features(self, X, degree=2):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        –£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        from sklearn.preprocessing import PolynomialFeatures
        
        poly = PolynomialFeatures(degree=degree, include_bias=False)
        X_poly = poly.fit_transform(X)
        
        return X_poly, poly.get_feature_names_out()
    
    def handle_missing_values(self, X):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        """
        # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
        strategies = {
            'mean': np.nanmean(X, axis=0),
            'median': np.nanmedian(X, axis=0),
            'mode': np.array([np.bincount(col[~np.isnan(col)]).argmax() 
                             for col in X.T])
        }
        
        return strategies
```

---

## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã ML

### Bias-Variance Tradeoff

```python
# ==================== BIAS-VARIANCE –†–ê–ó–õ–û–ñ–ï–ù–ò–ï ====================
class BiasVarianceAnalysis:
    """
    –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É —Å–º–µ—â–µ–Ω–∏–µ–º –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
    """
    
    def simulate_bias_variance(self, n_experiments=100):
        """
        –°–∏–º—É–ª—è—Ü–∏—è bias-variance tradeoff
        
        Bias: –æ—à–∏–±–∫–∞ –∏–∑-–∑–∞ —É–ø—Ä–æ—â–∞—é—â–∏—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π
        Variance: –æ—à–∏–±–∫–∞ –∏–∑-–∑–∞ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        """
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import LinearRegression
        from sklearn.tree import DecisionTreeRegressor
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏—Å—Ç–∏–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
        def true_function(x):
            return 1.5 * x + 0.3 * x**2 + 0.1 * x**3
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        results = {
            'linear': [],
            'tree_depth_3': [],
            'tree_depth_10': [],
            'random_forest': []
        }
        
        # –ü—Ä–æ–≤–æ–¥–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
        for _ in range(n_experiments):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å —à—É–º–æ–º
            X = np.random.uniform(-2, 2, 100).reshape(-1, 1)
            y = true_function(X.ravel()) + np.random.normal(0, 0.2, 100)
            
            # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            X_test = np.linspace(-2, 2, 50).reshape(-1, 1)
            y_test_true = true_function(X_test.ravel())
            
            # –û–±—É—á–∞–µ–º —Ä–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏
            models = {
                'linear': LinearRegression(),
                'tree_depth_3': DecisionTreeRegressor(max_depth=3),
                'tree_depth_10': DecisionTreeRegressor(max_depth=10),
                'random_forest': RandomForestRegressor(n_estimators=100, max_depth=5)
            }
            
            for name, model in models.items():
                model.fit(X, y)
                y_pred = model.predict(X_test)
                results[name].append(y_pred)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis = {}
        X_test = np.linspace(-2, 2, 50).reshape(-1, 1)
        y_test_true = true_function(X_test.ravel())
        
        for name, predictions in results.items():
            predictions = np.array(predictions)
            
            # –°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            mean_pred = np.mean(predictions, axis=0)
            
            # Bias¬≤ = (—Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - –∏—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)¬≤
            bias_squared = np.mean((mean_pred - y_test_true)**2)
            
            # Variance = —Å—Ä–µ–¥–Ω–µ–µ –æ—Ç (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ - —Å—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)¬≤
            variance = np.mean(np.var(predictions, axis=0))
            
            # Irreducible error (—à—É–º)
            noise = 0.2**2
            
            # Total error = Bias¬≤ + Variance + Noise
            total_error = bias_squared + variance + noise
            
            analysis[name] = {
                'bias_squared': bias_squared,
                'variance': variance,
                'noise': noise,
                'total_error': total_error
            }
        
        return analysis

# ==================== –†–ï–ì–£–õ–Ø–†–ò–ó–ê–¶–ò–Ø ====================
class Regularization:
    """
    –¢–µ—Ö–Ω–∏–∫–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    """
    
    def ridge_regression(self, X, y, alpha=1.0):
        """
        Ridge —Ä–µ–≥—Ä–µ—Å—Å–∏—è (L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
        
        –î–æ–±–∞–≤–ª—è–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞: Loss + Œ± * ||w||¬≤
        """
        # –î–æ–±–∞–≤–ª—è–µ–º bias
        X_with_bias = np.column_stack([np.ones(len(X)), X])
        
        # –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ: w = (X^T*X + Œ±I)^(-1) * X^T * y
        identity = np.eye(X_with_bias.shape[1])
        identity[0, 0] = 0  # –ù–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É–µ–º bias
        
        weights = np.linalg.inv(X_with_bias.T @ X_with_bias + alpha * identity) @ X_with_bias.T @ y
        
        return weights
    
    def lasso_regression(self, X, y, alpha=1.0, max_iter=1000):
        """
        Lasso —Ä–µ–≥—Ä–µ—Å—Å–∏—è (L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
        
        –î–æ–±–∞–≤–ª—è–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: Loss + Œ± * ||w||‚ÇÅ
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è Lasso
        X_with_bias = np.column_stack([np.ones(len(X)), X])
        weights = np.zeros(X_with_bias.shape[1])
        
        for _ in range(max_iter):
            old_weights = weights.copy()
            
            for j in range(len(weights)):
                # –í—ã—á–∏—Å–ª—è–µ–º –æ—Å—Ç–∞—Ç–∫–∏ –±–µ–∑ j-–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
                residuals = y - X_with_bias @ weights + weights[j] * X_with_bias[:, j]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º j-–π –≤–µ—Å
                rho = X_with_bias[:, j] @ residuals
                
                if j == 0:  # Bias –Ω–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑—É–µ–º
                    weights[j] = rho / (X_with_bias[:, j] @ X_with_bias[:, j])
                else:
                    # Soft thresholding
                    z = X_with_bias[:, j] @ X_with_bias[:, j]
                    if rho > alpha:
                        weights[j] = (rho - alpha) / z
                    elif rho < -alpha:
                        weights[j] = (rho + alpha) / z
                    else:
                        weights[j] = 0
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if np.allclose(weights, old_weights, rtol=1e-6):
                break
        
        return weights
    
    def elastic_net(self, X, y, alpha=1.0, l1_ratio=0.5):
        """
        Elastic Net: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è L1 –∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        
        Loss + Œ± * (l1_ratio * ||w||‚ÇÅ + (1-l1_ratio) * ||w||¬≤)
        """
        from sklearn.linear_model import ElasticNet
        
        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)
        model.fit(X, y)
        
        return model.coef_, model.intercept_
```

---

## üìö –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

### –û–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è

```python
# ==================== –°–¢–†–ê–¢–ï–ì–ò–ò –í–ê–õ–ò–î–ê–¶–ò–ò ====================
class ValidationStrategies:
    """
    –†–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
    """
    
    def holdout_validation(self, X, y, test_size=0.2):
        """
        –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        """
        return train_test_split(X, y, test_size=test_size, random_state=42)
    
    def k_fold_cv(self, X, y, k=5):
        """
        K-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        """
        from sklearn.model_selection import KFold
        
        kf = KFold(n_splits=k, shuffle=True, random_state=42)
        return [(train_idx, val_idx) for train_idx, val_idx in kf.split(X)]
    
    def stratified_k_fold(self, X, y, k=5):
        """
        –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –≤ –∫–∞–∂–¥–æ–π fold
        """
        from sklearn.model_selection import StratifiedKFold
        
        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)
        return [(train_idx, val_idx) for train_idx, val_idx in skf.split(X, y)]
    
    def time_series_split(self, X, y, n_splits=5):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
        
        –ù–µ –Ω–∞—Ä—É—à–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        """
        from sklearn.model_selection import TimeSeriesSplit
        
        tss = TimeSeriesSplit(n_splits=n_splits)
        return [(train_idx, val_idx) for train_idx, val_idx in tss.split(X)]

# ==================== ENSEMBLE METHODS ====================
class EnsembleMethods:
    """
    –ú–µ—Ç–æ–¥—ã –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    
    def voting_classifier(self, X, y, X_test):
        """
        –ì–æ–ª–æ—Å—É—é—â–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        from sklearn.ensemble import VotingClassifier
        from sklearn.linear_model import LogisticRegression
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.svm import SVC
        
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        clf1 = LogisticRegression(random_state=42)
        clf2 = DecisionTreeClassifier(random_state=42)
        clf3 = SVC(probability=True, random_state=42)
        
        # –ì–æ–ª–æ—Å—É—é—â–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        voting_clf = VotingClassifier(
            estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)],
            voting='soft'  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        )
        
        voting_clf.fit(X, y)
        return voting_clf.predict(X_test)
    
    def bagging_example(self, X, y):
        """
        Bagging (Bootstrap Aggregating)
        
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞—Ö
        """
        from sklearn.ensemble import BaggingClassifier
        from sklearn.tree import DecisionTreeClassifier
        
        bagging = BaggingClassifier(
            base_estimator=DecisionTreeClassifier(),
            n_estimators=100,
            random_state=42
        )
        
        bagging.fit(X, y)
        return bagging
    
    def boosting_example(self, X, y):
        """
        Boosting
        
        –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏, –∏—Å–ø—Ä–∞–≤–ª—è—è –æ—à–∏–±–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö
        """
        from sklearn.ensemble import AdaBoostClassifier
        from sklearn.tree import DecisionTreeClassifier
        
        ada_boost = AdaBoostClassifier(
            base_estimator=DecisionTreeClassifier(max_depth=1),
            n_estimators=100,
            random_state=42
        )
        
        ada_boost.fit(X, y)
        return ada_boost

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –°–û–í–ï–¢–´ ====================
ml_best_practices = {
    "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö": [
        "–í—Å–µ–≥–¥–∞ –∏—Å—Å–ª–µ–¥—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º",
        "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è",
        "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö",
        "–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã –∏ –∞–Ω–æ–º–∞–ª–∏–∏"
    ],
    
    "–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏": [
        "–ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –ø—Ä–æ—Å—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏",
        "–°—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤",
        "–£—á–∏—Ç—ã–≤–∞–π—Ç–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å"
    ],
    
    "–ò–∑–±–µ–∂–∞–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è": [
        "–ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö > —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é",
        "–†–∞–Ω–Ω–µ–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏",
        "–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ"
    ],
    
    "–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞": [
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫",
        "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫",
        "–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–≥—Ä—É–ø–ø–∞—Ö –¥–∞–Ω–Ω—ã—Ö",
        "–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    ]
}
```

---

*–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö* ü§ñüìä 
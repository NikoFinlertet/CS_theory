# AI Algorithms & Modern Approaches

> **–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞**
>
> –û—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä

## üî¨ –ê–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–∏—Å–∫–∞

### –ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π

```python
import heapq
import numpy as np
from typing import List, Dict, Tuple, Optional, Set
from collections import deque
from dataclasses import dataclass
from abc import ABC, abstractmethod

# ==================== –ë–ê–ó–û–í–´–ï –ê–õ–ì–û–†–ò–¢–ú–´ –ü–û–ò–°–ö–ê ====================
@dataclass
class SearchNode:
    """
    –£–∑–µ–ª –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    state: tuple
    parent: Optional['SearchNode'] = None
    action: Optional[str] = None
    path_cost: float = 0
    heuristic_cost: float = 0
    
    def __lt__(self, other):
        return (self.path_cost + self.heuristic_cost) < (other.path_cost + other.heuristic_cost)

class SearchProblem(ABC):
    """
    –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞
    """
    
    @abstractmethod
    def get_initial_state(self) -> tuple:
        """–ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
        pass
    
    @abstractmethod
    def is_goal(self, state: tuple) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        pass
    
    @abstractmethod
    def get_successors(self, state: tuple) -> List[Tuple[tuple, str, float]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (—Å–ª–µ–¥—É—é—â–µ–µ_—Å–æ—Å—Ç–æ—è–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏–µ, —Å—Ç–æ–∏–º–æ—Å—Ç—å)"""
        pass
    
    @abstractmethod
    def heuristic(self, state: tuple) -> float:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
        pass

class AISearch:
    """
    –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–∏—Å–∫–∞ –ò–ò
    """
    
    def breadth_first_search(self, problem: SearchProblem) -> Optional[List[str]]:
        """
        –ü–æ–∏—Å–∫ –≤ —à–∏—Ä–∏–Ω—É (BFS)
        
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤
        –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: O(b^d)
        –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: O(b^d)
        """
        frontier = deque([SearchNode(problem.get_initial_state())])
        explored = set()
        
        while frontier:
            node = frontier.popleft()
            
            if problem.is_goal(node.state):
                return self._get_path(node)
            
            if node.state in explored:
                continue
                
            explored.add(node.state)
            
            for next_state, action, cost in problem.get_successors(node.state):
                if next_state not in explored:
                    child = SearchNode(
                        state=next_state,
                        parent=node,
                        action=action,
                        path_cost=node.path_cost + cost
                    )
                    frontier.append(child)
        
        return None
    
    def depth_first_search(self, problem: SearchProblem) -> Optional[List[str]]:
        """
        –ü–æ–∏—Å–∫ –≤ –≥–ª—É–±–∏–Ω—É (DFS)
        
        –ú–æ–∂–µ—Ç –Ω–µ –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: O(b^m)
        –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: O(bm)
        """
        frontier = [SearchNode(problem.get_initial_state())]
        explored = set()
        
        while frontier:
            node = frontier.pop()
            
            if problem.is_goal(node.state):
                return self._get_path(node)
            
            if node.state in explored:
                continue
                
            explored.add(node.state)
            
            for next_state, action, cost in problem.get_successors(node.state):
                if next_state not in explored:
                    child = SearchNode(
                        state=next_state,
                        parent=node,
                        action=action,
                        path_cost=node.path_cost + cost
                    )
                    frontier.append(child)
        
        return None
    
    def uniform_cost_search(self, problem: SearchProblem) -> Optional[List[str]]:
        """
        –ü–æ–∏—Å–∫ —Å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é (UCS)
        
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤
        –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω –∞–ª–≥–æ—Ä–∏—Ç–º—É –î–µ–π–∫—Å—Ç—Ä—ã
        """
        frontier = [SearchNode(problem.get_initial_state())]
        explored = set()
        
        while frontier:
            node = heapq.heappop(frontier)
            
            if problem.is_goal(node.state):
                return self._get_path(node)
            
            if node.state in explored:
                continue
                
            explored.add(node.state)
            
            for next_state, action, cost in problem.get_successors(node.state):
                if next_state not in explored:
                    child = SearchNode(
                        state=next_state,
                        parent=node,
                        action=action,
                        path_cost=node.path_cost + cost
                    )
                    heapq.heappush(frontier, child)
        
        return None
    
    def a_star_search(self, problem: SearchProblem) -> Optional[List[str]]:
        """
        A* –ø–æ–∏—Å–∫
        
        –û–ø—Ç–∏–º–∞–ª–µ–Ω –µ—Å–ª–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º–∞—è (–Ω–µ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞–µ—Ç)
        f(n) = g(n) + h(n)
        """
        frontier = [SearchNode(
            state=problem.get_initial_state(),
            heuristic_cost=problem.heuristic(problem.get_initial_state())
        )]
        explored = set()
        
        while frontier:
            node = heapq.heappop(frontier)
            
            if problem.is_goal(node.state):
                return self._get_path(node)
            
            if node.state in explored:
                continue
                
            explored.add(node.state)
            
            for next_state, action, cost in problem.get_successors(node.state):
                if next_state not in explored:
                    child = SearchNode(
                        state=next_state,
                        parent=node,
                        action=action,
                        path_cost=node.path_cost + cost,
                        heuristic_cost=problem.heuristic(next_state)
                    )
                    heapq.heappush(frontier, child)
        
        return None
    
    def _get_path(self, node: SearchNode) -> List[str]:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—É—Ç–∏ –æ—Ç —É–∑–ª–∞ –¥–æ –∫–æ—Ä–Ω—è"""
        path = []
        current = node
        
        while current.parent is not None:
            path.append(current.action)
            current = current.parent
        
        return path[::-1]

# ==================== –ó–ê–î–ê–ß–ê N-–ì–û–õ–û–í–û–õ–û–ú–ö–ò ====================
class NPuzzleProblem(SearchProblem):
    """
    –ó–∞–¥–∞—á–∞ N-–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 8-–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞)
    """
    
    def __init__(self, initial_state: tuple, goal_state: tuple, size: int = 3):
        self.initial_state = initial_state
        self.goal_state = goal_state
        self.size = size
    
    def get_initial_state(self) -> tuple:
        return self.initial_state
    
    def is_goal(self, state: tuple) -> bool:
        return state == self.goal_state
    
    def get_successors(self, state: tuple) -> List[Tuple[tuple, str, float]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        """
        successors = []
        state_list = list(state)
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –ø—É—Å—Ç–æ–π –∫–ª–µ—Ç–∫–∏ (0)
        empty_pos = state_list.index(0)
        row, col = empty_pos // self.size, empty_pos % self.size
        
        # –í–æ–∑–º–æ–∂–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è
        moves = [
            (-1, 0, 'UP'),
            (1, 0, 'DOWN'),
            (0, -1, 'LEFT'),
            (0, 1, 'RIGHT')
        ]
        
        for dr, dc, action in moves:
            new_row, new_col = row + dr, col + dc
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–Ω–∏—Ü
            if 0 <= new_row < self.size and 0 <= new_col < self.size:
                new_empty_pos = new_row * self.size + new_col
                
                # –û–±–º–µ–Ω –º–µ—Å—Ç–∞–º–∏
                new_state = state_list.copy()
                new_state[empty_pos], new_state[new_empty_pos] = new_state[new_empty_pos], new_state[empty_pos]
                
                successors.append((tuple(new_state), action, 1))
        
        return successors
    
    def heuristic(self, state: tuple) -> float:
        """
        –ú–∞–Ω—Ö—ç—Ç—Ç–µ–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –∫–∞–∫ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        
        –°—É–º–º–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –∫–∞–∂–¥–æ–π –∫–ª–µ—Ç–∫–∏ –¥–æ –µ—ë —Ü–µ–ª–µ–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏
        """
        distance = 0
        
        for i, value in enumerate(state):
            if value != 0:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—É—é –∫–ª–µ—Ç–∫—É
                # –¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è
                current_row, current_col = i // self.size, i % self.size
                
                # –¶–µ–ª–µ–≤–∞—è –ø–æ–∑–∏—Ü–∏—è
                target_pos = self.goal_state.index(value)
                target_row, target_col = target_pos // self.size, target_pos % self.size
                
                # –ú–∞–Ω—Ö—ç—Ç—Ç–µ–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                distance += abs(current_row - target_row) + abs(current_col - target_col)
        
        return distance

# ==================== –ì–ï–ù–ï–¢–ò–ß–ï–°–ö–ò–ï –ê–õ–ì–û–†–ò–¢–ú–´ ====================
class GeneticAlgorithm:
    """
    –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
    
    def __init__(self, population_size: int = 100, mutation_rate: float = 0.1,
                 crossover_rate: float = 0.7, elitism: int = 2):
        self.population_size = population_size
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.elitism = elitism
    
    def create_individual(self, gene_length: int) -> List[int]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–π –æ—Å–æ–±–∏
        """
        return [np.random.randint(0, 2) for _ in range(gene_length)]
    
    def create_population(self, gene_length: int) -> List[List[int]]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π –ø–æ–ø—É–ª—è—Ü–∏–∏
        """
        return [self.create_individual(gene_length) for _ in range(self.population_size)]
    
    def fitness_function(self, individual: List[int]) -> float:
        """
        –§—É–Ω–∫—Ü–∏—è –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–æ—Å—Ç–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏)
        
        –ü—Ä–∏–º–µ—Ä: OneMax problem - –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –µ–¥–∏–Ω–∏—Ü
        """
        return sum(individual)
    
    def selection(self, population: List[List[int]], fitness_scores: List[float]) -> List[List[int]]:
        """
        –¢—É—Ä–Ω–∏—Ä–Ω–∞—è —Å–µ–ª–µ–∫—Ü–∏—è
        
        –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏—Ö –æ—Å–æ–±–µ–π –¥–ª—è —Ä–∞–∑–º–Ω–æ–∂–µ–Ω–∏—è
        """
        selected = []
        
        for _ in range(self.population_size):
            # –¢—É—Ä–Ω–∏—Ä –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ª—É—á–∞–π–Ω—ã–º–∏ –æ—Å–æ–±—è–º–∏
            i, j = np.random.choice(len(population), 2, replace=False)
            
            if fitness_scores[i] > fitness_scores[j]:
                selected.append(population[i].copy())
            else:
                selected.append(population[j].copy())
        
        return selected
    
    def crossover(self, parent1: List[int], parent2: List[int]) -> Tuple[List[int], List[int]]:
        """
        –û–¥–Ω–æ—Ç–æ—á–µ—á–Ω–æ–µ —Å–∫—Ä–µ—â–∏–≤–∞–Ω–∏–µ
        """
        if np.random.random() < self.crossover_rate:
            crossover_point = np.random.randint(1, len(parent1))
            
            offspring1 = parent1[:crossover_point] + parent2[crossover_point:]
            offspring2 = parent2[:crossover_point] + parent1[crossover_point:]
            
            return offspring1, offspring2
        else:
            return parent1.copy(), parent2.copy()
    
    def mutate(self, individual: List[int]) -> List[int]:
        """
        –ú—É—Ç–∞—Ü–∏—è —Å –∑–∞–¥–∞–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        """
        mutated = individual.copy()
        
        for i in range(len(mutated)):
            if np.random.random() < self.mutation_rate:
                mutated[i] = 1 - mutated[i]  # –ë–∏—Ç–æ–≤–∞—è –∏–Ω–≤–µ—Ä—Å–∏—è
        
        return mutated
    
    def evolve(self, gene_length: int, max_generations: int = 1000) -> Tuple[List[int], List[float]]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —ç–≤–æ–ª—é—Ü–∏–∏
        """
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω–æ–π –ø–æ–ø—É–ª—è—Ü–∏–∏
        population = self.create_population(gene_length)
        fitness_history = []
        
        for generation in range(max_generations):
            # –û—Ü–µ–Ω–∫–∞ –ø—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–æ—Å—Ç–∏
            fitness_scores = [self.fitness_function(ind) for ind in population]
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            best_fitness = max(fitness_scores)
            fitness_history.append(best_fitness)
            
            # –≠–ª–∏—Ç–∏–∑–º - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –æ—Å–æ–±–µ–π
            elite_indices = np.argsort(fitness_scores)[-self.elitism:]
            elite = [population[i].copy() for i in elite_indices]
            
            # –°–µ–ª–µ–∫—Ü–∏—è
            selected = self.selection(population, fitness_scores)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è
            new_population = elite.copy()
            
            while len(new_population) < self.population_size:
                # –í—ã–±–æ—Ä —Ä–æ–¥–∏—Ç–µ–ª–µ–π
                parent1, parent2 = np.random.choice(len(selected), 2, replace=False)
                
                # –°–∫—Ä–µ—â–∏–≤–∞–Ω–∏–µ
                offspring1, offspring2 = self.crossover(selected[parent1], selected[parent2])
                
                # –ú—É—Ç–∞—Ü–∏—è
                offspring1 = self.mutate(offspring1)
                offspring2 = self.mutate(offspring2)
                
                new_population.extend([offspring1, offspring2])
            
            # –û–±—Ä–µ–∑–∫–∞ –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
            population = new_population[:self.population_size]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
            if best_fitness == gene_length:  # –î–ª—è OneMax problem
                break
        
        # –í–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –ª—É—á—à–µ–π –æ—Å–æ–±–∏
        final_fitness = [self.fitness_function(ind) for ind in population]
        best_individual = population[np.argmax(final_fitness)]
        
        return best_individual, fitness_history

# ==================== PARTICLE SWARM OPTIMIZATION ====================
class ParticleSwarmOptimization:
    """
    –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–æ—è —á–∞—Å—Ç–∏—Ü –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
    
    def __init__(self, num_particles: int = 30, max_iterations: int = 1000,
                 w: float = 0.5, c1: float = 1.5, c2: float = 1.5):
        self.num_particles = num_particles
        self.max_iterations = max_iterations
        self.w = w    # –ò–Ω–µ—Ä—Ü–∏—è
        self.c1 = c1  # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
        self.c2 = c2  # –°–æ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
    
    def objective_function(self, x: np.ndarray) -> float:
        """
        –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è)
        
        –ü—Ä–∏–º–µ—Ä: —Ñ—É–Ω–∫—Ü–∏—è –°—Ñ–µ—Ä—ã f(x) = Œ£(x_i¬≤)
        """
        return np.sum(x**2)
    
    def optimize(self, dimensions: int, bounds: Tuple[float, float]) -> Tuple[np.ndarray, float]:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PSO
        """
        lower_bound, upper_bound = bounds
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞—Å—Ç–∏—Ü
        positions = np.random.uniform(lower_bound, upper_bound, (self.num_particles, dimensions))
        velocities = np.random.uniform(-1, 1, (self.num_particles, dimensions))
        
        # –õ—É—á—à–∏–µ –ø–æ–∑–∏—Ü–∏–∏ —á–∞—Å—Ç–∏—Ü
        personal_best_positions = positions.copy()
        personal_best_scores = np.array([self.objective_function(pos) for pos in positions])
        
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        global_best_index = np.argmin(personal_best_scores)
        global_best_position = personal_best_positions[global_best_index].copy()
        global_best_score = personal_best_scores[global_best_index]
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        history = []
        
        for iteration in range(self.max_iterations):
            for i in range(self.num_particles):
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
                r1, r2 = np.random.random(), np.random.random()
                
                velocity = (self.w * velocities[i] + 
                           self.c1 * r1 * (personal_best_positions[i] - positions[i]) +
                           self.c2 * r2 * (global_best_position - positions[i]))
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏
                position = positions[i] + velocity
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º
                position = np.clip(position, lower_bound, upper_bound)
                
                # –û—Ü–µ–Ω–∫–∞ –Ω–æ–≤–æ–π –ø–æ–∑–∏—Ü–∏–∏
                score = self.objective_function(position)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–µ–π –ª–∏—á–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏
                if score < personal_best_scores[i]:
                    personal_best_positions[i] = position.copy()
                    personal_best_scores[i] = score
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ª—É—á—à–µ–≥–æ
                    if score < global_best_score:
                        global_best_position = position.copy()
                        global_best_score = score
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
                velocities[i] = velocity
                positions[i] = position
            
            history.append(global_best_score)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if iteration % 100 == 0:
                print(f"Iteration {iteration}: Best score = {global_best_score:.6f}")
        
        return global_best_position, global_best_score

# ==================== –°–ò–ú–£–õ–Ø–¶–ò–Ø –û–¢–ñ–ò–ì–ê ====================
class SimulatedAnnealing:
    """
    –ê–ª–≥–æ—Ä–∏—Ç–º —Å–∏–º—É–ª—è—Ü–∏–∏ –æ—Ç–∂–∏–≥–∞
    """
    
    def __init__(self, initial_temp: float = 1000, cooling_rate: float = 0.95,
                 min_temp: float = 1e-8, max_iterations: int = 10000):
        self.initial_temp = initial_temp
        self.cooling_rate = cooling_rate
        self.min_temp = min_temp
        self.max_iterations = max_iterations
    
    def objective_function(self, x: np.ndarray) -> float:
        """
        –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏
        """
        return np.sum(x**2)
    
    def get_neighbor(self, current: np.ndarray, step_size: float = 0.1) -> np.ndarray:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ—Å–µ–¥–Ω–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è
        """
        neighbor = current + np.random.normal(0, step_size, len(current))
        return neighbor
    
    def acceptance_probability(self, current_energy: float, new_energy: float, 
                             temperature: float) -> float:
        """
        –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω—è—Ç–∏—è —Ö—É–¥—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è
        
        P = exp(-(E_new - E_current) / T)
        """
        if new_energy < current_energy:
            return 1.0
        else:
            return np.exp(-(new_energy - current_energy) / temperature)
    
    def optimize(self, dimensions: int, bounds: Tuple[float, float]) -> Tuple[np.ndarray, float]:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º —Å–∏–º—É–ª—è—Ü–∏–∏ –æ—Ç–∂–∏–≥–∞
        """
        lower_bound, upper_bound = bounds
        
        # –ù–∞—á–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        current_solution = np.random.uniform(lower_bound, upper_bound, dimensions)
        current_energy = self.objective_function(current_solution)
        
        # –õ—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ
        best_solution = current_solution.copy()
        best_energy = current_energy
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        temperature = self.initial_temp
        history = []
        
        for iteration in range(self.max_iterations):
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ—Å–µ–¥–Ω–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è
            neighbor = self.get_neighbor(current_solution)
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º
            neighbor = np.clip(neighbor, lower_bound, upper_bound)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–æ–≤–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
            neighbor_energy = self.objective_function(neighbor)
            
            # –†–µ—à–µ–Ω–∏–µ –æ –ø—Ä–∏–Ω—è—Ç–∏–∏
            if (neighbor_energy < current_energy or 
                np.random.random() < self.acceptance_probability(current_energy, neighbor_energy, temperature)):
                
                current_solution = neighbor
                current_energy = neighbor_energy
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è
                if current_energy < best_energy:
                    best_solution = current_solution.copy()
                    best_energy = current_energy
            
            # –û—Ö–ª–∞–∂–¥–µ–Ω–∏–µ
            temperature *= self.cooling_rate
            
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ –Ω–∏–∑–∫–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ
            if temperature < self.min_temp:
                break
            
            history.append(best_energy)
        
        return best_solution, best_energy

# ==================== REINFORCEMENT LEARNING ====================
class QLearningAgent:
    """
    Q-Learning –∞–≥–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º
    """
    
    def __init__(self, num_states: int, num_actions: int, learning_rate: float = 0.1,
                 discount_factor: float = 0.95, epsilon: float = 0.1):
        self.num_states = num_states
        self.num_actions = num_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        
        # Q-—Ç–∞–±–ª–∏—Ü–∞
        self.q_table = np.zeros((num_states, num_actions))
    
    def choose_action(self, state: int) -> int:
        """
        –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
        """
        if np.random.random() < self.epsilon:
            # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            return np.random.randint(self.num_actions)
        else:
            # –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è: –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            return np.argmax(self.q_table[state])
    
    def update_q_value(self, state: int, action: int, reward: float, next_state: int):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è
        
        Q(s,a) = Q(s,a) + Œ± * (r + Œ≥ * max(Q(s',a')) - Q(s,a))
        """
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]
        td_error = td_target - self.q_table[state][action]
        
        self.q_table[state][action] += self.learning_rate * td_error
    
    def train(self, environment, num_episodes: int = 1000):
        """
        –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –≤ —Å—Ä–µ–¥–µ
        """
        episode_rewards = []
        
        for episode in range(num_episodes):
            state = environment.reset()
            total_reward = 0
            done = False
            
            while not done:
                action = self.choose_action(state)
                next_state, reward, done = environment.step(action)
                
                self.update_q_value(state, action, reward, next_state)
                
                state = next_state
                total_reward += reward
            
            episode_rewards.append(total_reward)
            
            # –£–º–µ–Ω—å—à–µ–Ω–∏–µ epsilon (–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ -> —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è)
            self.epsilon = max(0.01, self.epsilon * 0.995)
        
        return episode_rewards

# ==================== –ü–†–û–°–¢–ê–Ø –°–†–ï–î–ê –î–õ–Ø RL ====================
class GridWorld:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Å–µ—Ç–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ RL
    """
    
    def __init__(self, size: int = 5, goal_position: Tuple[int, int] = (4, 4)):
        self.size = size
        self.goal_position = goal_position
        self.current_position = (0, 0)
        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # right, left, down, up
    
    def reset(self) -> int:
        """–°–±—Ä–æ—Å —Å—Ä–µ–¥—ã"""
        self.current_position = (0, 0)
        return self.position_to_state(self.current_position)
    
    def position_to_state(self, position: Tuple[int, int]) -> int:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ –Ω–æ–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        return position[0] * self.size + position[1]
    
    def state_to_position(self, state: int) -> Tuple[int, int]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –ø–æ–∑–∏—Ü–∏—é"""
        return (state // self.size, state % self.size)
    
    def step(self, action: int) -> Tuple[int, float, bool]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (next_state, reward, done)
        """
        dx, dy = self.actions[action]
        x, y = self.current_position
        
        new_x = max(0, min(self.size - 1, x + dx))
        new_y = max(0, min(self.size - 1, y + dy))
        
        self.current_position = (new_x, new_y)
        
        # –ù–∞–≥—Ä–∞–¥–∞
        if self.current_position == self.goal_position:
            reward = 10
            done = True
        else:
            reward = -0.1  # –ù–µ–±–æ–ª—å—à–æ–π —à—Ç—Ä–∞—Ñ –∑–∞ –∫–∞–∂–¥—ã–π —à–∞–≥
            done = False
        
        next_state = self.position_to_state(self.current_position)
        return next_state, reward, done

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ú–ï–†–´ ====================
def ai_algorithms_examples():
    """
    –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
    """
    print("=== –ü—Ä–∏–º–µ—Ä—ã AI –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ===")
    
    # –ü—Ä–∏–º–µ—Ä 1: –†–µ—à–µ–Ω–∏–µ 8-–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∏
    print("\n1. –†–µ—à–µ–Ω–∏–µ 8-–≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∏ —Å A*")
    
    initial_state = (1, 2, 3, 4, 5, 6, 7, 8, 0)
    goal_state = (1, 2, 3, 4, 5, 6, 0, 7, 8)
    
    puzzle = NPuzzleProblem(initial_state, goal_state)
    search = AISearch()
    
    solution = search.a_star_search(puzzle)
    print(f"–†–µ—à–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ: {solution}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {len(solution) if solution else 0}")
    
    # –ü—Ä–∏–º–µ—Ä 2: –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º
    print("\n2. –ì–µ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (OneMax)")
    
    ga = GeneticAlgorithm(population_size=50, mutation_rate=0.1)
    best_individual, fitness_history = ga.evolve(gene_length=20, max_generations=100)
    
    print(f"–õ—É—á—à–∞—è –æ—Å–æ–±—å: {best_individual}")
    print(f"–ü—Ä–∏—Å–ø–æ—Å–æ–±–ª–µ–Ω–Ω–æ—Å—Ç—å: {ga.fitness_function(best_individual)}")
    print(f"–ü–æ–∫–æ–ª–µ–Ω–∏–π: {len(fitness_history)}")
    
    # –ü—Ä–∏–º–µ—Ä 3: Particle Swarm Optimization
    print("\n3. Particle Swarm Optimization")
    
    pso = ParticleSwarmOptimization(num_particles=20, max_iterations=200)
    best_position, best_score = pso.optimize(dimensions=2, bounds=(-10, 10))
    
    print(f"–õ—É—á—à–∞—è –ø–æ–∑–∏—Ü–∏—è: {best_position}")
    print(f"–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_score}")
    
    # –ü—Ä–∏–º–µ—Ä 4: Q-Learning
    print("\n4. Q-Learning –≤ GridWorld")
    
    env = GridWorld(size=5, goal_position=(4, 4))
    agent = QLearningAgent(num_states=25, num_actions=4)
    
    rewards = agent.train(env, num_episodes=500)
    
    print(f"–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 —ç–ø–∏–∑–æ–¥–æ–≤: {np.mean(rewards[-100:]):.2f}")
    print(f"Q-—Ç–∞–±–ª–∏—Ü–∞ (–ø–µ—Ä–≤—ã–µ 5 —Å–æ—Å—Ç–æ—è–Ω–∏–π):")
    for i in range(5):
        print(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ {i}: {agent.q_table[i]}")
    
    return {
        'puzzle_solution': solution,
        'ga_fitness': fitness_history,
        'pso_score': best_score,
        'rl_rewards': rewards
    }

# ==================== –°–û–í–†–ï–ú–ï–ù–ù–´–ï –ü–û–î–•–û–î–´ ====================
class ModernAIApproaches:
    """
    –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –≤ AI
    """
    
    @staticmethod
    def few_shot_learning_concepts():
        """
        –ö–æ–Ω—Ü–µ–ø—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤
        """
        return {
            "Meta-Learning": {
                "–ò–¥–µ—è": "–û–±—É—á–µ–Ω–∏–µ —Ç–æ–º—É, –∫–∞–∫ —É—á–∏—Ç—å—Å—è",
                "–ú–µ—Ç–æ–¥—ã": ["MAML", "Prototypical Networks", "Matching Networks"],
                "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ë—ã—Å—Ç—Ä–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º"
            },
            
            "Transfer Learning": {
                "–ò–¥–µ—è": "–ü–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏",
                "–ú–µ—Ç–æ–¥—ã": ["Fine-tuning", "Feature extraction", "Domain adaptation"],
                "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
            },
            
            "Data Augmentation": {
                "–ò–¥–µ—è": "–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ",
                "–ú–µ—Ç–æ–¥—ã": ["Rotation", "Scaling", "Synthetic data generation"],
                "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–£–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–µ–Ω–∏—è –ø—Ä–∏ –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            }
        }
    
    @staticmethod
    def explainable_ai_methods():
        """
        –ú–µ—Ç–æ–¥—ã –æ–±—ä—è—Å–Ω–∏–º–æ–≥–æ –ò–ò
        """
        return {
            "Local Explanations": {
                "LIME": "Local Interpretable Model-agnostic Explanations",
                "SHAP": "SHapley Additive exPlanations",
                "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"
            },
            
            "Global Explanations": {
                "Feature Importance": "–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
                "Partial Dependence": "–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
                "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ü–µ–ª–æ–º"
            },
            
            "Attention Visualization": {
                "Attention Maps": "–ö–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è",
                "Activation Maximization": "–ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏",
                "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π"
            }
        }
    
    @staticmethod
    def ai_safety_considerations():
        """
        –°–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ò–ò
        """
        return {
            "Alignment Problem": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–π –ò–ò —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏",
                "–ú–µ—Ç–æ–¥—ã": ["RLHF", "Constitutional AI", "Value Learning"],
                "–í–∞–∂–Ω–æ—Å—Ç—å": "–ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è AGI"
            },
            
            "Robustness": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –∞—Ç–∞–∫–∞–º –∏ –æ—à–∏–±–∫–∞–º",
                "–ú–µ—Ç–æ–¥—ã": ["Adversarial Training", "Certified Defense", "Randomized Smoothing"],
                "–í–∞–∂–Ω–æ—Å—Ç—å": "–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤ production"
            },
            
            "Fairness": {
                "–û–ø–∏—Å–∞–Ω–∏–µ": "–°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏–∏",
                "–ú–µ—Ç–æ–¥—ã": ["Bias Detection", "Fairness Constraints", "Demographic Parity"],
                "–í–∞–∂–Ω–æ—Å—Ç—å": "–≠—Ç–∏—á–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"
            }
        }

# ==================== –ë–£–î–£–©–ò–ï –ù–ê–ü–†–ê–í–õ–ï–ù–ò–Ø ====================
future_ai_directions = {
    "Neuromorphic Computing": {
        "–û–ø–∏—Å–∞–Ω–∏–µ": "–í—ã—á–∏—Å–ª–µ–Ω–∏—è, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–µ –º–æ–∑–≥",
        "–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞": "–≠–Ω–µ—Ä–≥–æ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º",
        "–ü—Ä–∏–º–µ—Ä—ã": "Intel Loihi, IBM TrueNorth"
    },
    
    "Quantum Machine Learning": {
        "–û–ø–∏—Å–∞–Ω–∏–µ": "ML –Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–∞—Ö",
        "–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞": "–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ",
        "–ü—Ä–∏–º–µ—Ä—ã": "Quantum SVM, Variational Quantum Eigensolver"
    },
    
    "Continual Learning": {
        "–û–ø–∏—Å–∞–Ω–∏–µ": "–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ –∑–∞–±—ã–≤–∞–Ω–∏—è",
        "–ü—Ä–æ–±–ª–µ–º—ã": "Catastrophic forgetting",
        "–ú–µ—Ç–æ–¥—ã": "Elastic Weight Consolidation, Progressive Networks"
    },
    
    "Multimodal AI": {
        "–û–ø–∏—Å–∞–Ω–∏–µ": "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π",
        "–ü—Ä–∏–º–µ—Ä—ã": "Vision-Language models, Audio-Visual learning",
        "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏—è": "–†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞, AR/VR"
    }
}

# ==================== –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –°–û–í–ï–¢–´ ====================
ai_development_best_practices = {
    "–í—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞": [
        "–ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏—Ä–æ–¥—ã –∑–∞–¥–∞—á–∏",
        "–†–∞–∑–º–µ—Ä –∏ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö",
        "–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
        "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏"
    ],
    
    "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è": [
        "–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏",
        "–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è",
        "–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –∏—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞",
        "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è"
    ],
    
    "–ü—Ä–æ–¥–∞–∫—à–Ω": [
        "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ drift –¥–∞–Ω–Ω—ã—Ö",
        "A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
        "–û—Ç–∫–∞—Ç –∫ –ø—Ä–µ–¥—ã–¥—É—â–∏–º –≤–µ—Ä—Å–∏—è–º",
        "–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ç–ª–∞–¥–∫–∞"
    ],
    
    "–≠—Ç–∏–∫–∞": [
        "Bias –≤ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª—è—Ö",
        "–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π",
        "–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö",
        "–°–æ—Ü–∏–∞–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ"
    ]
}
```

---

*–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç - —ç—Ç–æ –Ω–µ –∑–∞–º–µ–Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑—É–º–∞, –∞ –µ–≥–æ —É—Å–∏–ª–µ–Ω–∏–µ* ü§ñüß† 
# Machine Learning –∏ AI/LLM ü§ñ

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
1. [–û—Å–Ω–æ–≤—ã Machine Learning](#–æ—Å–Ω–æ–≤—ã-machine-learning)
2. [–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏](#–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ-—Å–µ—Ç–∏)
3. [Deep Learning](#deep-learning)
4. [Natural Language Processing](#natural-language-processing)
5. [Large Language Models](#large-language-models)
6. [Computer Vision](#computer-vision)
7. [Reinforcement Learning](#reinforcement-learning)

---

## –û—Å–Ω–æ–≤—ã Machine Learning

### Supervised Learning
```python
# –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –Ω—É–ª—è
import numpy as np
import matplotlib.pyplot as plt

class LinearRegression:
    """–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º"""
    
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
        for i in range(self.iterations):
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: y_pred = X * w + b
            y_pred = np.dot(X, self.weights) + self.bias
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ (MSE)
            cost = (1 / (2 * n_samples)) * np.sum((y_pred - y) ** 2)
            self.cost_history.append(cost)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        return np.dot(X, self.weights) + self.bias
    
    def r2_score(self, y_true, y_pred):
        """–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∞—Ü–∏–∏ R¬≤"""
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        return 1 - (ss_res / ss_tot)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def demo_linear_regression():
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    np.random.seed(42)
    X = np.random.randn(100, 1)
    y = 2 * X.squeeze() + 1 + np.random.randn(100) * 0.1
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = LinearRegression(learning_rate=0.01, iterations=1000)
    model.fit(X, y)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    y_pred = model.predict(X)
    r2 = model.r2_score(y, y_pred)
    
    print(f"–í–µ—Å–∞: {model.weights}")
    print(f"–°–º–µ—â–µ–Ω–∏–µ: {model.bias}")
    print(f"R¬≤ score: {r2}")
```

### Logistic Regression
```python
class LogisticRegression:
    """–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, learning_rate=0.01, iterations=1000):
        self.learning_rate = learning_rate
        self.iterations = iterations
        self.weights = None
        self.bias = None
    
    def _sigmoid(self, z):
        """–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"""
        # –ò–∑–±–µ–≥–∞–µ–º –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        for i in range(self.iterations):
            # –õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å: z = X * w + b
            z = np.dot(X, self.weights) + self.bias
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: p = sigmoid(z)
            y_pred = self._sigmoid(z)
            
            # –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
            cost = (-1 / n_samples) * np.sum(y * np.log(y_pred + 1e-8) + 
                                           (1 - y) * np.log(1 - y_pred + 1e-8))
            
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤"""
        z = np.dot(X, self.weights) + self.bias
        y_pred = self._sigmoid(z)
        return [1 if p > 0.5 else 0 for p in y_pred]
    
    def predict_proba(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π"""
        z = np.dot(X, self.weights) + self.bias
        return self._sigmoid(z)
```

### Decision Trees
```python
class DecisionTree:
    """–ü—Ä–æ—Å—Ç–æ–µ –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π"""
    
    def __init__(self, max_depth=5, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None
    
    def _gini_impurity(self, y):
        """–ò–Ω–¥–µ–∫—Å –î–∂–∏–Ω–∏ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏"""
        classes = np.unique(y)
        impurity = 1.0
        
        for cls in classes:
            p = np.sum(y == cls) / len(y)
            impurity -= p ** 2
        
        return impurity
    
    def _information_gain(self, X_column, y, threshold):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –≤—ã–∏–≥—Ä—ã—à –æ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"""
        left_mask = X_column <= threshold
        right_mask = ~left_mask
        
        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
            return 0
        
        n = len(y)
        left_weight = np.sum(left_mask) / n
        right_weight = np.sum(right_mask) / n
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –ø—Ä–∏–º–µ—Å–µ–π –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        gain = (self._gini_impurity(y) - 
                left_weight * self._gini_impurity(y[left_mask]) - 
                right_weight * self._gini_impurity(y[right_mask]))
        
        return gain
    
    def _best_split(self, X, y):
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è"""
        best_gain = 0
        best_feature = None
        best_threshold = None
        
        n_features = X.shape[1]
        
        for feature_idx in range(n_features):
            column = X[:, feature_idx]
            thresholds = np.unique(column)
            
            for threshold in thresholds:
                gain = self._information_gain(column, y, threshold)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
        
        return best_feature, best_threshold
    
    def _build_tree(self, X, y, depth=0):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞"""
        n_samples = len(y)
        
        # –£—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        if (depth >= self.max_depth or 
            n_samples < self.min_samples_split or 
            len(np.unique(y)) == 1):
            
            # –õ–∏—Å—Ç–æ–≤–æ–π —É–∑–µ–ª - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å
            return np.bincount(y).argmax()
        
        # –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        feature, threshold = self._best_split(X, y)
        
        if feature is None:
            return np.bincount(y).argmax()
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        left_mask = X[:, feature] <= threshold
        right_mask = ~left_mask
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–µ–≤—å–µ–≤
        node = {
            'feature': feature,
            'threshold': threshold,
            'left': self._build_tree(X[left_mask], y[left_mask], depth + 1),
            'right': self._build_tree(X[right_mask], y[right_mask], depth + 1)
        }
        
        return node
    
    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤–∞"""
        self.tree = self._build_tree(X, y)
    
    def _predict_sample(self, sample):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞"""
        node = self.tree
        
        while isinstance(node, dict):
            if sample[node['feature']] <= node['threshold']:
                node = node['left']
            else:
                node = node['right']
        
        return node
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –º–∞—Å—Å–∏–≤–∞ –æ–±—Ä–∞–∑—Ü–æ–≤"""
        return [self._predict_sample(sample) for sample in X]
```

---

## –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏

### –ü–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω
```python
class Perceptron:
    """–ü—Ä–æ—Å—Ç–æ–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None
    
    def _activation(self, z):
        """–°—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"""
        return np.where(z >= 0, 1, 0)
    
    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞"""
        n_samples, n_features = X.shape
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self.weights = np.random.normal(0, 0.01, n_features)
        self.bias = 0
        
        for epoch in range(self.epochs):
            for i in range(n_samples):
                # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                z = np.dot(X[i], self.weights) + self.bias
                y_pred = self._activation(z)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ (–ø—Ä–∞–≤–∏–ª–æ –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞)
                error = y[i] - y_pred
                self.weights += self.learning_rate * error * X[i]
                self.bias += self.learning_rate * error
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        z = np.dot(X, self.weights) + self.bias
        return self._activation(z)
```

### –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω
```python
class MLP:
    """–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω —Å –æ–±—Ä–∞—Ç–Ω—ã–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º"""
    
    def __init__(self, layers, learning_rate=0.01, epochs=1000):
        self.layers = layers  # [input_size, hidden_size, ..., output_size]
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = []
        self.biases = []
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π
        for i in range(len(layers) - 1):
            # Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            b = np.zeros((1, layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def _sigmoid(self, z):
        """–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def _sigmoid_derivative(self, z):
        """–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Å–∏–≥–º–æ–∏–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
        return z * (1 - z)
    
    def _relu(self, z):
        """ReLU —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"""
        return np.maximum(0, z)
    
    def _relu_derivative(self, z):
        """–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è ReLU"""
        return np.where(z > 0, 1, 0)
    
    def forward(self, X):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ"""
        activations = [X]
        
        for i in range(len(self.weights)):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            
            if i < len(self.weights) - 1:
                # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ - ReLU
                activation = self._relu(z)
            else:
                # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π - —Å–∏–≥–º–æ–∏–¥–∞
                activation = self._sigmoid(z)
            
            activations.append(activation)
        
        return activations
    
    def backward(self, X, y, activations):
        """–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ"""
        m = X.shape[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π
        dW = []
        dB = []
        
        # –û—à–∏–±–∫–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
        delta = activations[-1] - y
        
        for i in range(len(self.weights) - 1, -1, -1):
            # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π
            dw = (1/m) * np.dot(activations[i].T, delta)
            db = (1/m) * np.sum(delta, axis=0, keepdims=True)
            
            dW.insert(0, dw)
            dB.insert(0, db)
            
            if i > 0:
                # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–ª–æ–π
                delta = np.dot(delta, self.weights[i].T)
                
                # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
                if i > 0:  # –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç ReLU
                    delta *= self._relu_derivative(activations[i])
        
        return dW, dB
    
    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ —Å–µ—Ç–∏"""
        for epoch in range(self.epochs):
            # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            activations = self.forward(X)
            
            # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            dW, dB = self.backward(X, y, activations)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–π
            for i in range(len(self.weights)):
                self.weights[i] -= self.learning_rate * dW[i]
                self.biases[i] -= self.learning_rate * dB[i]
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
            if epoch % 100 == 0:
                loss = np.mean((activations[-1] - y) ** 2)
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        activations = self.forward(X)
        return activations[-1]
```

---

## Deep Learning

### Convolutional Neural Network
```python
class ConvolutionalLayer:
    """–°–ª–æ–π —Å–≤–µ—Ä—Ç–∫–∏ –¥–ª—è CNN"""
    
    def __init__(self, input_channels, output_channels, kernel_size, stride=1, padding=0):
        self.input_channels = input_channels
        self.output_channels = output_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        self.weights = np.random.randn(
            output_channels, input_channels, kernel_size, kernel_size
        ) * 0.1
        self.bias = np.zeros(output_channels)
    
    def forward(self, X):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–µ—Ä—Ç–∫–∏"""
        batch_size, channels, height, width = X.shape
        
        # –î–æ–±–∞–≤–ª—è–µ–º padding
        if self.padding > 0:
            X = np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding), 
                          (self.padding, self.padding)), mode='constant')
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
        out_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1
        out_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä
        output = np.zeros((batch_size, self.output_channels, out_height, out_width))
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–≤–µ—Ä—Ç–∫—É
        for b in range(batch_size):
            for f in range(self.output_channels):
                for i in range(out_height):
                    for j in range(out_width):
                        h_start = i * self.stride
                        h_end = h_start + self.kernel_size
                        w_start = j * self.stride
                        w_end = w_start + self.kernel_size
                        
                        # –≠–ª–µ–º–µ–Ω—Ç–∞—Ä–Ω–∞—è —Å–≤–µ—Ä—Ç–∫–∞
                        region = X[b, :, h_start:h_end, w_start:w_end]
                        output[b, f, i, j] = np.sum(region * self.weights[f]) + self.bias[f]
        
        return output
    
    def relu_activation(self, x):
        """ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è"""
        return np.maximum(0, x)

class MaxPoolingLayer:
    """–°–ª–æ–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—É–ª–∏–Ω–≥–∞"""
    
    def __init__(self, pool_size=2, stride=2):
        self.pool_size = pool_size
        self.stride = stride
    
    def forward(self, X):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—É–ª–∏–Ω–≥–∞"""
        batch_size, channels, height, width = X.shape
        
        out_height = (height - self.pool_size) // self.stride + 1
        out_width = (width - self.pool_size) // self.stride + 1
        
        output = np.zeros((batch_size, channels, out_height, out_width))
        
        for b in range(batch_size):
            for c in range(channels):
                for i in range(out_height):
                    for j in range(out_width):
                        h_start = i * self.stride
                        h_end = h_start + self.pool_size
                        w_start = j * self.stride
                        w_end = w_start + self.pool_size
                        
                        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–∫–Ω–µ
                        pool_region = X[b, c, h_start:h_end, w_start:w_end]
                        output[b, c, i, j] = np.max(pool_region)
        
        return output

class SimpleCNN:
    """–ü—Ä–æ—Å—Ç–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å"""
    
    def __init__(self):
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Conv -> ReLU -> MaxPool -> Flatten -> Dense
        self.conv1 = ConvolutionalLayer(1, 32, 3, padding=1)
        self.pool1 = MaxPoolingLayer(2, 2)
        self.flatten_size = None
        self.dense_weights = None
        self.dense_bias = None
    
    def forward(self, X):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤—Å—é —Å–µ—Ç—å"""
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π
        conv_out = self.conv1.forward(X)
        relu_out = self.conv1.relu_activation(conv_out)
        
        # –ü—É–ª–∏–Ω–≥
        pool_out = self.pool1.forward(relu_out)
        
        # –í—ã–ø—Ä—è–º–ª–µ–Ω–∏–µ (flatten)
        batch_size = pool_out.shape[0]
        flattened = pool_out.reshape(batch_size, -1)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
        if self.dense_weights is None:
            self.flatten_size = flattened.shape[1]
            self.dense_weights = np.random.randn(self.flatten_size, 10) * 0.1
            self.dense_bias = np.zeros(10)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π
        dense_out = np.dot(flattened, self.dense_weights) + self.dense_bias
        
        # Softmax –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        exp_scores = np.exp(dense_out - np.max(dense_out, axis=1, keepdims=True))
        probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
        
        return probabilities
```

---

## Natural Language Processing

### Bag of Words
```python
class BagOfWords:
    """–ú–æ–¥–µ–ª—å –º–µ—à–∫–∞ —Å–ª–æ–≤"""
    
    def __init__(self, max_features=1000):
        self.max_features = max_features
        self.vocabulary = {}
        self.word_to_index = {}
        self.index_to_word = {}
    
    def fit(self, documents):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"""
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        word_counts = {}
        for doc in documents:
            words = doc.lower().split()
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        top_words = sorted_words[:self.max_features]
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        for idx, (word, count) in enumerate(top_words):
            self.word_to_index[word] = idx
            self.index_to_word[idx] = word
            self.vocabulary[word] = count
    
    def transform(self, documents):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã"""
        vectors = []
        
        for doc in documents:
            vector = np.zeros(len(self.word_to_index))
            words = doc.lower().split()
            
            for word in words:
                if word in self.word_to_index:
                    vector[self.word_to_index[word]] += 1
            
            vectors.append(vector)
        
        return np.array(vectors)
    
    def fit_transform(self, documents):
        """–û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"""
        self.fit(documents)
        return self.transform(documents)

class TFIDFVectorizer:
    """TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"""
    
    def __init__(self, max_features=1000):
        self.max_features = max_features
        self.vocabulary = {}
        self.idf_values = {}
    
    def fit(self, documents):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ IDF –∑–Ω–∞—á–µ–Ω–∏–π"""
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        word_doc_count = {}
        word_total_count = {}
        
        for doc in documents:
            words = set(doc.lower().split())  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            for word in words:
                word_doc_count[word] = word_doc_count.get(word, 0) + 1
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–π —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        for doc in documents:
            words = doc.lower().split()
            for word in words:
                word_total_count[word] = word_total_count.get(word, 0) + 1
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        sorted_words = sorted(word_total_count.items(), key=lambda x: x[1], reverse=True)
        top_words = sorted_words[:self.max_features]
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ –≤—ã—á–∏—Å–ª—è–µ–º IDF
        for idx, (word, _) in enumerate(top_words):
            self.vocabulary[word] = idx
            # IDF = log(N / df), –≥–¥–µ N - –æ–±—â–µ–µ —á–∏—Å–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, df - —á–∏—Å–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –¥–∞–Ω–Ω—ã–º —Å–ª–æ–≤–æ–º
            self.idf_values[word] = np.log(len(documents) / word_doc_count[word])
    
    def transform(self, documents):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä—ã"""
        vectors = []
        
        for doc in documents:
            vector = np.zeros(len(self.vocabulary))
            words = doc.lower().split()
            word_counts = {}
            
            # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (TF)
            for word in words:
                if word in self.vocabulary:
                    word_counts[word] = word_counts.get(word, 0) + 1
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ TF-IDF
            for word, count in word_counts.items():
                if word in self.vocabulary:
                    tf = count / len(words)  # Term Frequency
                    idf = self.idf_values[word]  # Inverse Document Frequency
                    vector[self.vocabulary[word]] = tf * idf
            
            vectors.append(vector)
        
        return np.array(vectors)
    
    def fit_transform(self, documents):
        """–û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"""
        self.fit(documents)
        return self.transform(documents)
```

### Word Embeddings
```python
class Word2Vec:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Word2Vec (Skip-gram)"""
    
    def __init__(self, vector_size=100, window=5, min_count=1, epochs=5):
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.epochs = epochs
        self.word_to_index = {}
        self.index_to_word = {}
        self.vocabulary = {}
        self.embeddings = None
    
    def build_vocabulary(self, sentences):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"""
        word_counts = {}
        
        for sentence in sentences:
            words = sentence.lower().split()
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        filtered_words = {word: count for word, count in word_counts.items() 
                         if count >= self.min_count}
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        for idx, word in enumerate(filtered_words.keys()):
            self.word_to_index[word] = idx
            self.index_to_word[idx] = word
            self.vocabulary[word] = filtered_words[word]
    
    def generate_training_data(self, sentences):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ)"""
        training_data = []
        
        for sentence in sentences:
            words = sentence.lower().split()
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ
            words = [word for word in words if word in self.word_to_index]
            
            for i, center_word in enumerate(words):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
                start = max(0, i - self.window)
                end = min(len(words), i + self.window + 1)
                
                for j in range(start, end):
                    if i != j:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
                        context_word = words[j]
                        training_data.append((center_word, context_word))
        
        return training_data
    
    def train(self, sentences, learning_rate=0.01):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        self.build_vocabulary(sentences)
        vocab_size = len(self.vocabulary)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü –≤–µ—Å–æ–≤
        # –ú–∞—Ç—Ä–∏—Ü–∞ –≤—Ö–æ–¥–Ω—ã—Ö –≤–µ—Å–æ–≤ (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞)
        self.embeddings = np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size,
                                          (vocab_size, self.vector_size))
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –≤–µ—Å–æ–≤ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞)
        output_weights = np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size,
                                         (vocab_size, self.vector_size))
        
        training_data = self.generate_training_data(sentences)
        
        for epoch in range(self.epochs):
            loss = 0
            for center_word, context_word in training_data:
                center_idx = self.word_to_index[center_word]
                context_idx = self.word_to_index[context_word]
                
                # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                h = self.embeddings[center_idx]  # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
                u = np.dot(output_weights, h)    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
                
                # Softmax
                exp_u = np.exp(u - np.max(u))
                y_pred = exp_u / np.sum(exp_u)
                
                # –¶–µ–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä (one-hot)
                y_true = np.zeros(vocab_size)
                y_true[context_idx] = 1
                
                # –ü–æ—Ç–µ—Ä—è
                loss += -np.log(y_pred[context_idx] + 1e-10)
                
                # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                error = y_pred - y_true
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
                grad_output = np.outer(error, h)
                grad_hidden = np.dot(output_weights.T, error)
                
                output_weights -= learning_rate * grad_output
                self.embeddings[center_idx] -= learning_rate * grad_hidden
            
            if epoch % 1 == 0:
                print(f"Epoch {epoch}, Loss: {loss/len(training_data):.4f}")
    
    def get_vector(self, word):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤–∞"""
        if word in self.word_to_index:
            return self.embeddings[self.word_to_index[word]]
        return None
    
    def most_similar(self, word, topn=5):
        """–ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤"""
        if word not in self.word_to_index:
            return []
        
        word_vector = self.get_vector(word)
        similarities = []
        
        for other_word in self.vocabulary:
            if other_word != word:
                other_vector = self.get_vector(other_word)
                # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity = np.dot(word_vector, other_vector) / (
                    np.linalg.norm(word_vector) * np.linalg.norm(other_vector)
                )
                similarities.append((other_word, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:topn]
```

---

## Large Language Models

### Transformer Architecture
```python
class MultiHeadAttention:
    """–ú–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è"""
    
    def __init__(self, d_model, num_heads):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        # –ú–∞—Ç—Ä–∏—Ü—ã –¥–ª—è Q, K, V
        self.W_q = np.random.randn(d_model, d_model) * 0.1
        self.W_k = np.random.randn(d_model, d_model) * 0.1
        self.W_v = np.random.randn(d_model, d_model) * 0.1
        self.W_o = np.random.randn(d_model, d_model) * 0.1
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """–í–Ω–∏–º–∞–Ω–∏–µ —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É (–¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –∫ –±—É–¥—É—â–∏–º —Ç–æ–∫–µ–Ω–∞–º)
        if mask is not None:
            scores = np.where(mask == 0, -1e9, scores)
        
        # Softmax –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
        attention_weights = self.softmax(scores)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∫ –∑–Ω–∞—á–µ–Ω–∏—è–º
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def softmax(self, x):
        """Softmax —Ñ—É–Ω–∫—Ü–∏—è"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def forward(self, query, key, value, mask=None):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ"""
        batch_size, seq_len, d_model = query.shape
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        Q = np.matmul(query, self.W_q)
        K = np.matmul(key, self.W_k)
        V = np.matmul(value, self.W_v)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è
        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–Ω–∏–º–∞–Ω–∏–µ
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≥–æ–ª–æ–≤—ã
        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(
            batch_size, seq_len, d_model
        )
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        output = np.matmul(attention_output, self.W_o)
        
        return output, attention_weights

class TransformerBlock:
    """–ë–ª–æ–∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout = dropout
        
        # –°–ª–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è
        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)
        
        # Feed-forward —Å–µ—Ç—å
        self.ff_w1 = np.random.randn(d_model, d_ff) * 0.1
        self.ff_b1 = np.zeros(d_ff)
        self.ff_w2 = np.random.randn(d_ff, d_model) * 0.1
        self.ff_b2 = np.zeros(d_model)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–µ–≤
        self.ln1_gamma = np.ones(d_model)
        self.ln1_beta = np.zeros(d_model)
        self.ln2_gamma = np.ones(d_model)
        self.ln2_beta = np.zeros(d_model)
    
    def layer_norm(self, x, gamma, beta, epsilon=1e-6):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è"""
        mean = np.mean(x, axis=-1, keepdims=True)
        var = np.var(x, axis=-1, keepdims=True)
        normalized = (x - mean) / np.sqrt(var + epsilon)
        return gamma * normalized + beta
    
    def feed_forward(self, x):
        """Feed-forward —Å–µ—Ç—å"""
        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π —Å ReLU
        h1 = np.maximum(0, np.matmul(x, self.ff_w1) + self.ff_b1)
        
        # –í—Ç–æ—Ä–æ–π —Å–ª–æ–π
        h2 = np.matmul(h1, self.ff_w2) + self.ff_b2
        
        return h2
    
    def forward(self, x, mask=None):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –±–ª–æ–∫"""
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å–≤—è–∑—å—é
        attn_output, attn_weights = self.multi_head_attention.forward(x, x, x, mask)
        x = self.layer_norm(x + attn_output, self.ln1_gamma, self.ln1_beta)
        
        # Feed-forward —Å –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Å–≤—è–∑—å—é
        ff_output = self.feed_forward(x)
        x = self.layer_norm(x + ff_output, self.ln2_gamma, self.ln2_beta)
        
        return x, attn_weights

class SimpleTransformer:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    
    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_seq_len=512):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.d_ff = d_ff
        self.max_seq_len = max_seq_len
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤
        self.token_embedding = np.random.randn(vocab_size, d_model) * 0.1
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.position_embedding = self.create_positional_encoding(max_seq_len, d_model)
        
        # –°–ª–æ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        self.transformer_blocks = [
            TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)
        ]
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.output_projection = np.random.randn(d_model, vocab_size) * 0.1
    
    def create_positional_encoding(self, max_len, d_model):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        pe = np.zeros((max_len, d_model))
        position = np.arange(0, max_len).reshape(-1, 1)
        
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        
        return pe
    
    def forward(self, input_ids, mask=None):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ"""
        seq_len = input_ids.shape[1]
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤
        token_emb = self.token_embedding[input_ids]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        pos_emb = self.position_embedding[:seq_len]
        x = token_emb + pos_emb
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ —Å–ª–æ–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        attention_weights = []
        for transformer_block in self.transformer_blocks:
            x, attn_weights = transformer_block.forward(x, mask)
            attention_weights.append(attn_weights)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        logits = np.matmul(x, self.output_projection)
        
        return logits, attention_weights
    
    def generate_text(self, input_ids, max_length=50, temperature=1.0):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        generated = input_ids.copy()
        
        for _ in range(max_length):
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            logits, _ = self.forward(generated)
            next_token_logits = logits[0, -1, :] / temperature
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax
            exp_logits = np.exp(next_token_logits - np.max(next_token_logits))
            probs = exp_logits / np.sum(exp_logits)
            
            # –°—ç–º–ø–ª–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
            next_token = np.random.choice(len(probs), p=probs)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            generated = np.concatenate([generated, [[next_token]]], axis=1)
        
        return generated
```

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å BERT-–ø–æ–¥–æ–±–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
```python
class TextClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    
    def __init__(self, vocab_size, num_classes, d_model=256, num_heads=8, num_layers=4):
        self.vocab_size = vocab_size
        self.num_classes = num_classes
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.transformer = SimpleTransformer(
            vocab_size=vocab_size,
            d_model=d_model,
            num_heads=num_heads,
            num_layers=num_layers
        )
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞
        self.classifier = np.random.randn(d_model, num_classes) * 0.1
        self.classifier_bias = np.zeros(num_classes)
    
    def forward(self, input_ids, labels=None):
        """–ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ"""
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        hidden_states, attention_weights = self.transformer.forward(input_ids)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º [CLS] —Ç–æ–∫–µ–Ω (–ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω) –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        cls_hidden = hidden_states[:, 0, :]
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        logits = np.matmul(cls_hidden, self.classifier) + self.classifier_bias
        
        if labels is not None:
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
            loss = self.cross_entropy_loss(logits, labels)
            return logits, loss
        
        return logits
    
    def cross_entropy_loss(self, logits, labels):
        """–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        batch_size = logits.shape[0]
        
        # Softmax
        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))
        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
        
        # –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
        log_probs = np.log(probs + 1e-10)
        loss = -np.sum(log_probs[range(batch_size), labels]) / batch_size
        
        return loss
    
    def predict(self, input_ids):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞"""
        logits = self.forward(input_ids)
        return np.argmax(logits, axis=1)
```

## –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏

- **[–ê–ª–≥–æ—Ä–∏—Ç–º—ã](./algorithms.md)** - –ê–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è ML
- **[–°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö](./data-structures.md)** - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è ML
- **[CS –æ—Å–Ω–æ–≤—ã](./computer-science-fundamentals.md)** - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è ML
- **[–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã](./mathematics-foundations.md)** - –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –¥–ª—è ML

---

*–°–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª: [–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã ‚Üí](./mathematics-foundations.md)* 
# Data Warehousing - –•—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–∞–Ω–Ω—ã—Ö üè≠

## üè† –ù–∞–≤–∏–≥–∞—Ü–∏—è
**[[README|üè† –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö]]** ‚Üí **Data Warehousing**

**–°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã:**
- üêò [[postgresql|PostgreSQL]]
- üçÉ [[mongodb|MongoDB]]  
- üî¥ [[redis|Redis]]
- üèóÔ∏è [[database-design|–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–î]]
- ‚ö° [[sql-optimization|–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î]]

---

## üéØ –ß—Ç–æ —Ç–∞–∫–æ–µ Data Warehousing?

**–•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö** (Data Warehouse) ‚Äî —ç—Ç–æ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –≠—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ —Å–∏—Å—Ç–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π.

### ‚ú® –ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

1. **üìä –ü—Ä–µ–¥–º–µ—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ** - –¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã –ø–æ –±–∏–∑–Ω–µ—Å-–æ–±–ª–∞—Å—Ç—è–º
2. **üîÑ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ** - –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –µ–¥–∏–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
3. **üìà –ù–µ–∏–∑–º–µ–Ω—è–µ–º–æ–µ** - –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω—è—é—Ç—Å—è
4. **‚è∞ –ó–∞–≤–∏—Å—è—â–µ–µ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏** - –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–≤—è–∑–∞–Ω—ã –∫ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –ø–µ—Ä–∏–æ–¥–∞–º

---

## üèóÔ∏è OLTP vs OLAP

### üìù –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | OLTP | OLAP |
|---|---|---|
| **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ** | –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π | –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã |
| **–û–ø–µ—Ä–∞—Ü–∏–∏** | INSERT, UPDATE, DELETE | SELECT (—Å–ª–æ–∂–Ω—ã–µ) |
| **–û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö** | –ù–µ–±–æ–ª—å—à–∏–µ –∑–∞–ø—Ä–æ—Å—ã | –ë–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã |
| **–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏** | –ú–Ω–æ–≥–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö | –ê–Ω–∞–ª–∏—Ç–∏–∫–∏, –º–µ–Ω–µ–¥–∂–µ—Ä—ã |
| **–°—Ç—Ä—É–∫—Ç—É—Ä–∞** | –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è | –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è |
| **–ü—Ä–∏–º–µ—Ä—ã** | PostgreSQL, MySQL | ClickHouse, Snowflake |

```python
# –ü—Ä–∏–º–µ—Ä —Ä–∞–∑–ª–∏—á–∏–π –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö

# OLTP - –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
class OLTPStructure:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ OLTP —Å–∏—Å—Ç–µ–º—ã"""
    orders_table = {
        'id': 'PRIMARY KEY',
        'user_id': 'FOREIGN KEY',
        'created_at': 'TIMESTAMP',
        'status': 'VARCHAR'
    }
    
    order_items_table = {
        'id': 'PRIMARY KEY', 
        'order_id': 'FOREIGN KEY',
        'product_id': 'FOREIGN KEY',
        'quantity': 'INTEGER',
        'price': 'DECIMAL'
    }

# OLAP - –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
class OLAPStructure:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ OLAP —Å–∏—Å—Ç–µ–º—ã"""
    sales_fact_table = {
        'order_id': 'BIGINT',
        'user_id': 'BIGINT',
        'user_name': 'VARCHAR',
        'user_city': 'VARCHAR',
        'user_country': 'VARCHAR',
        'product_id': 'BIGINT',
        'product_name': 'VARCHAR',
        'product_category': 'VARCHAR',
        'quantity': 'INTEGER',
        'unit_price': 'DECIMAL',
        'total_amount': 'DECIMAL',
        'order_date': 'DATE',
        'year': 'INTEGER',
        'quarter': 'INTEGER',
        'month': 'INTEGER'
    }
```

---

## ‚≠ê –°—Ö–µ–º–∞ "–ó–≤–µ–∑–¥–∞" (Star Schema)

### üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Data Warehouse

```python
# services/data-warehouse/architecture.py
from dataclasses import dataclass
from typing import List, Dict, Any
from datetime import datetime
import pandas as pd

@dataclass
class DimensionTable:
    """–¢–∞–±–ª–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏–π"""
    name: str
    primary_key: str
    attributes: List[str]
    slowly_changing_type: int  # SCD Type 1, 2, 3

@dataclass
class FactTable:
    """–¢–∞–±–ª–∏—Ü–∞ —Ñ–∞–∫—Ç–æ–≤"""
    name: str
    dimensions: List[str]  # –í–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏
    measures: List[str]    # –ú–µ—Ç—Ä–∏–∫–∏
    grain: str            # –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏

class StarSchemaBuilder:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ö–µ–º—ã –∑–≤–µ–∑–¥–∞"""
    
    def __init__(self):
        self.dimensions = {}
        self.facts = {}
    
    def add_dimension(self, dimension: DimensionTable):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏–π"""
        self.dimensions[dimension.name] = dimension
    
    def add_fact(self, fact: FactTable):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Ñ–∞–∫—Ç–æ–≤"""
        self.facts[fact.name] = fact
    
    def generate_sql_schema(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL —Å—Ö–µ–º—ã"""
        sql_statements = []
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏–π
        for dim in self.dimensions.values():
            sql = f"""
CREATE TABLE {dim.name} (
    {dim.primary_key} SERIAL PRIMARY KEY,
    {', '.join([f'{attr} VARCHAR(255)' for attr in dim.attributes])},
    valid_from TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    valid_to TIMESTAMP DEFAULT '9999-12-31',
    is_current BOOLEAN DEFAULT TRUE
);
            """
            sql_statements.append(sql.strip())
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü —Ñ–∞–∫—Ç–æ–≤
        for fact in self.facts.values():
            foreign_keys = [f"{dim}_key INTEGER REFERENCES {dim}({self.dimensions[dim].primary_key})" 
                          for dim in fact.dimensions if dim in self.dimensions]
            measures_def = [f"{measure} DECIMAL(15,2)" for measure in fact.measures]
            
            sql = f"""
CREATE TABLE {fact.name} (
    {fact.name}_key SERIAL PRIMARY KEY,
    {', '.join(foreign_keys)},
    {', '.join(measures_def)},
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
            """
            sql_statements.append(sql.strip())
        
        return '\n\n'.join(sql_statements)

# –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ö–µ–º—ã –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∑–∞–∫–∞–∑–æ–≤
schema_builder = StarSchemaBuilder()

# –ò–∑–º–µ—Ä–µ–Ω–∏—è
schema_builder.add_dimension(DimensionTable(
    name="dim_customer",
    primary_key="customer_key",
    attributes=["customer_id", "name", "email", "city", "country", "segment"],
    slowly_changing_type=2
))

schema_builder.add_dimension(DimensionTable(
    name="dim_product",
    primary_key="product_key", 
    attributes=["product_id", "name", "category", "brand", "price_range"],
    slowly_changing_type=1
))

schema_builder.add_dimension(DimensionTable(
    name="dim_date",
    primary_key="date_key",
    attributes=["date", "year", "quarter", "month", "day_of_week", "is_holiday"],
    slowly_changing_type=1
))

# –§–∞–∫—Ç—ã
schema_builder.add_fact(FactTable(
    name="fact_sales",
    dimensions=["dim_customer", "dim_product", "dim_date"],
    measures=["quantity", "unit_price", "total_amount", "discount"],
    grain="–û–¥–∏–Ω –∑–∞–∫–∞–∑ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç"
))

print(schema_builder.generate_sql_schema())
```

---

## üîÑ ETL/ELT –ø—Ä–æ—Ü–µ—Å—Å—ã

### üì• Extract-Transform-Load (ETL)

```python
# services/etl/etl_pipeline.py
import pandas as pd
import psycopg2
from sqlalchemy import create_engine
from typing import Dict, List, Any
import logging
from datetime import datetime, timedelta

class ETLPipeline:
    def __init__(self, source_configs: Dict, target_config: Dict):
        self.source_configs = source_configs
        self.target_engine = create_engine(target_config['connection_string'])
        self.logger = logging.getLogger(__name__)
    
    def extract_from_postgresql(self, config: Dict) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ PostgreSQL"""
        source_engine = create_engine(config['connection_string'])
        
        query = config.get('query', f"SELECT * FROM {config['table']}")
        if 'incremental_column' in config:
            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            last_run = self.get_last_run_timestamp(config['table'])
            query += f" WHERE {config['incremental_column']} > '{last_run}'"
        
        return pd.read_sql(query, source_engine)
    
    def extract_from_mongodb(self, config: Dict) -> pd.DataFrame:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ MongoDB"""
        from pymongo import MongoClient
        
        client = MongoClient(config['connection_string'])
        db = client[config['database']]
        collection = db[config['collection']]
        
        # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        query = {}
        if 'incremental_field' in config:
            last_run = self.get_last_run_timestamp(config['collection'])
            query[config['incremental_field']] = {'$gt': last_run}
        
        cursor = collection.find(query)
        data = list(cursor)
        
        return pd.DataFrame(data)
    
    def transform_sales_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥–∞–∂–∞—Ö"""
        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = df.dropna(subset=['order_id', 'user_id', 'total_amount'])
        df = df[df['total_amount'] > 0]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        df['order_date'] = pd.to_datetime(df['created_at']).dt.date
        df['year'] = pd.to_datetime(df['created_at']).dt.year
        df['month'] = pd.to_datetime(df['created_at']).dt.month
        df['quarter'] = pd.to_datetime(df['created_at']).dt.quarter
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Å—É–º–º –∑–∞–∫–∞–∑–æ–≤
        df['amount_category'] = pd.cut(
            df['total_amount'],
            bins=[0, 50, 200, 500, float('inf')],
            labels=['Small', 'Medium', 'Large', 'Enterprise']
        )
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–Ω—è–º
        daily_sales = df.groupby(['order_date', 'user_id']).agg({
            'total_amount': 'sum',
            'order_id': 'count'
        }).reset_index()
        
        daily_sales.rename(columns={'order_id': 'order_count'}, inplace=True)
        
        return daily_sales
    
    def load_to_warehouse(self, df: pd.DataFrame, table_name: str, if_exists: str = 'append'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        try:
            df.to_sql(
                table_name,
                self.target_engine,
                if_exists=if_exists,
                index=False,
                method='multi',
                chunksize=1000
            )
            
            self.logger.info(f"Loaded {len(df)} rows to {table_name}")
            self.update_last_run_timestamp(table_name)
            
        except Exception as e:
            self.logger.error(f"Error loading data to {table_name}: {e}")
            raise
    
    def run_sales_etl(self):
        """–ó–∞–ø—É—Å–∫ ETL –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥–∞–∂–∞—Ö"""
        try:
            # Extract
            sales_df = self.extract_from_postgresql(self.source_configs['sales'])
            users_df = self.extract_from_postgresql(self.source_configs['users'])
            products_df = self.extract_from_mongodb(self.source_configs['products'])
            
            # Transform
            sales_transformed = self.transform_sales_data(sales_df)
            
            # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            enriched_sales = sales_transformed.merge(
                users_df[['id', 'name', 'email', 'city', 'country']],
                left_on='user_id',
                right_on='id',
                how='left'
            )
            
            # Load
            self.load_to_warehouse(enriched_sales, 'fact_daily_sales')
            
            self.logger.info("Sales ETL completed successfully")
            
        except Exception as e:
            self.logger.error(f"Sales ETL failed: {e}")
            raise
    
    def get_last_run_timestamp(self, table_name: str) -> datetime:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        try:
            query = f"SELECT MAX(etl_timestamp) FROM etl_log WHERE table_name = '{table_name}'"
            result = pd.read_sql(query, self.target_engine)
            last_run = result.iloc[0, 0]
            return last_run if last_run else datetime(1900, 1, 1)
        except:
            return datetime(1900, 1, 1)
    
    def update_last_run_timestamp(self, table_name: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        with self.target_engine.connect() as conn:
            conn.execute(f"""
                INSERT INTO etl_log (table_name, etl_timestamp) 
                VALUES ('{table_name}', '{datetime.now()}')
            """)
```

---

## ‚úÖ Data Quality - –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö

```python
# services/etl/data_quality.py
import pandas as pd
from typing import List, Dict, Any, Callable
import numpy as np

class DataQualityChecker:
    def __init__(self):
        self.rules = {}
        self.results = {}
    
    def add_rule(self, name: str, rule_func: Callable, severity: str = 'ERROR'):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.rules[name] = {
            'function': rule_func,
            'severity': severity
        }
    
    def check_completeness(self, df: pd.DataFrame, required_columns: List[str]) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö"""
        results = {}
        for col in required_columns:
            if col in df.columns:
                null_count = df[col].isnull().sum()
                null_percentage = (null_count / len(df)) * 100
                results[col] = {
                    'null_count': null_count,
                    'null_percentage': null_percentage,
                    'status': 'PASS' if null_percentage < 5 else 'FAIL'
                }
            else:
                results[col] = {'status': 'MISSING_COLUMN'}
        
        return results
    
    def check_uniqueness(self, df: pd.DataFrame, unique_columns: List[str]) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏"""
        results = {}
        for col in unique_columns:
            if col in df.columns:
                duplicate_count = df[col].duplicated().sum()
                results[col] = {
                    'duplicate_count': duplicate_count,
                    'status': 'PASS' if duplicate_count == 0 else 'FAIL'
                }
        
        return results
    
    def check_data_ranges(self, df: pd.DataFrame, range_rules: Dict) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        results = {}
        for col, (min_val, max_val) in range_rules.items():
            if col in df.columns:
                out_of_range = df[(df[col] < min_val) | (df[col] > max_val)]
                results[col] = {
                    'out_of_range_count': len(out_of_range),
                    'status': 'PASS' if len(out_of_range) == 0 else 'FAIL'
                }
        
        return results
    
    def check_referential_integrity(self, df: pd.DataFrame, parent_df: pd.DataFrame, 
                                  foreign_key: str, parent_key: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ—á–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏"""
        orphaned_records = df[~df[foreign_key].isin(parent_df[parent_key])]
        
        return {
            'orphaned_count': len(orphaned_records),
            'orphaned_percentage': (len(orphaned_records) / len(df)) * 100,
            'status': 'PASS' if len(orphaned_records) == 0 else 'FAIL'
        }
    
    def generate_quality_report(self, df: pd.DataFrame) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'row_count': len(df),
            'column_count': len(df.columns),
            'checks': {}
        }
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        report['checks']['completeness'] = self.check_completeness(
            df, ['user_id', 'order_id', 'total_amount']
        )
        
        report['checks']['uniqueness'] = self.check_uniqueness(
            df, ['order_id']
        )
        
        report['checks']['ranges'] = self.check_data_ranges(
            df, {'total_amount': (0, 10000)}
        )
        
        # –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        all_checks = []
        for check_type, checks in report['checks'].items():
            for check_name, result in checks.items():
                if isinstance(result, dict) and 'status' in result:
                    all_checks.append(result['status'])
        
        report['overall_status'] = 'PASS' if all(s == 'PASS' for s in all_checks) else 'FAIL'
        
        return report

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
quality_checker = DataQualityChecker()

# –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–∫–∞–∑–æ–≤
sales_data = pd.read_sql("SELECT * FROM orders", engine)
quality_report = quality_checker.generate_quality_report(sales_data)

print(f"Data quality status: {quality_report['overall_status']}")
```

---

## üìä OLAP –∫—É–±—ã –∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑

### üßÆ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ OLAP –∫—É–±–∞

```python
# services/olap/cube_builder.py
import pandas as pd
import numpy as np
from typing import List, Dict, Any

class OLAPCube:
    def __init__(self, data: pd.DataFrame, dimensions: List[str], measures: List[str]):
        self.data = data
        self.dimensions = dimensions
        self.measures = measures
        self.cube = self._build_cube()
    
    def _build_cube(self) -> Dict:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –∫—É–±–∞"""
        cube = {}
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –∏–∑–º–µ—Ä–µ–Ω–∏–π
        from itertools import combinations
        
        for r in range(len(self.dimensions) + 1):
            for dim_combo in combinations(self.dimensions, r):
                if not dim_combo:
                    # –û–±—â–∏–µ –∏—Ç–æ–≥–∏
                    key = 'TOTAL'
                    cube[key] = self.data[self.measures].sum().to_dict()
                else:
                    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∏–∑–º–µ—Ä–µ–Ω–∏–π
                    grouped = self.data.groupby(list(dim_combo))[self.measures].sum()
                    cube[dim_combo] = grouped.to_dict('index')
        
        return cube
    
    def slice(self, dimension: str, value: Any) -> Dict:
        """–°—Ä–µ–∑ –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é"""
        filtered_data = self.data[self.data[dimension] == value]
        return OLAPCube(
            filtered_data, 
            [d for d in self.dimensions if d != dimension],
            self.measures
        )
    
    def dice(self, filters: Dict[str, Any]) -> Dict:
        """–ö—É–±–∏–∫ (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º)"""
        filtered_data = self.data.copy()
        for dim, value in filters.items():
            if isinstance(value, list):
                filtered_data = filtered_data[filtered_data[dim].isin(value)]
            else:
                filtered_data = filtered_data[filtered_data[dim] == value]
        
        return OLAPCube(filtered_data, self.dimensions, self.measures)
    
    def drill_down(self, dimension: str, parent_value: Any, child_dimension: str) -> Dict:
        """–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è (drill-down)"""
        filtered_data = self.data[self.data[dimension] == parent_value]
        return filtered_data.groupby(child_dimension)[self.measures].sum().to_dict()
    
    def roll_up(self, from_dimension: str, to_dimension: str) -> Dict:
        """–ê–≥—Ä–µ–≥–∞—Ü–∏—è (roll-up)"""
        # –ü—Ä–∏–º–µ—Ä: –æ—Ç –¥–Ω—è –∫ –º–µ—Å—è—Ü—É, –æ—Ç –ø—Ä–æ–¥—É–∫—Ç–∞ –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        return self.data.groupby(to_dimension)[self.measures].sum().to_dict()
    
    def pivot_analysis(self, row_dims: List[str], col_dims: List[str], 
                      measure: str, aggfunc: str = 'sum') -> pd.DataFrame:
        """–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞"""
        return pd.pivot_table(
            self.data,
            values=measure,
            index=row_dims,
            columns=col_dims,
            aggfunc=aggfunc,
            fill_value=0
        )

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–¥–∞–∂
sales_data = pd.read_sql("""
    SELECT 
        o.created_at::date as order_date,
        EXTRACT(year FROM o.created_at) as year,
        EXTRACT(month FROM o.created_at) as month,
        u.city,
        u.country,
        p.category as product_category,
        oi.quantity,
        oi.price,
        oi.quantity * oi.price as revenue
    FROM orders o
    JOIN users u ON o.user_id = u.id
    JOIN order_items oi ON o.id = oi.order_id
    JOIN products p ON oi.product_id = p.id
    WHERE o.created_at >= '2024-01-01'
""", engine)

# –°–æ–∑–¥–∞–Ω–∏–µ OLAP –∫—É–±–∞
cube = OLAPCube(
    sales_data,
    dimensions=['year', 'month', 'city', 'country', 'product_category'],
    measures=['quantity', 'revenue']
)

# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–∞–∂ –ø–æ –≥–æ—Ä–æ–¥–∞–º
city_analysis = cube.pivot_analysis(['city'], ['month'], 'revenue')
print("Revenue by City and Month:")
print(city_analysis)

# –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –≤ –ú–æ—Å–∫–≤–µ
moscow_products = cube.dice({'city': 'Moscow'}).pivot_analysis(
    ['product_category'], ['month'], 'revenue'
)
print("\nMoscow Revenue by Product Category:")
print(moscow_products)
```

---

## üìà ClickHouse –¥–ª—è –ê–Ω–∞–ª–∏—Ç–∏–∫–∏

### ‚ö° Columnstore –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö

```python
# services/analytics/clickhouse_service.py
from clickhouse_driver import Client
import pandas as pd
from typing import Dict, List, Any
import datetime

class ClickHouseAnalytics:
    def __init__(self, host: str = 'localhost', port: int = 9000):
        self.client = Client(host=host, port=port)
    
    def create_analytics_tables(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        
        # –¢–∞–±–ª–∏—Ü–∞ —Å–æ–±—ã—Ç–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        self.client.execute("""
            CREATE TABLE IF NOT EXISTS user_events (
                event_id UUID,
                user_id UInt64,
                event_type LowCardinality(String),
                event_timestamp DateTime,
                page_url String,
                user_agent String,
                ip_address IPv4,
                session_id String,
                properties Map(String, String),
                created_date Date MATERIALIZED toDate(event_timestamp)
            ) ENGINE = MergeTree()
            PARTITION BY toYYYYMM(event_timestamp)
            ORDER BY (user_id, event_timestamp)
            TTL event_timestamp + INTERVAL 2 YEAR
        """)
        
        # –ú–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –¥–Ω–µ–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        self.client.execute("""
            CREATE MATERIALIZED VIEW IF NOT EXISTS daily_user_stats
            ENGINE = SummingMergeTree()
            PARTITION BY toYYYYMM(event_date)
            ORDER BY (event_date, user_id)
            AS SELECT
                toDate(event_timestamp) as event_date,
                user_id,
                count() as event_count,
                uniq(session_id) as session_count,
                countIf(event_type = 'page_view') as page_views,
                countIf(event_type = 'purchase') as purchases
            FROM user_events
            GROUP BY event_date, user_id
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–¥–∞–∂ —Å –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        self.client.execute("""
            CREATE TABLE IF NOT EXISTS sales_fact (
                order_id UInt64,
                user_id UInt64,
                product_id UInt64,
                order_date Date,
                order_timestamp DateTime,
                quantity UInt32,
                unit_price Decimal(10,2),
                total_amount Decimal(10,2),
                discount_amount Decimal(10,2),
                user_city LowCardinality(String),
                user_country LowCardinality(String),
                product_name String,
                product_category LowCardinality(String),
                product_brand LowCardinality(String),
                year UInt16 MATERIALIZED toYear(order_date),
                month UInt8 MATERIALIZED toMonth(order_date),
                day_of_week UInt8 MATERIALIZED toDayOfWeek(order_date)
            ) ENGINE = MergeTree()
            PARTITION BY toYYYYMM(order_date)
            ORDER BY (order_date, user_id, product_id)
        """)
    
    def insert_user_event(self, event_data: Dict[str, Any]):
        """–í—Å—Ç–∞–≤–∫–∞ —Å–æ–±—ã—Ç–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        self.client.execute(
            """
            INSERT INTO user_events 
            (event_id, user_id, event_type, event_timestamp, page_url, 
             user_agent, ip_address, session_id, properties)
            VALUES
            """,
            [event_data]
        )
    
    def get_user_behavior_funnel(self, start_date: str, end_date: str) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ –≤–æ—Ä–æ–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        query = f"""
        SELECT 
            formatDateTime(event_timestamp, '%Y-%m-%d') as date,
            countIf(event_type = 'page_view') as page_views,
            uniq(user_id) as unique_visitors,
            countIf(event_type = 'add_to_cart') as cart_additions,
            countIf(event_type = 'purchase') as purchases,
            countIf(event_type = 'add_to_cart') / uniq(user_id) * 100 as cart_conversion_rate,
            countIf(event_type = 'purchase') / countIf(event_type = 'add_to_cart') * 100 as purchase_conversion_rate
        FROM user_events
        WHERE event_timestamp >= '{start_date}' AND event_timestamp <= '{end_date}'
        GROUP BY date
        ORDER BY date
        """
        
        result = self.client.execute(query)
        columns = ['date', 'page_views', 'unique_visitors', 'cart_additions', 
                  'purchases', 'cart_conversion_rate', 'purchase_conversion_rate']
        
        return pd.DataFrame(result, columns=columns)
    
    def get_cohort_analysis(self, months_back: int = 12) -> pd.DataFrame:
        """–ö–æ–≥–æ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        query = f"""
        WITH first_orders AS (
            SELECT 
                user_id,
                min(order_date) as first_order_date,
                toStartOfMonth(min(order_date)) as cohort_month
            FROM sales_fact
            WHERE order_date >= today() - INTERVAL {months_back} MONTH
            GROUP BY user_id
        ),
        cohort_data AS (
            SELECT 
                f.cohort_month,
                dateDiff('month', f.cohort_month, toStartOfMonth(s.order_date)) as period_number,
                count(DISTINCT s.user_id) as users
            FROM first_orders f
            JOIN sales_fact s ON f.user_id = s.user_id
            WHERE s.order_date >= f.first_order_date
            GROUP BY f.cohort_month, period_number
        ),
        cohort_sizes AS (
            SELECT 
                cohort_month,
                count(DISTINCT user_id) as cohort_size
            FROM first_orders
            GROUP BY cohort_month
        )
        SELECT 
            c.cohort_month,
            c.period_number,
            c.users,
            cs.cohort_size,
            c.users / cs.cohort_size * 100 as retention_rate
        FROM cohort_data c
        JOIN cohort_sizes cs ON c.cohort_month = cs.cohort_month
        ORDER BY c.cohort_month, c.period_number
        """
        
        result = self.client.execute(query)
        columns = ['cohort_month', 'period_number', 'users', 'cohort_size', 'retention_rate']
        
        return pd.DataFrame(result, columns=columns)
    
    def get_product_performance(self, days_back: int = 30) -> pd.DataFrame:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        query = f"""
        SELECT 
            product_category,
            product_name,
            count() as order_count,
            sum(quantity) as total_quantity,
            sum(total_amount) as total_revenue,
            avg(total_amount) as avg_order_value,
            uniq(user_id) as unique_customers,
            sum(total_amount) / uniq(user_id) as revenue_per_customer
        FROM sales_fact
        WHERE order_date >= today() - {days_back}
        GROUP BY product_category, product_name
        HAVING total_revenue > 1000
        ORDER BY total_revenue DESC
        LIMIT 50
        """
        
        result = self.client.execute(query)
        columns = ['product_category', 'product_name', 'order_count', 'total_quantity',
                  'total_revenue', 'avg_order_value', 'unique_customers', 'revenue_per_customer']
        
        return pd.DataFrame(result, columns=columns)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
analytics = ClickHouseAnalytics()
analytics.create_analytics_tables()

# –ê–Ω–∞–ª–∏–∑ –≤–æ—Ä–æ–Ω–∫–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
funnel_data = analytics.get_user_behavior_funnel(
    (datetime.datetime.now() - datetime.timedelta(days=30)).strftime('%Y-%m-%d %H:%M:%S'),
    datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
)
print("Funnel Analysis:")
print(funnel_data)

# –ö–æ–≥–æ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
cohort_data = analytics.get_cohort_analysis(months_back=6)
print("\nCohort Analysis:")
print(cohort_data.pivot(index='cohort_month', columns='period_number', values='retention_rate'))
```

---

## üéØ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è

### üî∞ –ù–∞—á–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –°–æ–∑–¥–∞–π—Ç–µ –ø—Ä–æ—Å—Ç—É—é —Å—Ö–µ–º—É "–∑–≤–µ–∑–¥–∞" –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–¥–∞–∂
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –±–∞–∑–æ–≤—ã–π ETL –ø—Ä–æ—Ü–µ—Å—Å –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
3. –°–æ–∑–¥–∞–π—Ç–µ –æ—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –º–µ—Å—è—Ü–∞–º

### üöÄ –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
1. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å—Ö–µ–º—É —Å –º–µ–¥–ª–µ–Ω–Ω–æ –∏–∑–º–µ–Ω—è—é—â–∏–º–∏—Å—è –∏–∑–º–µ—Ä–µ–Ω–∏—è–º–∏ (SCD Type 2)
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ ETL —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
3. –°–æ–∑–¥–∞–π—Ç–µ OLAP –∫—É–± —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é drill-down –∏ roll-up –æ–ø–µ—Ä–∞—Ü–∏–π

### üíé –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å
1. –°–æ–∑–¥–∞–π—Ç–µ —Ä–µ–∞–ª-—Ç–∞–π–º ETL —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
2. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
3. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è

---

## üåü –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

### ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

#### üèóÔ∏è –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- **–ù–∞—á–∏–Ω–∞–π—Ç–µ —Å –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π** - —Å—Ö–µ–º–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –±–∏–∑–Ω–µ—Å-–≤–æ–ø—Ä–æ—Å—ã
- **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å** - –Ω–∞–π–¥–∏—Ç–µ –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
- **–ü–ª–∞–Ω–∏—Ä—É–π—Ç–µ –¥–ª—è —Ä–æ—Å—Ç–∞** - —É—á–∏—Ç—ã–≤–∞–π—Ç–µ –±—É–¥—É—â–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö
- **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –≤—Å–µ** - –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã

#### üîÑ ETL –ø—Ä–æ—Ü–µ—Å—Å—ã
- **–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞** - –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- **–ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å** - –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫ –¥–æ–ª–∂–µ–Ω –¥–∞–≤–∞—Ç—å —Ç–æ—Ç –∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫** - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö
- **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

#### üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- **–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã** - –Ω–∞ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
- **–ü–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ** - –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–ª–∏ –¥—Ä—É–≥–∏–º –∫–ª—é—á–∞–º
- **–ö–æ–º–ø—Ä–µ—Å—Å–∏—è –¥–∞–Ω–Ω—ã—Ö** - —ç–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ I/O
- **–ú–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è** - –¥–ª—è —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

### ‚ùå –ß–µ–≥–æ –∏–∑–±–µ–≥–∞—Ç—å

- **Overengineering** - –Ω–µ —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞
- **–ü—Ä–µ–Ω–µ–±—Ä–µ–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö** - –ø–ª–æ—Ö–∏–µ –¥–∞–Ω–Ω—ã–µ = –ø–ª–æ—Ö–∏–µ —Ä–µ—à–µ–Ω–∏—è
- **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏** - –∑–∞—â–∏—â–∞–π—Ç–µ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è** - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—Ö–µ–º–µ

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã

- üêò **[[postgresql|PostgreSQL]]** - –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ OLTP –¥–∞–Ω–Ω—ã—Ö
- üçÉ **[[mongodb|MongoDB]]** - –¥–ª—è –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- üî¥ **[[redis|Redis]]** - –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
- üèóÔ∏è **[[database-design|–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ë–î]]** - –ø—Ä–∏–Ω—Ü–∏–ø—ã –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
- ‚ö° **[[sql-optimization|–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î]]** - –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤

**[[README|üè† –í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –æ–±–∑–æ—Ä—É –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö]]** 
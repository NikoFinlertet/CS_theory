# GPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã üöÄ

> **–ù–∞–≤–∏–≥–∞—Ü–∏—è**: [[README|‚Üê –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–≤]] | [[parallel-architectures|‚Üê –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã]] | [[performance-analysis|–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ‚Üí]]

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
1. [–û—Å–Ω–æ–≤—ã GPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã](#üéÆ-–æ—Å–Ω–æ–≤—ã-gpu-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
2. [CUDA –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ](#‚ö°-cuda-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ)
3. [–ò–µ—Ä–∞—Ä—Ö–∏—è –ø–∞–º—è—Ç–∏ GPU](#üß†-–∏–µ—Ä–∞—Ä—Ö–∏—è-–ø–∞–º—è—Ç–∏-gpu)
4. [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏](#üöÄ-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
5. [–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ GPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã](#üî¨-—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ-gpu-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)

---

## üéÆ –û—Å–Ω–æ–≤—ã GPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### GPU vs CPU –ø–∞—Ä–∞–¥–∏–≥–º–∞

```c
void gpu_vs_cpu_concepts() {
    printf("=== GPU vs CPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ ===\n\n");
    
    printf("CPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n");
    printf("‚Ä¢ –ú–∞–ª–æ —è–¥–µ—Ä (4-32), —Å–ª–æ–∂–Ω—ã–µ\n");
    printf("‚Ä¢ –í—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ (3-5 –ì–ì—Ü)\n");
    printf("‚Ä¢ –ë–æ–ª—å—à–∏–µ –∫—ç—à–∏ (MB)\n");
    printf("‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è latency\n");
    printf("‚Ä¢ –°–ª–æ–∂–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–π\n");
    printf("‚Ä¢ Out-of-order execution\n\n");
    
    printf("GPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n");
    printf("‚Ä¢ –ú–Ω–æ–≥–æ —è–¥–µ—Ä (1000+), –ø—Ä–æ—Å—Ç—ã–µ\n");
    printf("‚Ä¢ –ù–∏–∑–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ (1-2 –ì–ì—Ü)\n");
    printf("‚Ä¢ –ú–∞–ª—ã–µ –∫—ç—à–∏ (KB)\n");
    printf("‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è throughput\n");
    printf("‚Ä¢ –ü—Ä–æ—Å—Ç–æ–µ/–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n");
    printf("‚Ä¢ In-order execution\n\n");
    
    printf("–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å:\n");
    printf("CPU: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞\n");
    printf("GPU: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –ø—Ä–æ—Å—Ç–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞\n");
}
```

---

## ‚ö° CUDA –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ò–µ—Ä–∞—Ä—Ö–∏—è threads

```c
void cuda_hierarchy_concepts() {
    printf("=== CUDA –∏–µ—Ä–∞—Ä—Ö–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ===\n\n");
    
    printf("–¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è:\n");
    printf("Grid ‚Üí Blocks ‚Üí Threads\n\n");
    
    printf("Thread:\n");
    printf("‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n");
    printf("‚Ä¢ –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–≥–∏—Å—Ç—Ä—ã\n");
    printf("‚Ä¢ –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω CUDA kernel\n\n");
    
    printf("Block (Thread Block):\n");
    printf("‚Ä¢ –ì—Ä—É–ø–ø–∞ –¥–æ 1024 threads\n");
    printf("‚Ä¢ –û–±—â–∞—è shared memory (48KB)\n");
    printf("‚Ä¢ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ __syncthreads()\n");
    printf("‚Ä¢ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–∞ –æ–¥–Ω–æ–º SM\n\n");
    
    printf("Grid:\n");
    printf("‚Ä¢ –ì—Ä—É–ø–ø–∞ blocks\n");
    printf("‚Ä¢ –û–¥–∏–Ω kernel = –æ–¥–∏–Ω grid\n");
    printf("‚Ä¢ Blocks –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã\n\n");
    
    printf("–ü—Ä–∏–º–µ—Ä kernel:\n");
    printf("```c\n");
    printf("__global__ void vector_add(float* a, float* b, float* c, int n) {\n");
    printf("    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n");
    printf("    if (idx < n) {\n");
    printf("        c[idx] = a[idx] + b[idx];\n");
    printf("    }\n");
    printf("}\n\n");
    
    printf("// –ó–∞–ø—É—Å–∫:\n");
    printf("dim3 blockSize(256);\n");
    printf("dim3 gridSize((n + blockSize.x - 1) / blockSize.x);\n");
    printf("vector_add<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\n");
    printf("```\n");
}
```

### Warp execution model

```c
void warp_execution_demo() {
    printf("\n=== Warp –º–æ–¥–µ–ª—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ===\n\n");
    
    printf("Warp (–≤–∞—Ä–ø):\n");
    printf("‚Ä¢ 32 threads –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ\n");
    printf("‚Ä¢ SIMT (Single Instruction, Multiple Thread)\n");
    printf("‚Ä¢ –í—Å–µ threads –≤ warp –≤—ã–ø–æ–ª–Ω—è—é—Ç –æ–¥–Ω—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é\n\n");
    
    printf("Branch divergence:\n");
    printf("```c\n");
    printf("// –ü–õ–û–•–û - divergent branches:\n");
    printf("if (threadIdx.x %% 2 == 0) {\n");
    printf("    result = a[idx] * 2;      // 16 threads\n");
    printf("} else {\n");
    printf("    result = a[idx] + 1;      // 16 threads\n");
    printf("}\n");
    printf("// –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: 50%% (—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è)\n\n");
    
    printf("// –•–û–†–û–®–û - no divergence:\n");
    printf("if (blockIdx.x %% 2 == 0) {\n");
    printf("    result = a[idx] * 2;      // –í–µ—Å—å warp\n");
    printf("} else {\n");
    printf("    result = a[idx] + 1;      // –í–µ—Å—å warp\n");
    printf("}\n");
    printf("// –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: 100%%\n");
    printf("```\n\n");
    
    printf("Occupancy:\n");
    printf("‚Ä¢ –û—Ç–Ω–æ—à–µ–Ω–∏–µ active warps / maximum warps\n");
    printf("‚Ä¢ –¶–µ–ª—å: –≤—ã—Å–æ–∫–∏–π occupancy –¥–ª—è —Å–∫—Ä—ã—Ç–∏—è latency\n");
    printf("‚Ä¢ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: —Ä–µ–≥–∏—Å—Ç—Ä—ã, shared memory, block size\n");
}
```

---

## üß† –ò–µ—Ä–∞—Ä—Ö–∏—è –ø–∞–º—è—Ç–∏ GPU

### –¢–∏–ø—ã –ø–∞–º—è—Ç–∏

```c
void gpu_memory_hierarchy() {
    printf("\n=== –ò–µ—Ä–∞—Ä—Ö–∏—è –ø–∞–º—è—Ç–∏ GPU ===\n\n");
    
    printf("1. Registers (–Ω–∞ thread):\n");
    printf("   ‚Ä¢ –†–∞–∑–º–µ—Ä: ~64KB –Ω–∞ SM\n");
    printf("   ‚Ä¢ –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: 1 —Ü–∏–∫–ª\n");
    printf("   ‚Ä¢ –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: >40 TB/s\n");
    printf("   ‚Ä¢ –í–∏–¥–∏–º–æ—Å—Ç—å: thread-local\n\n");
    
    printf("2. Shared Memory (–Ω–∞ block):\n");
    printf("   ‚Ä¢ –†–∞–∑–º–µ—Ä: 48-164KB –Ω–∞ SM\n");
    printf("   ‚Ä¢ –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: 1-32 —Ü–∏–∫–ª–∞\n");
    printf("   ‚Ä¢ –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: ~19 TB/s\n");
    printf("   ‚Ä¢ –í–∏–¥–∏–º–æ—Å—Ç—å: block-local\n\n");
    
    printf("3. Global Memory:\n");
    printf("   ‚Ä¢ –†–∞–∑–º–µ—Ä: 8-80GB (–≤–µ—Å—å GPU)\n");
    printf("   ‚Ä¢ –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: 200-800 —Ü–∏–∫–ª–æ–≤\n");
    printf("   ‚Ä¢ –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å: 0.5-1.5 TB/s\n");
    printf("   ‚Ä¢ –í–∏–¥–∏–º–æ—Å—Ç—å: –≤—Å–µ threads\n\n");
    
    printf("4. Constant Memory:\n");
    printf("   ‚Ä¢ –†–∞–∑–º–µ—Ä: 64KB\n");
    printf("   ‚Ä¢ –ö—ç—à–∏—Ä—É–µ—Ç—Å—è\n");
    printf("   ‚Ä¢ Read-only\n\n");
    
    printf("5. Texture Memory:\n");
    printf("   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è 2D –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏\n");
    printf("   ‚Ä¢ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n");
    printf("   ‚Ä¢ Read-only\n");
}
```

### Coalesced memory access

```c
void memory_coalescing_demo() {
    printf("\n=== Memory Coalescing ===\n\n");
    
    printf("Coalesced (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π) –¥–æ—Å—Ç—É–ø:\n");
    printf("```c\n");
    printf("__global__ void good_access(float* data) {\n");
    printf("    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n");
    printf("    float value = data[idx];  // –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∞–¥—Ä–µ—Å–∞\n");
    printf("    // Thread 0: data[0], Thread 1: data[1], ...\n");
    printf("}\n");
    printf("```\n");
    printf("–†–µ–∑—É–ª—å—Ç–∞—Ç: 1 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è 32 threads\n\n");
    
    printf("Non-coalesced (–Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π) –¥–æ—Å—Ç—É–ø:\n");
    printf("```c\n");
    printf("__global__ void bad_access(float* data) {\n");
    printf("    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n");
    printf("    float value = data[idx * 37];  // Stride 37\n");
    printf("    // Thread 0: data[0], Thread 1: data[37], ...\n");
    printf("}\n");
    printf("```\n");
    printf("–†–µ–∑—É–ª—å—Ç–∞—Ç: 32 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è 32 threads\n\n");
    
    printf("–í–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\n");
    printf("‚Ä¢ Coalesced: ~900 GB/s\n");
    printf("‚Ä¢ Non-coalesced: ~30 GB/s\n");
    printf("‚Ä¢ –ó–∞–º–µ–¥–ª–µ–Ω–∏–µ: –¥–æ 30x!\n");
}
```

---

## üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### Shared memory –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

```c
void shared_memory_optimization() {
    printf("\n=== Shared Memory –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ===\n\n");
    
    printf("–ü—Ä–∏–º–µ—Ä: Matrix transpose\n");
    printf("```c\n");
    printf("// –í–µ—Ä—Å–∏—è 1: –ù–∞–∏–≤–Ω–∞—è (–º–µ–¥–ª–µ–Ω–Ω–∞—è)\n");
    printf("__global__ void naive_transpose(float* input, float* output, int n) {\n");
    printf("    int row = blockIdx.y * blockDim.y + threadIdx.y;\n");
    printf("    int col = blockIdx.x * blockDim.x + threadIdx.x;\n");
    printf("    \n");
    printf("    if (row < n && col < n) {\n");
    printf("        output[col * n + row] = input[row * n + col];\n");
    printf("    }\n");
    printf("}\n\n");
    
    printf("// –í–µ—Ä—Å–∏—è 2: –° shared memory (–±—ã—Å—Ç—Ä–∞—è)\n");
    printf("__global__ void optimized_transpose(float* input, float* output, int n) {\n");
    printf("    __shared__ float tile[32][33];  // +1 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è bank conflicts\n");
    printf("    \n");
    printf("    int row = blockIdx.y * 32 + threadIdx.y;\n");
    printf("    int col = blockIdx.x * 32 + threadIdx.x;\n");
    printf("    \n");
    printf("    // Coalesced load –≤ shared memory\n");
    printf("    if (row < n && col < n) {\n");
    printf("        tile[threadIdx.y][threadIdx.x] = input[row * n + col];\n");
    printf("    }\n");
    printf("    __syncthreads();\n");
    printf("    \n");
    printf("    // Coalesced store –∏–∑ shared memory\n");
    printf("    row = blockIdx.x * 32 + threadIdx.y;\n");
    printf("    col = blockIdx.y * 32 + threadIdx.x;\n");
    printf("    if (row < n && col < n) {\n");
    printf("        output[row * n + col] = tile[threadIdx.x][threadIdx.y];\n");
    printf("    }\n");
    printf("}\n");
    printf("```\n\n");
    
    printf("–£—Å–∫–æ—Ä–µ–Ω–∏–µ: –¥–æ 10x –±–ª–∞–≥–æ–¥–∞—Ä—è:\n");
    printf("‚Ä¢ Coalesced memory access\n");
    printf("‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ fast shared memory\n");
    printf("‚Ä¢ –ò–∑–±–µ–∂–∞–Ω–∏–µ bank conflicts\n");
}
```

### Occupancy –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

```c
void occupancy_optimization() {
    printf("\n=== Occupancy –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ===\n\n");
    
    printf("–§–∞–∫—Ç–æ—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ occupancy:\n\n");
    
    printf("1. –†–µ–≥–∏—Å—Ç—Ä—ã –Ω–∞ thread:\n");
    printf("   ‚Ä¢ SM –∏–º–µ–µ—Ç 65536 —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤\n");
    printf("   ‚Ä¢ 256 threads/block √ó 64 reg/thread = 16384 reg\n");
    printf("   ‚Ä¢ –ú–∞–∫—Å–∏–º—É–º 4 block –Ω–∞ SM\n");
    printf("   ‚Ä¢ –ï—Å–ª–∏ 128 reg/thread ‚Üí —Ç–æ–ª—å–∫–æ 2 block\n\n");
    
    printf("2. Shared memory –Ω–∞ block:\n");
    printf("   ‚Ä¢ SM –∏–º–µ–µ—Ç 48KB shared memory\n");
    printf("   ‚Ä¢ 16KB/block ‚Üí –º–∞–∫—Å–∏–º—É–º 3 block\n");
    printf("   ‚Ä¢ 24KB/block ‚Üí –º–∞–∫—Å–∏–º—É–º 2 block\n\n");
    
    printf("3. Threads –Ω–∞ block:\n");
    printf("   ‚Ä¢ –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫—Ä–∞—Ç–Ω–æ 32 (warp size)\n");
    printf("   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: 128, 256, 512\n");
    printf("   ‚Ä¢ –ë–æ–ª—å—à–µ blocks = –ª—É—á—à–µ load balancing\n\n");
    
    printf("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:\n");
    printf("‚Ä¢ nvprof --metrics achieved_occupancy\n");
    printf("‚Ä¢ CUDA Occupancy Calculator\n");
    printf("‚Ä¢ Nsight Compute\n\n");
    
    printf("–¶–µ–ª–µ–≤–∞—è occupancy: 50-100%%\n");
    printf("–ù–∏–∑–∫–∏–π occupancy –º–æ–∂–µ—Ç –±—ã—Ç—å OK –¥–ª—è memory-bound kernels\n");
}
```

---

## üî¨ –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ GPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã NVIDIA

```c
void nvidia_architectures() {
    printf("\n=== –≠–≤–æ–ª—é—Ü–∏—è NVIDIA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä ===\n\n");
    
    printf("Maxwell (GTX 900):\n");
    printf("‚Ä¢ 28nm –ø—Ä–æ—Ü–µ—Å—Å\n");
    printf("‚Ä¢ –ü–µ—Ä–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å unified memory\n");
    printf("‚Ä¢ –£–ª—É—á—à–µ–Ω–Ω—ã–π scheduler\n\n");
    
    printf("Pascal (GTX 10xx, P100):\n");
    printf("‚Ä¢ 16nm FinFET\n");
    printf("‚Ä¢ HBM2 –ø–∞–º—è—Ç—å\n");
    printf("‚Ä¢ Half-precision (FP16) support\n");
    printf("‚Ä¢ NVLink interconnect\n\n");
    
    printf("Volta (V100):\n");
    printf("‚Ä¢ 12nm FFN\n");
    printf("‚Ä¢ Tensor Cores (mixed precision)\n");
    printf("‚Ä¢ Independent thread scheduling\n");
    printf("‚Ä¢ Cooperative groups\n\n");
    
    printf("Turing (RTX 20xx):\n");
    printf("‚Ä¢ 12nm FFN\n");
    printf("‚Ä¢ RT Cores (ray tracing)\n");
    printf("‚Ä¢ Improved Tensor Cores\n");
    printf("‚Ä¢ Variable rate shading\n\n");
    
    printf("Ampere (RTX 30xx, A100):\n");
    printf("‚Ä¢ 7nm Samsung / 7nm TSMC\n");
    printf("‚Ä¢ 3rd gen Tensor Cores\n");
    printf("‚Ä¢ Multi-instance GPU (MIG)\n");
    printf("‚Ä¢ Structural sparsity support\n\n");
    
    printf("Ada Lovelace (RTX 40xx):\n");
    printf("‚Ä¢ 4nm TSMC\n");
    printf("‚Ä¢ 4th gen RT Cores\n");
    printf("‚Ä¢ DLSS 3.0\n");
    printf("‚Ä¢ AV1 encoding\n\n");
    
    printf("Hopper (H100):\n");
    printf("‚Ä¢ 4nm TSMC\n");
    printf("‚Ä¢ 4th gen Tensor Cores\n");
    printf("‚Ä¢ Transformer Engine\n");
    printf("‚Ä¢ DPX instructions\n");
}
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å AMD –∏ Intel

```c
void gpu_vendor_comparison() {
    printf("\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ–Ω–¥–æ—Ä–æ–≤ GPU ===\n\n");
    
    printf("NVIDIA:\n");
    printf("‚úÖ –õ–∏–¥–µ—Ä –≤ GPGPU\n");
    printf("‚úÖ –ó—Ä–µ–ª–∞—è CUDA —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞\n");
    printf("‚úÖ –û—Ç–ª–∏—á–Ω—ã–µ ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n");
    printf("‚úÖ Tensor Cores –¥–ª—è AI\n");
    printf("‚ùå –í—ã—Å–æ–∫–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å\n");
    printf("‚ùå –ü—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏\n\n");
    
    printf("AMD (RDNA/CDNA):\n");
    printf("‚úÖ –û—Ç–∫—Ä—ã—Ç—ã–π ROCm stack\n");
    printf("‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω–∞/–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n");
    printf("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å CPU\n");
    printf("‚ùå –ú–µ–Ω—å—à–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞\n");
    printf("‚ùå –û—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ –≤ ML accelerators\n\n");
    
    printf("Intel (Arc/Ponte Vecchio):\n");
    printf("‚úÖ oneAPI —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n");
    printf("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è CPU+GPU\n");
    printf("‚úÖ –û—Ç–∫—Ä—ã—Ç—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã\n");
    printf("‚ùå –ù–æ–≤–∏—á–æ–∫ –Ω–∞ —Ä—ã–Ω–∫–µ\n");
    printf("‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ legacy\n");
}
```

---

## üìä –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

### Machine Learning workloads

```c
void ml_gpu_optimization() {
    printf("\n=== ML –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ GPU ===\n\n");
    
    printf("–¢–∏–ø–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏:\n");
    printf("‚Ä¢ Matrix multiplication (GEMM): 80-90%% –≤—Ä–µ–º–µ–Ω–∏\n");
    printf("‚Ä¢ Convolution: –º–æ–∂–Ω–æ —Å–≤–µ—Å—Ç–∏ –∫ GEMM\n");
    printf("‚Ä¢ Element-wise operations: bandwidth-bound\n");
    printf("‚Ä¢ Reductions: sync-intensive\n\n");
    
    printf("Tensor Cores –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:\n");
    printf("```c\n");
    printf("// FP16 input/output, FP32 accumulate\n");
    printf("// C = A √ó B + C\n");
    printf("// A: [M√óK] FP16, B: [K√óN] FP16, C: [M√óN] FP32\n");
    printf("cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n");
    printf("             m, n, k,\n");
    printf("             &alpha, A, CUDA_R_16F, lda,\n");
    printf("                     B, CUDA_R_16F, ldb,\n");
    printf("             &beta,  C, CUDA_R_32F, ldc,\n");
    printf("             CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP);\n");
    printf("```\n\n");
    
    printf("Batch processing:\n");
    printf("‚Ä¢ –£–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ batch size –¥–ª—è –ª—É—á—à–µ–≥–æ GPU utilization\n");
    printf("‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ cudaStreamCreate() –¥–ª—è overlap\n");
    printf("‚Ä¢ Mixed precision –¥–ª—è 2x speedup\n\n");
    
    printf("Memory optimization:\n");
    printf("‚Ä¢ Gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n");
    printf("‚Ä¢ Model parallelism –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π\n");
    printf("‚Ä¢ Unified memory –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏\n");
}
```

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã

- [[parallel-architectures|–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã]] - –æ–±—â–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
- [[performance-analysis|–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏]] - –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ GPU –∫–æ–¥–∞
- [[memory-hierarchy|–ò–µ—Ä–∞—Ä—Ö–∏—è –ø–∞–º—è—Ç–∏]] - –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã —Å –ø–∞–º—è—Ç—å—é

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **CUDA Programming Guide** - –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è NVIDIA
- **CUDA Best Practices Guide** - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- **PTX ISA** - –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∞—Å—Å–µ–º–±–ª–µ—Ä GPU

### üõ†Ô∏è –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è
- **Nsight Compute** - –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ kernel'–æ–≤
- **Nsight Systems** - —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä
- **nvprof** - –∫–æ–º–∞–Ω–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä
- **NVIDIA Visual Profiler** - GUI –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä

### üìä –ë–µ–Ω—á–º–∞—Ä–∫–∏
- **CUDA Samples** - –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞
- **SHOC** - GPU benchmark suite
- **Rodinia** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã

---

> **–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥**: –ò–∑—É—á–∏—Ç–µ [[performance-analysis|–∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏]] –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU –∫–æ–¥–∞. 
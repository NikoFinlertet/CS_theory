# –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è üìà

> **–ù–∞–≤–∏–≥–∞—Ü–∏—è**: [[probability-randomized-algorithms|‚Üê –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã]] | [[mathematics-algorithms|–ì–ª–∞–≤–Ω–∞—è]] | [[number-theory-cryptography|–¢–µ–æ—Ä–∏—è —á–∏—Å–µ–ª ‚Üí]]

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
1. [–ß–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã](#üìà-—á–∏—Å–ª–µ–Ω–Ω—ã–µ-–º–µ—Ç–æ–¥—ã)
2. [–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#üéØ-–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
3. [–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ](#ü§ñ-–º–∞—à–∏–Ω–Ω–æ–µ-–æ–±—É—á–µ–Ω–∏–µ)
4. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è](#üöÄ-–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ-–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è)

---

## üìà –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:** –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ, –∏–Ω—Ç–µ–≥—Ä–∞–ª—ã, —ç–∫—Å—Ç—Ä–µ–º—É–º—ã —Ñ—É–Ω–∫—Ü–∏–π

```python
import numpy as np
import math

class NumericalMethods:
    """–ß–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    @staticmethod
    def newton_raphson(f, df, x0, tolerance=1e-10, max_iterations=100):
        """
        –ú–µ—Ç–æ–¥ –ù—å—é—Ç–æ–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω–µ–π
        x_{n+1} = x_n - f(x_n)/f'(x_n)
        """
        x = x0
        for i in range(max_iterations):
            fx = f(x)
            if abs(fx) < tolerance:
                return x, i
            
            dfx = df(x)
            if abs(dfx) < tolerance:
                raise ValueError("–ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –±–ª–∏–∑–∫–∞ –∫ –Ω—É–ª—é")
            
            x_new = x - fx / dfx
            if abs(x_new - x) < tolerance:
                return x_new, i
            
            x = x_new
        
        raise ValueError("–ù–µ —Å–æ—à–µ–ª—Å—è –∑–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π")
    
    @staticmethod
    def numerical_derivative(f, x, h=1e-8):
        """–ß–∏—Å–ª–µ–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π"""
        return (f(x + h) - f(x - h)) / (2 * h)
    
    @staticmethod
    def simpson_integration(f, a, b, n=1000):
        """
        –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–º –°–∏–º–ø—Å–æ–Ω–∞
        ‚à´f(x)dx ‚âà (h/3)[f(x0) + 4f(x1) + 2f(x2) + ... + f(xn)]
        """
        if n % 2 == 1:
            n += 1  # n –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —á–µ—Ç–Ω—ã–º
        
        h = (b - a) / n
        x = a
        result = f(a)
        
        for i in range(1, n):
            x += h
            if i % 2 == 0:
                result += 2 * f(x)
            else:
                result += 4 * f(x)
        
        result += f(b)
        return result * h / 3
```

---

## üéØ –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

```python
class OptimizationAlgorithms:
    """–ê–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    
    @staticmethod
    def gradient_descent(f, gradient_f, x0, learning_rate=0.01, 
                        tolerance=1e-6, max_iterations=10000):
        """
        –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏
        x_{k+1} = x_k - Œ±‚àáf(x_k)
        """
        x = np.array(x0, dtype=float)
        trajectory = [x.copy()]
        
        for i in range(max_iterations):
            grad = np.array(gradient_f(x))
            x_new = x - learning_rate * grad
            
            if np.linalg.norm(x_new - x) < tolerance:
                return x_new, trajectory, i
            
            x = x_new
            trajectory.append(x.copy())
        
        return x, trajectory, max_iterations
    
    @staticmethod
    def adam_optimizer(f, gradient_f, x0, learning_rate=0.001, 
                      beta1=0.9, beta2=0.999, epsilon=1e-8,
                      max_iterations=10000):
        """
        Adam –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä - –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–º–µ–Ω—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        """
        x = np.array(x0, dtype=float)
        m = np.zeros_like(x)  # –ü–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç
        v = np.zeros_like(x)  # –í—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç
        
        for t in range(1, max_iterations + 1):
            grad = np.array(gradient_f(x))
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–æ–≤
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * grad**2
            
            # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è
            m_hat = m / (1 - beta1**t)
            v_hat = v / (1 - beta2**t)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
        
        return x
```

---

## ü§ñ –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º

```python
class LinearRegressionGD:
    """–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def cost_function(self, X, y):
        """–§—É–Ω–∫—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ (MSE)"""
        m = len(y)
        predictions = X.dot(self.weights) + self.bias
        cost = (1/(2*m)) * np.sum((predictions - y)**2)
        return cost
    
    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        m, n = X.shape
        self.weights = np.zeros(n)
        self.bias = 0
        
        for i in range(self.max_iterations):
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = X.dot(self.weights) + self.bias
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            dw = (1/m) * X.T.dot(predictions - y)
            db = (1/m) * np.sum(predictions - y)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
            cost = self.cost_function(X, y)
            self.cost_history.append(cost)
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        return X.dot(self.weights) + self.bias

class LogisticRegressionGD:
    """–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º —Å–ø—É—Å–∫–æ–º"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
    
    def sigmoid(self, z):
        """–°–∏–≥–º–æ–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º z –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def cost_function(self, h, y):
        """–§—É–Ω–∫—Ü–∏—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–∏"""
        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    
    def fit(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        m, n = X.shape
        self.weights = np.zeros(n)
        self.bias = 0
        
        for i in range(self.max_iterations):
            # –ü—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            z = X.dot(self.weights) + self.bias
            h = self.sigmoid(z)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            dw = (1/m) * X.T.dot(h - y)
            db = (1/m) * np.sum(h - y)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict_proba(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏"""
        z = X.dot(self.weights) + self.bias
        return self.sigmoid(z)
    
    def predict(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞"""
        return (self.predict_proba(X) >= 0.5).astype(int)
```

---

## üöÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

### –ê–ª–≥–æ—Ä–∏—Ç–º—ã –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

```python
class ComputerGraphicsOptimization:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–µ"""
    
    @staticmethod
    def bezier_curve(control_points, t):
        """
        –ö—Ä–∏–≤–∞—è –ë–µ–∑—å–µ - –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –≤ –≥—Ä–∞—Ñ–∏–∫–µ
        B(t) = Œ£(i=0 to n) C(n,i) * (1-t)^(n-i) * t^i * P_i
        """
        n = len(control_points) - 1
        result = np.zeros_like(control_points[0])
        
        for i, point in enumerate(control_points):
            # –ë–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
            coeff = math.comb(n, i)
            # –ë–∞–∑–∏—Å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞
            basis = coeff * ((1-t)**(n-i)) * (t**i)
            result += basis * np.array(point)
        
        return result
    
    @staticmethod
    def optimize_camera_position(target_objects, constraints):
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∑–∏—Ü–∏–∏ –∫–∞–º–µ—Ä—ã –¥–ª—è –Ω–∞–∏–ª—É—á—à–µ–≥–æ –æ–±–∑–æ—Ä–∞
        –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–∏–º–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–æ–≤
        """
        def cost_function(camera_pos):
            total_cost = 0
            for obj in target_objects:
                # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –æ–±—ä–µ–∫—Ç–∞
                distance = np.linalg.norm(np.array(camera_pos) - np.array(obj['position']))
                
                # –£–≥–æ–ª –æ–±–∑–æ—Ä–∞
                view_angle = math.atan2(obj['size'], distance)
                
                # –§—É–Ω–∫—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ (—Ö–æ—Ç–∏–º –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å)
                cost = -view_angle + 0.1 * distance  # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –±–ª–∏–∑–æ—Å—Ç—å—é –∏ —É–≥–ª–æ–º
                total_cost += cost
            
            return total_cost
        
        def gradient_cost(camera_pos):
            """–ß–∏—Å–ª–µ–Ω–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
            grad = np.zeros(3)
            h = 1e-8
            
            for i in range(3):
                pos_plus = camera_pos.copy()
                pos_minus = camera_pos.copy()
                pos_plus[i] += h
                pos_minus[i] -= h
                
                grad[i] = (cost_function(pos_plus) - cost_function(pos_minus)) / (2 * h)
            
            return grad
        
        # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –∫–∞–º–µ—Ä—ã
        initial_pos = np.array([0.0, 0.0, 5.0])
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
        optimal_pos, _, _ = OptimizationAlgorithms.gradient_descent(
            cost_function, gradient_cost, initial_pos, learning_rate=0.1
        )
        
        return optimal_pos

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def demo_calculus_applications():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    # –ü–æ–∏—Å–∫ –∫–æ—Ä–Ω—è —Ñ—É–Ω–∫—Ü–∏–∏ f(x) = x^2 - 2 (‚àö2)
    def f(x):
        return x**2 - 2
    
    def df(x):
        return 2*x
    
    root, iterations = NumericalMethods.newton_raphson(f, df, x0=1.0)
    print(f"‚àö2 ‚âà {root:.10f} (–Ω–∞–π–¥–µ–Ω–æ –∑–∞ {iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π)")
    
    # –ß–∏—Å–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ sin(x) –æ—Ç 0 –¥–æ œÄ
    integral = NumericalMethods.simpson_integration(math.sin, 0, math.pi)
    print(f"‚à´sin(x)dx –æ—Ç 0 –¥–æ œÄ ‚âà {integral:.6f} (—Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 2)")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    def quadratic(x):
        return (x[0] - 3)**2 + (x[1] + 1)**2
    
    def grad_quadratic(x):
        return np.array([2*(x[0] - 3), 2*(x[1] + 1)])
    
    minimum, trajectory, iterations = OptimizationAlgorithms.gradient_descent(
        quadratic, grad_quadratic, [0, 0]
    )
    print(f"–ú–∏–Ω–∏–º—É–º —Ñ—É–Ω–∫—Ü–∏–∏: {minimum} –∑–∞ {iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π")
```

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∞:**

- ‚úÖ **–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫**: –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- ‚úÖ **–ß–∏—Å–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ**: –§–∏–∑–∏—á–µ—Å–∫–∏–µ —Å–∏–º—É–ª—è—Ü–∏–∏, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π
- ‚úÖ **–ü–æ–∏—Å–∫ –∫–æ—Ä–Ω–µ–π**: –†–µ—à–µ–Ω–∏–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–π, –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π
- ‚úÖ **–ö—Ä–∏–≤—ã–µ –ë–µ–∑—å–µ**: –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞, –∞–Ω–∏–º–∞—Ü–∏—è, –¥–∏–∑–∞–π–Ω

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã

- [[linear-algebra-computation|–õ–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞]] - –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- [[probability-randomized-algorithms|–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã]] - —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã
- [[statistics-data-analysis|–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞]] - —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
- [[practical-applications|–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è]] - –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- **–ö–Ω–∏–≥–∏**: "Numerical Analysis" - Burden & Faires
- **–ö—É—Ä—Å—ã**: Stanford CS205, MIT 18.335  
- **–ü—Ä–∞–∫—Ç–∏–∫–∞**: SciPy optimization, TensorFlow/PyTorch 
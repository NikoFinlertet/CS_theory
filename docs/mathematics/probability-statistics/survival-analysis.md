# –ê–Ω–∞–ª–∏–∑ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏ üìà

> [[advanced-methods|‚Üê –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã]] | [[README|–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ]]

## üéØ –ß—Ç–æ –∏–∑—É—á–∏–º

- **–ê–Ω–∞–ª–∏–∑ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏** - –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ —Å–æ–±—ã—Ç–∏—è
- **Retention –∞–Ω–∞–ª–∏–∑** - —É–¥–µ—Ä–∂–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –∫–ª–∏–µ–Ω—Ç–æ–≤
- **Churn prediction** - –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞
- **A/B —Ç–µ—Å—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏** - —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ retention

---

## 1. –û—Å–Ω–æ–≤—ã –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏

```python
class SurvivalAnalysis:
    @staticmethod
    def kaplan_meier(times, events):
        """–û—Ü–µ–Ω–∫–∞ –ö–∞–ø–ª–∞–Ω–∞-–ú–µ–π–µ—Ä–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤—ã–∂–∏–≤–∞–Ω–∏—è"""
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω —Å–æ–±—ã—Ç–∏–π
        event_times = sorted(set(t for t, e in zip(times, events) if e == 1))
        
        survival_function = []
        n_at_risk = len(times)
        survival_prob = 1.0
        
        for t in event_times:
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –≤ –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ t
            events_at_t = sum(1 for time, event in zip(times, events) 
                             if time == t and event == 1)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–∂–∏–≤–∞–Ω–∏—è
            if n_at_risk > 0:
                survival_prob *= (n_at_risk - events_at_t) / n_at_risk
            
            survival_function.append({
                'time': t,
                'survival_probability': survival_prob,
                'events': events_at_t,
                'at_risk': n_at_risk
            })
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –≥—Ä—É–ø–ø–µ —Ä–∏—Å–∫–∞
            n_at_risk -= sum(1 for time in times if time == t)
        
        return survival_function
    
    @staticmethod
    def median_survival_time(survival_function):
        """–ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–∂–∏–≤–∞–Ω–∏—è"""
        for point in survival_function:
            if point['survival_probability'] <= 0.5:
                return point['time']
        return None  # –ú–µ–¥–∏–∞–Ω–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞
    
    @staticmethod
    def survival_at_time(survival_function, target_time):
        """–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–∂–∏–≤–∞–Ω–∏—è –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏"""
        last_prob = 1.0
        for point in survival_function:
            if point['time'] > target_time:
                return last_prob
            last_prob = point['survival_probability']
        return last_prob

# –ü—Ä–∏–º–µ—Ä: –∞–Ω–∞–ª–∏–∑ retention –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
# times - –¥–Ω–∏ –¥–æ churn, events - 1 –µ—Å–ª–∏ churn –ø—Ä–æ–∏–∑–æ—à–µ–ª, 0 –µ—Å–ª–∏ —Ü–µ–Ω–∑—É—Ä–∏—Ä–æ–≤–∞–Ω–æ
user_data = [
    (7, 1), (14, 1), (21, 0), (30, 1), (45, 0),    # –î–Ω–∏, —É—à–µ–ª –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
    (60, 1), (90, 0), (120, 1), (180, 0), (365, 0)
]

times = [t for t, e in user_data]
events = [e for t, e in user_data]

survival_curve = SurvivalAnalysis.kaplan_meier(times, events)

print("–ê–Ω–∞–ª–∏–∑ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:")
for point in survival_curve:
    print(f"–î–µ–Ω—å {point['time']:3d}: {point['survival_probability']:.3f} "
          f"(—Å–æ–±—ã—Ç–∏—è: {point['events']}, –≤ —Ä–∏—Å–∫–µ: {point['at_risk']})")

# –ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è retention
median_time = SurvivalAnalysis.median_survival_time(survival_curve)
if median_time:
    print(f"\n–ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è retention: {median_time} –¥–Ω–µ–π")
else:
    print("\n–ú–µ–¥–∏–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è retention –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ (>50% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Å—Ç–∞—é—Ç—Å—è)")

# Retention –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–∫–∞—Ö
key_timepoints = [30, 90, 180]
print("\nRetention –≤ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:")
for days in key_timepoints:
    retention = SurvivalAnalysis.survival_at_time(survival_curve, days)
    print(f"  {days} –¥–Ω–µ–π: {retention:.1%}")
```

---

## 2. Retention –∞–Ω–∞–ª–∏–∑

```python
class RetentionAnalysis:
    @staticmethod
    def cohort_retention(user_cohorts):
        """–ö–æ–≥–æ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ retention"""
        cohort_data = {}
        
        for cohort_month, users in user_cohorts.items():
            cohort_data[cohort_month] = {}
            initial_size = len(users)
            
            # –ü–æ–¥—Å—á–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –º–µ—Å—è—Ü–∞–º
            for month_offset in range(12):  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º 12 –º–µ—Å—è—Ü–µ–≤
                active_users = sum(1 for user in users 
                                 if user.get('active_months', 0) > month_offset)
                
                retention_rate = active_users / initial_size if initial_size > 0 else 0
                cohort_data[cohort_month][f'month_{month_offset}'] = {
                    'active_users': active_users,
                    'retention_rate': retention_rate,
                    'initial_size': initial_size
                }
        
        return cohort_data
    
    @staticmethod
    def calculate_ltv(monthly_revenue, churn_rate):
        """–†–∞—Å—á–µ—Ç Lifetime Value"""
        if churn_rate <= 0 or churn_rate >= 1:
            return float('inf') if churn_rate <= 0 else 0
        
        # LTV = Average Revenue Per User / Churn Rate
        ltv = monthly_revenue / churn_rate
        return ltv
    
    @staticmethod
    def retention_metrics(active_users_timeline):
        """–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ retention"""
        if not active_users_timeline:
            return {}
        
        # Day 1, Day 7, Day 30 retention
        retention_points = [1, 7, 30]
        metrics = {}
        
        initial_users = active_users_timeline[0] if active_users_timeline else 0
        
        for day in retention_points:
            if day < len(active_users_timeline):
                active_at_day = active_users_timeline[day]
                retention_rate = active_at_day / initial_users if initial_users > 0 else 0
                metrics[f'day_{day}_retention'] = retention_rate
            else:
                metrics[f'day_{day}_retention'] = 0
        
        # Churn rate (–æ–±—Ä–∞—Ç–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å retention)
        if 'day_7_retention' in metrics:
            metrics['weekly_churn_rate'] = 1 - metrics['day_7_retention']
        
        if 'day_30_retention' in metrics:
            metrics['monthly_churn_rate'] = 1 - metrics['day_30_retention']
        
        return metrics

# –ü—Ä–∏–º–µ—Ä: –∞–Ω–∞–ª–∏–∑ retention –º–æ–±–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app_cohorts = {
    '2024-01': [
        {'user_id': 1, 'active_months': 6},
        {'user_id': 2, 'active_months': 3},
        {'user_id': 3, 'active_months': 8},
        {'user_id': 4, 'active_months': 1},
        {'user_id': 5, 'active_months': 12}
    ],
    '2024-02': [
        {'user_id': 6, 'active_months': 5},
        {'user_id': 7, 'active_months': 2},
        {'user_id': 8, 'active_months': 7},
        {'user_id': 9, 'active_months': 4}
    ]
}

retention_data = RetentionAnalysis.cohort_retention(app_cohorts)

print("–ö–æ–≥–æ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ retention:")
for cohort, data in retention_data.items():
    print(f"\n–ö–æ–≥–æ—Ä—Ç–∞ {cohort}:")
    print(f"  –ù–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {data['month_0']['initial_size']}")
    for month in [0, 1, 3, 6]:
        if f'month_{month}' in data:
            rate = data[f'month_{month}']['retention_rate']
            print(f"  –ú–µ—Å—è—Ü {month}: {rate:.1%}")

# –†–∞—Å—á–µ—Ç LTV
average_monthly_revenue = 15  # $15 –≤ –º–µ—Å—è—Ü –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
monthly_churn = 0.05  # 5% –≤ –º–µ—Å—è—Ü

ltv = RetentionAnalysis.calculate_ltv(average_monthly_revenue, monthly_churn)
print(f"\nLifetime Value: ${ltv:.0f}")

# –î–Ω–µ–≤–Ω–æ–π retention
daily_active_users = [1000, 850, 750, 700, 650, 600, 580, 560]  # –î–µ–Ω—å 0-7
retention_metrics = RetentionAnalysis.retention_metrics(daily_active_users)

print("\nMetrics retention:")
for metric, value in retention_metrics.items():
    if 'retention' in metric:
        print(f"  {metric}: {value:.1%}")
    elif 'churn' in metric:
        print(f"  {metric}: {value:.1%}")
```

---

## 3. Churn Prediction

```python
class ChurnPredictor:
    def __init__(self):
        self.features = []
        self.weights = []
        self.threshold = 0.5
    
    def extract_features(self, user_data):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è churn"""
        features = []
        
        for user in user_data:
            feature_vector = [
                user.get('days_since_last_login', 0),
                user.get('session_frequency', 0),
                user.get('total_sessions', 0),
                user.get('average_session_duration', 0),
                user.get('feature_usage_score', 0),
                user.get('support_tickets', 0),
                1 if user.get('premium_user', False) else 0
            ]
            features.append(feature_vector)
        
        return features
    
    def simple_logistic_regression(self, features, labels, learning_rate=0.01, iterations=1000):
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è"""
        import math
        
        n_features = len(features[0])
        self.weights = [0.0] * (n_features + 1)  # +1 –¥–ª—è bias
        
        for _ in range(iterations):
            for i, (feature_vector, label) in enumerate(zip(features, labels)):
                # –î–æ–±–∞–≤–ª—è–µ–º bias term
                x = [1.0] + feature_vector
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                z = sum(w * xi for w, xi in zip(self.weights, x))
                prediction = 1 / (1 + math.exp(-min(max(z, -250), 250)))  # Sigmoid —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
                error = label - prediction
                for j in range(len(self.weights)):
                    self.weights[j] += learning_rate * error * x[j]
    
    def predict_churn(self, user_features):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ churn"""
        import math
        
        if not self.weights:
            return 0.5  # –ù–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        x = [1.0] + user_features  # –î–æ–±–∞–≤–ª—è–µ–º bias
        z = sum(w * xi for w, xi in zip(self.weights, x))
        
        # Sigmoid —Ñ—É–Ω–∫—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è
        try:
            probability = 1 / (1 + math.exp(-z))
        except OverflowError:
            probability = 0.0 if z < 0 else 1.0
        
        return probability
    
    def risk_segmentation(self, users_data):
        """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ —Ä–∏—Å–∫—É churn"""
        risk_segments = {'low': [], 'medium': [], 'high': []}
        
        for user in users_data:
            features = self.extract_features([user])[0]
            churn_prob = self.predict_churn(features)
            
            user_risk = {
                'user_id': user.get('user_id'),
                'churn_probability': churn_prob,
                'features': features
            }
            
            if churn_prob < 0.3:
                risk_segments['low'].append(user_risk)
            elif churn_prob < 0.7:
                risk_segments['medium'].append(user_risk)
            else:
                risk_segments['high'].append(user_risk)
        
        return risk_segments

# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
training_users = [
    {'days_since_last_login': 1, 'session_frequency': 5, 'total_sessions': 100, 
     'average_session_duration': 15, 'feature_usage_score': 8, 'support_tickets': 0, 
     'premium_user': True, 'churned': 0},
    
    {'days_since_last_login': 15, 'session_frequency': 1, 'total_sessions': 20, 
     'average_session_duration': 5, 'feature_usage_score': 3, 'support_tickets': 2, 
     'premium_user': False, 'churned': 1},
     
    {'days_since_last_login': 3, 'session_frequency': 4, 'total_sessions': 80, 
     'average_session_duration': 12, 'feature_usage_score': 7, 'support_tickets': 0, 
     'premium_user': True, 'churned': 0},
     
    {'days_since_last_login': 30, 'session_frequency': 0.5, 'total_sessions': 10, 
     'average_session_duration': 3, 'feature_usage_score': 2, 'support_tickets': 3, 
     'premium_user': False, 'churned': 1}
]

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
predictor = ChurnPredictor()
features = predictor.extract_features(training_users)
labels = [user['churned'] for user in training_users]

predictor.simple_logistic_regression(features, labels)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
new_users = [
    {'user_id': 'user_001', 'days_since_last_login': 7, 'session_frequency': 3, 
     'total_sessions': 50, 'average_session_duration': 10, 'feature_usage_score': 6, 
     'support_tickets': 1, 'premium_user': False},
     
    {'user_id': 'user_002', 'days_since_last_login': 25, 'session_frequency': 0.8, 
     'total_sessions': 15, 'average_session_duration': 4, 'feature_usage_score': 2, 
     'support_tickets': 4, 'premium_user': False}
]

# –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ —Ä–∏—Å–∫—É
risk_segments = predictor.risk_segmentation(new_users)

print("–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ —Ä–∏—Å–∫—É churn:")
for risk_level, users in risk_segments.items():
    print(f"\n{risk_level.upper()} —Ä–∏—Å–∫ ({len(users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π):")
    for user in users:
        print(f"  {user['user_id']}: {user['churn_probability']:.3f}")

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ retention
print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ retention:")
if risk_segments['high']:
    print("üö® –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫: –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")
if risk_segments['medium']:
    print("‚ö†Ô∏è –°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫: Email-–∫–∞–º–ø–∞–Ω–∏–∏, feature highlighting")
if risk_segments['low']:
    print("‚úÖ –ù–∏–∑–∫–∏–π —Ä–∏—Å–∫: –ü—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏, upselling")
```

---

## 4. A/B —Ç–µ—Å—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

```python
class TimeBasedABTest:
    @staticmethod
    def retention_ab_test(control_group, treatment_group, time_periods):
        """A/B —Ç–µ—Å—Ç –¥–ª—è retention –º–µ—Ç—Ä–∏–∫"""
        
        def calculate_retention_curve(group, periods):
            curve = []
            initial_size = len(group)
            
            for period in periods:
                retained = sum(1 for user in group 
                             if user.get('retention_days', 0) >= period)
                retention_rate = retained / initial_size if initial_size > 0 else 0
                curve.append(retention_rate)
            
            return curve
        
        control_curve = calculate_retention_curve(control_group, time_periods)
        treatment_curve = calculate_retention_curve(treatment_group, time_periods)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫—Ä–∏–≤—ã—Ö retention
        improvements = []
        for i, period in enumerate(time_periods):
            control_rate = control_curve[i]
            treatment_rate = treatment_curve[i]
            
            lift = ((treatment_rate - control_rate) / control_rate * 100 
                   if control_rate > 0 else 0)
            
            improvements.append({
                'period': period,
                'control_retention': control_rate,
                'treatment_retention': treatment_rate,
                'lift_percent': lift
            })
        
        return improvements
    
    @staticmethod
    def ltv_comparison(control_ltv_data, treatment_ltv_data):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ LTV –º–µ–∂–¥—É –≥—Ä—É–ø–ø–∞–º–∏"""
        control_avg_ltv = sum(control_ltv_data) / len(control_ltv_data)
        treatment_avg_ltv = sum(treatment_ltv_data) / len(treatment_ltv_data)
        
        ltv_lift = ((treatment_avg_ltv - control_avg_ltv) / control_avg_ltv * 100 
                   if control_avg_ltv > 0 else 0)
        
        return {
            'control_avg_ltv': control_avg_ltv,
            'treatment_avg_ltv': treatment_avg_ltv,
            'ltv_lift_percent': ltv_lift,
            'significant': abs(ltv_lift) > 5  # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        }

# –ü—Ä–∏–º–µ—Ä: A/B —Ç–µ—Å—Ç –Ω–æ–≤–æ–π onboarding –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
control_users = [
    {'user_id': f'c_{i}', 'retention_days': days} 
    for i, days in enumerate([7, 14, 30, 45, 60, 90, 120])
]

treatment_users = [
    {'user_id': f't_{i}', 'retention_days': days} 
    for i, days in enumerate([14, 21, 35, 50, 75, 105, 135])  # –£–ª—É—á—à–µ–Ω–Ω—ã–π retention
]

test_periods = [7, 14, 30, 60, 90]
retention_results = TimeBasedABTest.retention_ab_test(
    control_users, treatment_users, test_periods
)

print("A/B —Ç–µ—Å—Ç retention:")
for result in retention_results:
    print(f"–î–µ–Ω—å {result['period']:2d}: "
          f"–ö–æ–Ω—Ç—Ä–æ–ª—å {result['control_retention']:.1%}, "
          f"–¢–µ—Å—Ç {result['treatment_retention']:.1%}, "
          f"–ü—Ä–∏—Ä–æ—Å—Ç {result['lift_percent']:+.1f}%")

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ LTV
control_ltv = [150, 120, 180, 200, 160, 140, 190]
treatment_ltv = [180, 150, 210, 240, 200, 170, 220]

ltv_comparison = TimeBasedABTest.ltv_comparison(control_ltv, treatment_ltv)
print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ LTV:")
print(f"–ö–æ–Ω—Ç—Ä–æ–ª—å: ${ltv_comparison['control_avg_ltv']:.0f}")
print(f"–¢–µ—Å—Ç: ${ltv_comparison['treatment_avg_ltv']:.0f}")
print(f"–ü—Ä–∏—Ä–æ—Å—Ç: {ltv_comparison['ltv_lift_percent']:+.1f}%")
print(f"–ó–Ω–∞—á–∏–º–æ: {ltv_comparison['significant']}")
```

---

## üìö –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

### üéØ –í –ø—Ä–æ–¥—É–∫—Ç–æ–≤–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–µ
- **User retention**: –ê–Ω–∞–ª–∏–∑ —É–¥–µ—Ä–∂–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- **Feature adoption**: –í—Ä–µ–º—è –¥–æ –ø—Ä–∏–Ω—è—Ç–∏—è –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
- **Customer journey**: –ê–Ω–∞–ª–∏–∑ –ø—É—Ç–∏ –∫–ª–∏–µ–Ω—Ç–∞

### üíº –í –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–µ
- **Churn prediction**: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤
- **LTV optimization**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∂–∏–∑–Ω–µ–Ω–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏
- **Cohort analysis**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–∫–æ–ª–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã

- [[time-series|–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã]] - –∞–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- [[statistical-tests|–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã]] - A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- [[practical-applications|–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è]] - —Ä–µ–∞–ª—å–Ω—ã–µ –∫–µ–π—Å—ã

---

## üí° –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

- **Retention Rate**: –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –æ—Å—Ç–∞—é—â–∏—Ö—Å—è –∞–∫—Ç–∏–≤–Ω—ã–º–∏
- **Churn Rate**: –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –ø—Ä–µ–∫—Ä–∞—â–∞—é—â–∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
- **LTV (Lifetime Value)**: –û–±—â–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –∑–∞ –≤—Ä–µ–º—è –∂–∏–∑–Ω–∏
- **Cohort Analysis**: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≥—Ä—É–ø–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏

> üìà **–ê–Ω–∞–ª–∏–∑ –≤—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç–∏**: –ü–æ–Ω–∏–º–∞–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π! 
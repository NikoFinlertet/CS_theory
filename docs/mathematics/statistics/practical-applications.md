# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è üöÄ

> [[time-series|‚Üê –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã]] | [[README|–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ]] | [[bayesian-statistics|–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ‚Üí]]

## üéØ –ß—Ç–æ –∏–∑—É—á–∏–º

- **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ** - –Ω–∞—É—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–¥—É–∫—Ç–æ–≤
- **–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–∏—Å—Ç–µ–º
- **–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã ML –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤

---

## 1. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ IT

### üß™ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫

```python
class ABTestFramework:
    def __init__(self):
        self.experiments = {}
        
    def design_experiment(self, metric_name, baseline_value, expected_improvement):
        """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        import math
        
        # –†–∞—Å—á—ë—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
        p1 = baseline_value
        p2 = baseline_value + expected_improvement
        p_pooled = (p1 + p2) / 2
        
        effect_size = abs(p2 - p1) / math.sqrt(p_pooled * (1 - p_pooled))
        z_alpha, z_beta = 1.96, 0.84  # Œ±=0.05, power=80%
        n_per_group = 2 * ((z_alpha + z_beta) / effect_size) ** 2
        
        return {
            'sample_size_per_group': math.ceil(n_per_group),
            'minimum_runtime_days': math.ceil(n_per_group / 1000),
            'effect_size': effect_size
        }
    
    def analyze_results(self, control_data, treatment_data):
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–π"""
        import math
        
        n1, n2 = len(control_data), len(treatment_data)
        x1, x2 = sum(control_data), sum(treatment_data)
        
        p1 = x1 / n1 if n1 > 0 else 0
        p2 = x2 / n2 if n2 > 0 else 0
        
        # Z-—Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
        p_pooled = (x1 + x2) / (n1 + n2) if (n1 + n2) > 0 else 0
        se = math.sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))
        z_stat = (p2 - p1) / se if se > 0 else 0
        
        # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        lift = (p2 - p1) / p1 * 100 if p1 > 0 else 0
        
        return {
            'control_rate': p1,
            'treatment_rate': p2,
            'lift_percent': lift,
            'statistically_significant': abs(z_stat) > 1.96,
            'z_statistic': z_stat
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
framework = ABTestFramework()

# –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
design = framework.design_experiment("conversion_rate", 0.05, 0.01)
print(f"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {design['sample_size_per_group']} –Ω–∞ –≥—Ä—É–ø–ø—É")

# –ê–Ω–∞–ª–∏–∑ (—Å–∏–º—É–ª—è—Ü–∏—è)
import random
random.seed(42)
control = [1 if random.random() < 0.05 else 0 for _ in range(2000)]
treatment = [1 if random.random() < 0.058 else 0 for _ in range(2000)]

results = framework.analyze_results(control, treatment)
print(f"–ö–æ–Ω–≤–µ—Ä—Å–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—å: {results['control_rate']:.3f}")
print(f"–ö–æ–Ω–≤–µ—Ä—Å–∏—è —Ç–µ—Å—Ç: {results['treatment_rate']:.3f}")
print(f"–ü—Ä–∏—Ä–æ—Å—Ç: {results['lift_percent']:.1f}%")
print(f"–ó–Ω–∞—á–∏–º–æ: {results['statistically_significant']}")
```

---

## 2. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º

```python
class PerformanceMonitor:
    def __init__(self):
        self.baselines = {}
    
    def set_baseline(self, metric_name, historical_data):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–∏"""
        import math
        
        n = len(historical_data)
        mean = sum(historical_data) / n
        variance = sum((x - mean) ** 2 for x in historical_data) / (n - 1)
        std = math.sqrt(variance)
        
        self.baselines[metric_name] = {
            'mean': mean,
            'std': std,
            'warning_threshold': mean + 2 * std,
            'critical_threshold': mean + 3 * std
        }
        
        return self.baselines[metric_name]
    
    def analyze_current(self, metric_name, current_values):
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        if metric_name not in self.baselines:
            return {"error": "–ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"}
        
        baseline = self.baselines[metric_name]
        current_mean = sum(current_values) / len(current_values)
        
        # Z-score
        z_score = (current_mean - baseline['mean']) / baseline['std']
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        if current_mean > baseline['critical_threshold']:
            status = "CRITICAL"
        elif current_mean > baseline['warning_threshold']:
            status = "WARNING"
        else:
            status = "NORMAL"
        
        # –ê–Ω–æ–º–∞–ª–∏–∏
        anomalies = [
            value for value in current_values 
            if abs(value - baseline['mean']) > 3 * baseline['std']
        ]
        
        return {
            'status': status,
            'current_mean': current_mean,
            'z_score': z_score,
            'anomalies_count': len(anomalies),
            'recommendations': self._get_recommendations(status, len(anomalies))
        }
    
    def _get_recommendations(self, status, anomaly_count):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        recommendations = []
        
        if status == "CRITICAL":
            recommendations.append("üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É")
        elif status == "WARNING":
            recommendations.append("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ - –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é")
        
        if anomaly_count > 0:
            recommendations.append(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {anomaly_count} –∞–Ω–æ–º–∞–ª–∏–π")
        
        if not recommendations:
            recommendations.append("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≤ –Ω–æ—Ä–º–µ")
        
        return recommendations

# –ü—Ä–∏–º–µ—Ä –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ API
monitor = PerformanceMonitor()

# –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞ (–º—Å)
historical_times = [120, 118, 125, 122, 119, 121, 117, 123, 116, 124]
baseline = monitor.set_baseline("api_response_time", historical_times)

print(f"–ë–∞–∑–æ–≤–∞—è –ª–∏–Ω–∏—è: {baseline['mean']:.1f} ¬± {baseline['std']:.1f} –º—Å")

# –¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ (—É—Ö—É–¥—à–µ–Ω–∏–µ)
current_times = [145, 138, 152, 148, 155]
analysis = monitor.analyze_current("api_response_time", current_times)

print(f"–°—Ç–∞—Ç—É—Å: {analysis['status']}")
print(f"–¢–µ–∫—É—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ: {analysis['current_mean']:.1f} –º—Å")
print(f"Z-score: {analysis['z_score']:.2f}")
for rec in analysis['recommendations']:
    print(f"  {rec}")
```

---

## 3. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã ML

```python
class MLStatistics:
    @staticmethod
    def cross_validation_stats(cv_scores):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        import math
        
        mean_score = sum(cv_scores) / len(cv_scores)
        variance = sum((score - mean_score) ** 2 for score in cv_scores) / (len(cv_scores) - 1)
        std_score = math.sqrt(variance)
        
        # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
        margin_error = 1.96 * (std_score / math.sqrt(len(cv_scores)))
        
        # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        stability = 1 - (std_score / mean_score) if mean_score > 0 else 0
        
        return {
            'mean_score': mean_score,
            'std_score': std_score,
            'confidence_interval': [mean_score - margin_error, mean_score + margin_error],
            'stability': stability,
            'is_stable': std_score < 0.05
        }
    
    @staticmethod
    def feature_significance_test(feature_scores):
        """–¢–µ—Å—Ç –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        import random
        
        # Baseline –∏–∑ —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        random.seed(42)
        baseline_scores = [random.random() for _ in range(1000)]
        baseline_mean = sum(baseline_scores) / len(baseline_scores)
        baseline_std = (sum((x - baseline_mean) ** 2 for x in baseline_scores) / len(baseline_scores)) ** 0.5
        
        significant_features = []
        for i, score in enumerate(feature_scores):
            z_score = (score - baseline_mean) / baseline_std
            p_value = 0.01 if z_score > 2.5 else 0.05 if z_score > 1.96 else 0.1
            
            significant_features.append({
                'feature_index': i,
                'score': score,
                'z_score': z_score,
                'p_value': p_value,
                'significant': p_value <= 0.05
            })
        
        return sorted(significant_features, key=lambda x: x['score'], reverse=True)

# –ü—Ä–∏–º–µ—Ä –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–µ–ª–∏
ml_stats = MLStatistics()

# Cross-validation —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
cv_scores = [0.85, 0.87, 0.83, 0.86, 0.84]
cv_analysis = ml_stats.cross_validation_stats(cv_scores)

print("Cross-Validation Analysis:")
print(f"–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {cv_analysis['mean_score']:.3f} ¬± {cv_analysis['std_score']:.3f}")
print(f"–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å: {cv_analysis['stability']:.3f}")
print(f"–°—Ç–∞–±–∏–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {cv_analysis['is_stable']}")

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
feature_scores = [0.25, 0.18, 0.15, 0.12, 0.08, 0.06, 0.04, 0.03]
significance = ml_stats.feature_significance_test(feature_scores)

print("\n–¢–æ–ø-3 –∑–Ω–∞—á–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞:")
for feature in significance[:3]:
    if feature['significant']:
        print(f"  –ü—Ä–∏–∑–Ω–∞–∫ {feature['feature_index']}: {feature['score']:.3f} (p={feature['p_value']:.3f})")
```

---

## üìö –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–µ–∫—Ç—ã

### 1. –°–∏—Å—Ç–µ–º–∞ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á—ë—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏
- –û–Ω–ª–∞–π–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è

### 2. Performance Dashboard
- Real-time –∞–Ω–æ–º–∞–ª–∏—è –¥–µ—Ç–µ–∫—Ü–∏—è
- SLA –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
- Predictive alerts

### 3. ML Model Validation
- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è CV –≤–∞–ª–∏–¥–∞—Ü–∏—è
- Feature importance –∞–Ω–∞–ª–∏–∑
- Bias-variance decomposition

---

## üõ†Ô∏è –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

```python
# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import scipy.stats as stats
import pandas as pd
import numpy as np
import scikit-learn as sklearn
```

---

## üéØ –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã

- **–ù–∞—É—á–Ω—ã–π –ø–æ–¥—Ö–æ–¥** - –∏–∑–º–µ—Ä—è–µ–º—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è vs –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å**
- **–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**

> üí° **–ü–æ–º–Ω–∏—Ç–µ**: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤ IT –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ —á–∏—Å–ª–∞, –Ω–æ –∏ –ø—Ä–æ –ø—Ä–∏–Ω—è—Ç–∏–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏! 
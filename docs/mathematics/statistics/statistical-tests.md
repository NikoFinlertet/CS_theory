# –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã üß™

> [[descriptive-statistics|‚Üê –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞]] | [[README|–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ]] | [[time-series|–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã ‚Üí]]

## üéØ –ß—Ç–æ –∏–∑—É—á–∏–º

- **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –≤–µ—Ä—Å–∏–π –ø—Ä–æ–¥—É–∫—Ç–∞
- **–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑** - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –æ –¥–∞–Ω–Ω—ã—Ö
- **–ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏** - –∫–æ–≥–¥–∞ —Ä–∞–∑–ª–∏—á–∏—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω—ã

---

## 1. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import math

class ABTest:
    def __init__(self, control_group, treatment_group):
        self.control = control_group
        self.treatment = treatment_group
    
    def two_sample_t_test(self, alpha=0.05):
        """–î–≤—É—Ö–≤—ã–±–æ—Ä–æ—á–Ω—ã–π t-—Ç–µ—Å—Ç"""
        n1, n2 = len(self.control), len(self.treatment)
        mean1 = sum(self.control) / n1
        mean2 = sum(self.treatment) / n2
        
        # –í—ã–±–æ—Ä–æ—á–Ω—ã–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏
        var1 = sum((x - mean1) ** 2 for x in self.control) / (n1 - 1)
        var2 = sum((x - mean2) ** 2 for x in self.treatment) / (n2 - 1)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        pooled_var = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Ä–∞–∑–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥–Ω–∏—Ö
        se = math.sqrt(pooled_var * (1/n1 + 1/n2))
        
        # t-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        t_stat = (mean2 - mean1) / se
        
        # –°—Ç–µ–ø–µ–Ω–∏ —Å–≤–æ–±–æ–¥—ã
        df = n1 + n2 - 2
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ)
        t_critical = 1.96 if df > 30 else 2.0  # –£–ø—Ä–æ—â–µ–Ω–∏–µ
        
        return {
            'mean_control': mean1,
            'mean_treatment': mean2,
            'difference': mean2 - mean1,
            't_statistic': t_stat,
            'degrees_of_freedom': df,
            'significant': abs(t_stat) > t_critical,
            'effect_size': (mean2 - mean1) / math.sqrt(pooled_var)
        }
    
    def conversion_rate_test(self, alpha=0.05):
        """–¢–µ—Å—Ç –¥–ª—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏"""
        conversions_control = sum(self.control)
        conversions_treatment = sum(self.treatment)
        n1, n2 = len(self.control), len(self.treatment)
        
        p1 = conversions_control / n1
        p2 = conversions_treatment / n2
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –ø—Ä–æ–ø–æ—Ä—Ü–∏—è
        p_pooled = (conversions_control + conversions_treatment) / (n1 + n2)
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
        se = math.sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))
        
        # Z-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        z_stat = (p2 - p1) / se
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        z_critical = 1.96  # –î–ª—è Œ± = 0.05
        
        return {
            'conversion_control': p1,
            'conversion_treatment': p2,
            'lift': (p2 - p1) / p1 if p1 > 0 else float('inf'),
            'z_statistic': z_stat,
            'significant': abs(z_stat) > z_critical,
            'confidence_interval': (p2 - p1) + z_critical * se * [-1, 1]
        }

# –ü—Ä–∏–º–µ—Ä: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
# –í—Ä–µ–º—è, –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω–æ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –Ω–∞ —Å–∞–π—Ç–µ (–≤ –º–∏–Ω—É—Ç–∞—Ö)
control_time = [5.2, 3.8, 4.1, 6.3, 2.9, 5.7, 4.5, 3.2, 5.9, 4.8]
treatment_time = [6.1, 4.9, 5.3, 7.2, 4.1, 6.8, 5.7, 4.3, 6.5, 5.9]

ab_test = ABTest(control_time, treatment_time)
result = ab_test.two_sample_t_test()

print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã A/B —Ç–µ—Å—Ç–∞:")
print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–∫–æ–Ω—Ç—Ä–æ–ª—å): {result['mean_control']:.2f} –º–∏–Ω")
print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (—Ç–µ—Å—Ç): {result['mean_treatment']:.2f} –º–∏–Ω")
print(f"–†–∞–∑–Ω–∏—Ü–∞: {result['difference']:.2f} –º–∏–Ω")
print(f"T-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {result['t_statistic']:.3f}")
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ: {result['significant']}")
print(f"–†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞: {result['effect_size']:.3f}")

# –¢–µ—Å—Ç –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ (1 - –∫–æ–Ω–≤–µ—Ä—Å–∏—è, 0 - –Ω–µ—Ç)
control_conversions = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0]
treatment_conversions = [1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1]

conversion_test = ABTest(control_conversions, treatment_conversions)
conv_result = conversion_test.conversion_rate_test()

print("\n–¢–µ—Å—Ç –∫–æ–Ω–≤–µ—Ä—Å–∏–∏:")
print(f"–ö–æ–Ω–≤–µ—Ä—Å–∏—è (–∫–æ–Ω—Ç—Ä–æ–ª—å): {conv_result['conversion_control']:.2%}")
print(f"–ö–æ–Ω–≤–µ—Ä—Å–∏—è (—Ç–µ—Å—Ç): {conv_result['conversion_treatment']:.2%}")
print(f"–ü—Ä–∏—Ä–æ—Å—Ç: {conv_result['lift']:.2%}")
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ: {conv_result['significant']}")
```

### üéØ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

| –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å | –ó–Ω–∞—á–µ–Ω–∏–µ | –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è |
|-----------|----------|---------------|
| **p-value < 0.05** | –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ | –†–∞–∑–ª–∏—á–∏–µ –Ω–µ —Å–ª—É—á–∞–π–Ω–æ |
| **Effect Size > 0.2** | –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ | –†–∞–∑–ª–∏—á–∏–µ –≤–∞–∂–Ω–æ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞ |
| **CI –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 0** | –ù–∞–¥—ë–∂–Ω–æ–µ —Ä–∞–∑–ª–∏—á–∏–µ | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ |

---

## 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑

```python
class HypothesisTest:
    def __init__(self, data, null_hypothesis_value):
        self.data = data
        self.null_value = null_hypothesis_value
        self.n = len(data)
        self.mean = sum(data) / self.n
        self.std = math.sqrt(sum((x - self.mean) ** 2 for x in data) / (self.n - 1))
    
    def one_sample_t_test(self, alpha=0.05, alternative='two-sided'):
        """–û–¥–Ω–æ–≤—ã–±–æ—Ä–æ—á–Ω—ã–π t-—Ç–µ—Å—Ç"""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ
        se = self.std / math.sqrt(self.n)
        
        # T-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        t_stat = (self.mean - self.null_value) / se
        
        # –°—Ç–µ–ø–µ–Ω–∏ —Å–≤–æ–±–æ–¥—ã
        df = self.n - 1
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ)
        if alternative == 'two-sided':
            t_critical = 2.0 if df < 30 else 1.96
            p_value_approx = 2 * (1 - 0.5 * (1 + math.erf(abs(t_stat) / math.sqrt(2))))
        else:
            t_critical = 1.65 if df < 30 else 1.64
            p_value_approx = 1 - 0.5 * (1 + math.erf(t_stat / math.sqrt(2)))
        
        return {
            'sample_mean': self.mean,
            'null_hypothesis': self.null_value,
            't_statistic': t_stat,
            'degrees_of_freedom': df,
            't_critical': t_critical,
            'p_value': p_value_approx,
            'significant': abs(t_stat) > t_critical,
            'reject_null': p_value_approx < alpha
        }
    
    def power_analysis(self, effect_size, alpha=0.05):
        """–ê–Ω–∞–ª–∏–∑ –º–æ—â–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –º–æ—â–Ω–æ—Å—Ç–∏
        z_alpha = 1.96  # –î–ª—è Œ± = 0.05
        z_beta = effect_size * math.sqrt(self.n) - z_alpha
        power = 0.5 * (1 + math.erf(z_beta / math.sqrt(2)))
        
        return {
            'effect_size': effect_size,
            'sample_size': self.n,
            'power': power,
            'adequate_power': power >= 0.8
        }

# –ü—Ä–∏–º–µ—Ä: –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
# –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: 150 –º—Å
# –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
optimized_times = [145, 138, 142, 140, 135, 148, 136, 143, 139, 141, 137, 144, 138, 142, 140]

test = HypothesisTest(optimized_times, 150)
result = test.one_sample_t_test()

print("–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑—ã –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
print(f"H‚ÇÄ: Œº = {test.null_value} –º—Å (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å)")
print(f"H‚ÇÅ: Œº ‚â† {test.null_value} –º—Å (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–∏–ª–∞—Å—å)")
print(f"–í—ã–±–æ—Ä–æ—á–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ: {result['sample_mean']:.2f} –º—Å")
print(f"T-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {result['t_statistic']:.3f}")
print(f"P-value: {result['p_value']:.4f}")
print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {'–û—Ç–∫–ª–æ–Ω—è–µ–º H‚ÇÄ' if result['reject_null'] else '–ù–µ –æ—Ç–∫–ª–æ–Ω—è–µ–º H‚ÇÄ'}")

# –ê–Ω–∞–ª–∏–∑ –º–æ—â–Ω–æ—Å—Ç–∏
effect_size = (test.mean - test.null_value) / test.std
power_result = test.power_analysis(effect_size)
print(f"\n–ê–Ω–∞–ª–∏–∑ –º–æ—â–Ω–æ—Å—Ç–∏:")
print(f"–†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞: {power_result['effect_size']:.3f}")
print(f"–ú–æ—â–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç–∞: {power_result['power']:.3f}")
print(f"–ê–¥–µ–∫–≤–∞—Ç–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å: {power_result['adequate_power']}")
```

### üìä –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞

```python
class ExperimentPlanner:
    @staticmethod
    def sample_size_calculator(effect_size, power=0.8, alpha=0.05):
        """–†–∞—Å—á—ë—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –¥–ª—è –¥–≤—É—Ö–≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ t-—Ç–µ—Å—Ç–∞
        z_alpha = 1.96  # –î–ª—è Œ± = 0.05 (–¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π —Ç–µ—Å—Ç)
        z_beta = 0.84   # –î–ª—è –º–æ—â–Ω–æ—Å—Ç–∏ 80%
        
        # –ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞
        n_per_group = 2 * ((z_alpha + z_beta) / effect_size) ** 2
        
        return {
            'n_per_group': math.ceil(n_per_group),
            'total_sample_size': math.ceil(n_per_group * 2),
            'effect_size': effect_size,
            'power': power,
            'alpha': alpha
        }
    
    @staticmethod
    def minimum_detectable_effect(n_per_group, power=0.8, alpha=0.05):
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç"""
        z_alpha = 1.96
        z_beta = 0.84
        
        mde = (z_alpha + z_beta) * math.sqrt(2 / n_per_group)
        
        return {
            'minimum_detectable_effect': mde,
            'n_per_group': n_per_group,
            'power': power,
            'alpha': alpha
        }

# –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ A/B —Ç–µ—Å—Ç–∞ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
planner = ExperimentPlanner()

# –•–æ—Ç–∏–º –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ –Ω–∞ 2%
baseline_conversion = 0.15  # 15%
target_conversion = 0.17    # 17%
effect_size = (target_conversion - baseline_conversion) / math.sqrt(baseline_conversion * (1 - baseline_conversion))

sample_size = planner.sample_size_calculator(effect_size)
print("–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ A/B —Ç–µ—Å—Ç–∞:")
print(f"–ë–∞–∑–æ–≤–∞—è –∫–æ–Ω–≤–µ—Ä—Å–∏—è: {baseline_conversion:.1%}")
print(f"–¶–µ–ª–µ–≤–∞—è –∫–æ–Ω–≤–µ—Ä—Å–∏—è: {target_conversion:.1%}")
print(f"–†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞: {effect_size:.3f}")
print(f"–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–π —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {sample_size['n_per_group']} –Ω–∞ –≥—Ä—É–ø–ø—É")
print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {sample_size['total_sample_size']} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")

# –ß—Ç–æ –º–æ–∂–µ–º –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —Å —Ç–µ–∫—É—â–∏–º —Ç—Ä–∞—Ñ–∏–∫–æ–º
current_traffic = 1000  # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –¥–µ–Ω—å –Ω–∞ –≥—Ä—É–ø–ø—É
mde = planner.minimum_detectable_effect(current_traffic)
print(f"\n–° —Ç–µ–∫—É—â–∏–º —Ç—Ä–∞—Ñ–∏–∫–æ–º ({current_traffic} –Ω–∞ –≥—Ä—É–ø–ø—É):")
print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç: {mde['minimum_detectable_effect']:.3f}")
```

---

## 3. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

```python
class MultipleComparisons:
    @staticmethod
    def bonferroni_correction(p_values, alpha=0.05):
        """–ü–æ–ø—Ä–∞–≤–∫–∞ –ë–æ–Ω—Ñ–µ—Ä—Ä–æ–Ω–∏"""
        n_tests = len(p_values)
        corrected_alpha = alpha / n_tests
        
        significant = [p < corrected_alpha for p in p_values]
        
        return {
            'original_alpha': alpha,
            'corrected_alpha': corrected_alpha,
            'significant': significant,
            'n_significant': sum(significant),
            'family_wise_error_rate': alpha
        }
    
    @staticmethod
    def false_discovery_rate(p_values, alpha=0.05):
        """–ö–æ–Ω—Ç—Ä–æ–ª—å –ª–æ–∂–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π (FDR)"""
        n_tests = len(p_values)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º p-values —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏
        indexed_p_values = [(p, i) for i, p in enumerate(p_values)]
        indexed_p_values.sort()
        
        significant = [False] * n_tests
        
        # –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –ë–µ–Ω–¥–∂–∞–º–∏–Ω–∏-–•–æ—Ö–±–µ—Ä–≥–∞
        for k, (p_value, original_index) in enumerate(indexed_p_values):
            threshold = (k + 1) / n_tests * alpha
            if p_value <= threshold:
                significant[original_index] = True
            else:
                break
        
        return {
            'alpha': alpha,
            'significant': significant,
            'n_significant': sum(significant),
            'expected_fdr': alpha
        }

# –ü—Ä–∏–º–µ—Ä: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–µ—Ä—Å–∏–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
page_versions = ['A', 'B', 'C', 'D', 'E']
p_values = [0.03, 0.01, 0.07, 0.002, 0.08]

bonferroni = MultipleComparisons.bonferroni_correction(p_values)
fdr = MultipleComparisons.false_discovery_rate(p_values)

print("–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
print("–í–µ—Ä—Å–∏—è | P-value | Bonferroni | FDR")
print("-------|---------|------------|----")
for i, version in enumerate(page_versions):
    print(f"   {version}   | {p_values[i]:.3f}   |    {'–î–∞' if bonferroni['significant'][i] else '–ù–µ—Ç'}     | {'–î–∞' if fdr['significant'][i] else '–ù–µ—Ç'}")

print(f"\n–ü–æ–ø—Ä–∞–≤–∫–∞ –ë–æ–Ω—Ñ–µ—Ä—Ä–æ–Ω–∏: {bonferroni['n_significant']}/{len(page_versions)} –∑–Ω–∞—á–∏–º—ã—Ö")
print(f"FDR –∫–æ–Ω—Ç—Ä–æ–ª—å: {fdr['n_significant']}/{len(page_versions)} –∑–Ω–∞—á–∏–º—ã—Ö")
```

---

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã

- [[probability-theory|–û—Å–Ω–æ–≤—ã —Ç–µ–æ—Ä–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π]] - —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- [[descriptive-statistics|–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞]] - –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
- [[bayesian-statistics|–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞]] - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥

---

## üõ†Ô∏è –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

### Python –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
```python
import scipy.stats as stats    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
import statsmodels.api as sm   # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
import pingouin as pg         # –£–¥–æ–±–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
from scipy.stats import ttest_ind, chi2_contingency
```

### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è

1. **API Performance Test** - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ, —É–ª—É—á—à–∏–ª–∞ –ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
2. **Feature Launch Analysis** - –æ—Ü–µ–Ω–∏—Ç–µ –≤–ª–∏—è–Ω–∏–µ –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏
3. **Multi-variant Testing** - —Å—Ä–∞–≤–Ω–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä—Å–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

---

## üéØ –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

- **–ù—É–ª–µ–≤–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞** - –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞
- **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å** - –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å** - –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞
- **–ú–æ—â–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç–∞** - —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç
- **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è** - –∫–æ–Ω—Ç—Ä–æ–ª—å –æ—à–∏–±–æ–∫ –ø—Ä–∏ –º–Ω–æ–≥–∏—Ö —Ç–µ—Å—Ç–∞—Ö 
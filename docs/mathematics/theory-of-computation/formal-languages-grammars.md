# –§–æ—Ä–º–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–∏ –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏

> **–û—Å–Ω–æ–≤—ã —Ç–µ–æ—Ä–∏–∏ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤**  
> –ê–ª—Ñ–∞–≤–∏—Ç—ã, —è–∑—ã–∫–∏, –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—è –•–æ–º—Å–∫–æ–≥–æ

[[–¢–µ–æ—Ä–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π]] ‚Üí **–§–æ—Ä–º–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–∏ –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏**

---

## üéØ –ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

### üìù –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
- **–ê–ª—Ñ–∞–≤–∏—Ç** - –∫–æ–Ω–µ—á–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
- **–°—Ç—Ä–æ–∫–∞** - –∫–æ–Ω–µ—á–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –∞–ª—Ñ–∞–≤–∏—Ç–∞
- **–Ø–∑—ã–∫** - –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –Ω–∞–¥ –∞–ª—Ñ–∞–≤–∏—Ç–æ–º
- **–ì—Ä–∞–º–º–∞—Ç–∏–∫–∞** - —Ñ–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ—Ä–æ–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–∞

### üîó –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Ç–µ–º–∞–º–∏
- **[[–ê–≤—Ç–æ–º–∞—Ç—ã]]** - –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤
- **[[–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è]]** - –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤
- **[[–ö–æ–º–ø–∏–ª—è—Ç–æ—Ä—ã –∏ –ø–∞—Ä—Å–µ—Ä—ã]]** - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

---

## üìö –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è

### –ê–ª—Ñ–∞–≤–∏—Ç –∏ —è–∑—ã–∫–∏

```python
# –ü—Ä–∏–º–µ—Ä: –∞–ª—Ñ–∞–≤–∏—Ç –¥–≤–æ–∏—á–Ω—ã—Ö —á–∏—Å–µ–ª
alphabet = {'0', '1'}

# –Ø–∑—ã–∫ –≤—Å–µ—Ö –¥–≤–æ–∏—á–Ω—ã—Ö —Å—Ç—Ä–æ–∫ —á–µ—Ç–Ω–æ–π –¥–ª–∏–Ω—ã
even_length_binary = {
    "", "01", "10", "11", "00", "0011", "0110", "1001", "1100"
    # ... –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ
}

# –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
def is_even_length_binary(word):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –ª–∏ —Å–ª–æ–≤–æ —è–∑—ã–∫—É –¥–≤–æ–∏—á–Ω—ã—Ö —Å—Ç—Ä–æ–∫ —á–µ—Ç–Ω–æ–π –¥–ª–∏–Ω—ã"""
    return len(word) % 2 == 0 and all(char in {'0', '1'} for char in word)

# –û–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ —è–∑—ã–∫–∞–º–∏
def language_operations_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞–¥ —è–∑—ã–∫–∞–º–∏"""
    
    # –ê–ª—Ñ–∞–≤–∏—Ç
    sigma = {'a', 'b'}
    
    # –Ø–∑—ã–∫–∏
    L1 = {'a', 'ab', 'abb'}
    L2 = {'b', 'ba'}
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    union = L1 | L2
    print(f"L1 ‚à™ L2 = {union}")
    
    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    intersection = L1 & L2
    print(f"L1 ‚à© L2 = {intersection}")
    
    # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è
    concatenation = {w1 + w2 for w1 in L1 for w2 in L2}
    print(f"L1 ¬∑ L2 = {concatenation}")
    
    # –ó–∞–º—ã–∫–∞–Ω–∏–µ –ö–ª–∏–Ω–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
    def kleene_star(language, max_length=10):
        """–ó–∞–º—ã–∫–∞–Ω–∏–µ –ö–ª–∏–Ω–∏ –¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã"""
        result = {''}  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤—Å–µ–≥–¥–∞ –≤ L*
        current_level = {''}
        
        for _ in range(max_length):
            next_level = set()
            for word in current_level:
                for lang_word in language:
                    new_word = word + lang_word
                    if len(new_word) <= max_length:
                        next_level.add(new_word)
            
            if not next_level:
                break
                
            result.update(next_level)
            current_level = next_level
        
        return result
    
    star_L1 = kleene_star(L1, 6)
    print(f"L1* (–¥–æ –¥–ª–∏–Ω—ã 6) = {star_L1}")

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
language_operations_demo()
```

### –ò–µ—Ä–∞—Ä—Ö–∏—è –•–æ–º—Å–∫–æ–≥–æ

```python
class ChomskiHierarchy:
    """–ò–µ—Ä–∞—Ä—Ö–∏—è —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –ø–æ –•–æ–º—Å–∫–æ–º—É"""
    
    def __init__(self):
        self.types = {
            0: "–ù–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–µ—Ä–µ—á–∏—Å–ª–∏–º—ã–µ —è–∑—ã–∫–∏)",
            1: "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏", 
            2: "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-—Å–≤–æ–±–æ–¥–Ω—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏",
            3: "–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏"
        }
    
    def explain_type(self, type_num):
        """–û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ç–∏–ø–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
        explanations = {
            3: """
            –†–µ–≥—É–ª—è—Ä–Ω—ã–µ —è–∑—ã–∫–∏ (–¢–∏–ø 3):
            - –†–∞—Å–ø–æ–∑–Ω–∞—é—Ç—Å—è –∫–æ–Ω–µ—á–Ω—ã–º–∏ –∞–≤—Ç–æ–º–∞—Ç–∞–º–∏
            - –ü—Ä–∏–º–µ—Ä—ã: email –≤–∞–ª–∏–¥–∞—Ü–∏—è, –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            - –ü—Ä–∞–≤–∏–ª–∞: A ‚Üí aB, A ‚Üí a (—Ç–æ–ª—å–∫–æ –ø—Ä–∞–≤—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ)
            - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: [[–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è]], –ª–µ–∫—Å–µ—Ä—ã
            """,
            2: """
            –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-—Å–≤–æ–±–æ–¥–Ω—ã–µ —è–∑—ã–∫–∏ (–¢–∏–ø 2):
            - –†–∞—Å–ø–æ–∑–Ω–∞—é—Ç—Å—è –º–∞–≥–∞–∑–∏–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ–º–∞—Ç–∞–º–∏
            - –ü—Ä–∏–º–µ—Ä—ã: –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è, —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
            - –ü—Ä–∞–≤–∏–ª–∞: A ‚Üí Œ± (–ª–µ–≤–∞—è —á–∞—Å—Ç—å - –æ–¥–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)
            - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: [[–ö–æ–º–ø–∏–ª—è—Ç–æ—Ä—ã –∏ –ø–∞—Ä—Å–µ—Ä—ã]]
            """,
            1: """
            –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–µ —è–∑—ã–∫–∏ (–¢–∏–ø 1):
            - –†–∞—Å–ø–æ–∑–Ω–∞—é—Ç—Å—è –ª–∏–Ω–µ–π–Ω–æ-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ–º–∞—Ç–∞–º–∏
            - –ü—Ä–∏–º–µ—Ä—ã: {a‚Åøb‚Åøc‚Åø | n ‚â• 1}
            - –ü—Ä–∞–≤–∏–ª–∞: Œ±AŒ≤ ‚Üí Œ±Œ≥Œ≤ (–∫–æ–Ω—Ç–µ–∫—Å—Ç Œ± –∏ Œ≤)
            - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            """,
            0: """
            –ù–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–∏ (–¢–∏–ø 0):
            - –†–∞—Å–ø–æ–∑–Ω–∞—é—Ç—Å—è [[–ú–∞—à–∏–Ω—ã –¢—å—é—Ä–∏–Ω–≥–∞|–º–∞—à–∏–Ω–∞–º–∏ –¢—å—é—Ä–∏–Ω–≥–∞]]
            - –ü—Ä–∏–º–µ—Ä—ã: –ª—é–±—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏ —Ä–∞–∑—Ä–µ—à–∏–º—ã–µ –∑–∞–¥–∞—á–∏
            - –ü—Ä–∞–≤–∏–ª–∞: –ª—é–±—ã–µ –ø—Ä–æ–¥—É–∫—Ü–∏–∏
            - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: –æ–±—â–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
            """
        }
        return explanations.get(type_num, "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø")
    
    def get_example_grammar(self, type_num):
        """–ü—Ä–∏–º–µ—Ä—ã –≥—Ä–∞–º–º–∞—Ç–∏–∫ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞"""
        examples = {
            3: {
                'description': '–†–µ–≥—É–ª—è—Ä–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è —è–∑—ã–∫–∞ (ab)*',
                'productions': ['S ‚Üí abS', 'S ‚Üí Œµ'],
                'language': '–Ø–∑—ã–∫ –≤—Å–µ—Ö —Å—Ç—Ä–æ–∫ –≤–∏–¥–∞ (ab)*'
            },
            2: {
                'description': '–ö–°-–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∫–æ–±–æ–∫',
                'productions': ['S ‚Üí (S)', 'S ‚Üí SS', 'S ‚Üí Œµ'],
                'language': '–Ø–∑—ã–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–∫–æ–±–æ–∫'
            },
            1: {
                'description': '–ö–ó-–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è —è–∑—ã–∫–∞ {a‚Åøb‚Åøc‚Åø | n ‚â• 1}',
                'productions': ['S ‚Üí aSBC', 'CB ‚Üí BC', 'aB ‚Üí ab', 'bB ‚Üí bb', 'bC ‚Üí bc', 'cC ‚Üí cc'],
                'language': '–Ø–∑—ã–∫ —Å —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º a, b, c'
            },
            0: {
                'description': '–ù–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–π',
                'productions': ['–õ—é–±—ã–µ –ø—Ä–æ–¥—É–∫—Ü–∏–∏ Œ± ‚Üí Œ≤'],
                'language': '–õ—é–±–æ–π —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–µ—Ä–µ—á–∏—Å–ª–∏–º—ã–π —è–∑—ã–∫'
            }
        }
        return examples.get(type_num, {})

# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
hierarchy = ChomskiHierarchy()
for i in range(4):
    print(f"\n=== –¢–∏–ø {i} ===")
    print(hierarchy.explain_type(i))
    example = hierarchy.get_example_grammar(i)
    if example:
        print(f"–ü—Ä–∏–º–µ—Ä: {example['description']}")
        print(f"–Ø–∑—ã–∫: {example['language']}")
```

---

## üîß –ì—Ä–∞–º–º–∞—Ç–∏–∫–∏

### –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞

```python
class ContextFreeGrammar:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-—Å–≤–æ–±–æ–¥–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self):
        # –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
        self.productions = {
            'E': ['E+T', 'E-T', 'T'],           # Expression
            'T': ['T*F', 'T/F', 'F'],           # Term  
            'F': ['(E)', 'id', 'num']           # Factor
        }
        
        self.start_symbol = 'E'
        self.terminals = {'+', '-', '*', '/', '(', ')', 'id', 'num'}
        self.non_terminals = {'E', 'T', 'F'}
    
    def parse_expression(self, expression):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞"""
        tokens = self.tokenize(expression)
        return self.parse_E(tokens, 0)
    
    def tokenize(self, expression):
        """–ü—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
        import re
        pattern = r'(\d+|\w+|[+\-*/()])'
        return re.findall(pattern, expression)
    
    def parse_E(self, tokens, pos):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—ã—Ä–∞–∂–µ–Ω–∏—è E"""
        left, pos = self.parse_T(tokens, pos)
        
        while pos < len(tokens) and tokens[pos] in ['+', '-']:
            op = tokens[pos]
            pos += 1
            right, pos = self.parse_T(tokens, pos)
            left = {'type': 'binary_op', 'op': op, 'left': left, 'right': right}
        
        return left, pos
    
    def parse_T(self, tokens, pos):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ—Ä–º–∞ T"""
        left, pos = self.parse_F(tokens, pos)
        
        while pos < len(tokens) and tokens[pos] in ['*', '/']:
            op = tokens[pos] 
            pos += 1
            right, pos = self.parse_F(tokens, pos)
            left = {'type': 'binary_op', 'op': op, 'left': left, 'right': right}
        
        return left, pos
    
    def parse_F(self, tokens, pos):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–∫—Ç–æ—Ä–∞ F"""
        if pos >= len(tokens):
            raise ValueError("Unexpected end of expression")
        
        token = tokens[pos]
        
        if token == '(':
            pos += 1  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º '('
            result, pos = self.parse_E(tokens, pos)
            if pos >= len(tokens) or tokens[pos] != ')':
                raise ValueError("Missing closing parenthesis")
            pos += 1  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º ')'
            return result, pos
        
        elif token.isdigit():
            return {'type': 'number', 'value': int(token)}, pos + 1
        
        elif token.isalpha():
            return {'type': 'variable', 'name': token}, pos + 1
        
        else:
            raise ValueError(f"Unexpected token: {token}")
    
    def generate_strings(self, max_depth=3):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ —è–∑—ã–∫–∞ –¥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –≥–ª—É–±–∏–Ω—ã"""
        def expand_symbol(symbol, depth):
            if depth <= 0:
                return []
            
            if symbol in self.terminals:
                return [symbol]
            
            if symbol not in self.productions:
                return []
            
            results = []
            for production in self.productions[symbol]:
                if len(production) == 1 and production in self.terminals:
                    results.append(production)
                else:
                    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –ø—Ä–æ–¥—É–∫—Ü–∏—é
                    sub_results = [[]]
                    for char in production:
                        new_sub_results = []
                        for sub_result in sub_results:
                            expansions = expand_symbol(char, depth - 1)
                            for expansion in expansions:
                                new_sub_results.append(sub_result + [expansion])
                        sub_results = new_sub_results
                    
                    for sub_result in sub_results:
                        results.append(''.join(sub_result))
            
            return results
        
        return expand_symbol(self.start_symbol, max_depth)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
grammar = ContextFreeGrammar()
try:
    ast, _ = grammar.parse_expression("2 + 3 * 4")
    print("–î–µ—Ä–µ–≤–æ —Ä–∞–∑–±–æ—Ä–∞ –¥–ª—è '2 + 3 * 4':")
    print(ast)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç—Ä–æ–∫ —è–∑—ã–∫–∞
    generated = grammar.generate_strings(2)
    print(f"\n–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–æ–∫–∏ —è–∑—ã–∫–∞: {generated[:10]}")
    
except Exception as e:
    print(f"–û—à–∏–±–∫–∞: {e}")
```

### –†–µ–≥—É–ª—è—Ä–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞

```python
class RegularGrammar:
    """–†–µ–≥—É–ª—è—Ä–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –∏ –µ–µ —Å–≤—è–∑—å —Å –∫–æ–Ω–µ—á–Ω—ã–º–∏ –∞–≤—Ç–æ–º–∞—Ç–∞–º–∏"""
    
    def __init__(self, productions, start_symbol):
        self.productions = productions  # dict: non_terminal -> [list of productions]
        self.start_symbol = start_symbol
        self.terminals = self._extract_terminals()
        self.non_terminals = set(productions.keys())
    
    def _extract_terminals(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –ø—Ä–æ–¥—É–∫—Ü–∏–π"""
        terminals = set()
        for productions_list in self.productions.values():
            for production in productions_list:
                for char in production:
                    if char.islower() or char in '()+-*/':
                        terminals.add(char)
        return terminals
    
    def to_finite_automaton(self):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –≤ –∫–æ–Ω–µ—á–Ω—ã–π –∞–≤—Ç–æ–º–∞—Ç"""
        states = set(self.non_terminals)
        states.add('ACCEPT')  # –ü—Ä–∏–Ω–∏–º–∞—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        
        transitions = {}
        accept_states = {'ACCEPT'}
        
        for non_terminal, productions_list in self.productions.items():
            for production in productions_list:
                if production == 'Œµ':  # –ü—É—Å—Ç–∞—è –ø—Ä–æ–¥—É–∫—Ü–∏—è
                    accept_states.add(non_terminal)
                elif len(production) == 1 and production in self.terminals:
                    # A ‚Üí a
                    transitions[(non_terminal, production)] = 'ACCEPT'
                elif len(production) == 2 and production[0] in self.terminals:
                    # A ‚Üí aB
                    terminal, next_state = production[0], production[1]
                    transitions[(non_terminal, terminal)] = next_state
        
        return {
            'states': states,
            'alphabet': self.terminals,
            'transitions': transitions,
            'start_state': self.start_symbol,
            'accept_states': accept_states
        }
    
    def generate_language_sample(self, max_length=5):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ç—Ä–æ–∫ —è–∑—ã–∫–∞"""
        def generate_from_symbol(symbol, current_string="", max_len=5):
            if len(current_string) > max_len:
                return []
            
            if symbol not in self.productions:
                return [current_string] if symbol == 'ACCEPT' else []
            
            results = []
            for production in self.productions[symbol]:
                if production == 'Œµ':
                    results.append(current_string)
                elif len(production) == 1 and production in self.terminals:
                    results.append(current_string + production)
                elif len(production) == 2:
                    terminal, next_symbol = production[0], production[1]
                    results.extend(generate_from_symbol(
                        next_symbol, 
                        current_string + terminal, 
                        max_len
                    ))
            
            return results
        
        return generate_from_symbol(self.start_symbol, "", max_length)

# –ü—Ä–∏–º–µ—Ä: —Ä–µ–≥—É–ª—è—Ä–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è —è–∑—ã–∫–∞ (ab)*
ab_star_grammar = RegularGrammar(
    productions={
        'S': ['abS', 'Œµ']  # S ‚Üí abS | Œµ
    },
    start_symbol='S'
)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∞–≤—Ç–æ–º–∞—Ç
automaton = ab_star_grammar.to_finite_automaton()
print("–ê–≤—Ç–æ–º–∞—Ç –¥–ª—è —è–∑—ã–∫–∞ (ab)*:")
print(f"–°–æ—Å—Ç–æ—è–Ω–∏—è: {automaton['states']}")
print(f"–ü–µ—Ä–µ—Ö–æ–¥—ã: {automaton['transitions']}")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤
examples = ab_star_grammar.generate_language_sample(8)
print(f"–ü—Ä–∏–º–µ—Ä—ã —Å—Ç—Ä–æ–∫ —è–∑—ã–∫–∞: {examples}")
```

---

## üéØ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

### 1. –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑

```python
def lexical_analyzer_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≥—Ä–∞–º–º–∞—Ç–∏–∫"""
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
    import re
    
    token_patterns = [
        ('NUMBER', r'\d+'),
        ('IDENTIFIER', r'[a-zA-Z_][a-zA-Z0-9_]*'),
        ('PLUS', r'\+'),
        ('MINUS', r'-'),
        ('MULTIPLY', r'\*'),
        ('DIVIDE', r'/'),
        ('LPAREN', r'\('),
        ('RPAREN', r'\)'),
        ('WHITESPACE', r'\s+'),
    ]
    
    def tokenize(text):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        tokens = []
        position = 0
        
        while position < len(text):
            matched = False
            
            for token_type, pattern in token_patterns:
                regex = re.compile(pattern)
                match = regex.match(text, position)
                
                if match:
                    token_value = match.group(0)
                    if token_type != 'WHITESPACE':  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–±–µ–ª—ã
                        tokens.append((token_type, token_value))
                    position = match.end()
                    matched = True
                    break
            
            if not matched:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {position}: '{text[position]}'")
        
        return tokens
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    test_expressions = [
        "x + 42",
        "result = (a + b) * 2",
        "func_name_123 - 456 / (x + y)"
    ]
    
    for expr in test_expressions:
        print(f"\n–í—ã—Ä–∞–∂–µ–Ω–∏–µ: {expr}")
        try:
            tokens = tokenize(expr)
            print("–¢–æ–∫–µ–Ω—ã:", tokens)
        except ValueError as e:
            print(f"–û—à–∏–±–∫–∞: {e}")

lexical_analyzer_demo()
```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞

```python
def syntax_validator_demo():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ —Å –ø–æ–º–æ—â—å—é –ö–°-–≥—Ä–∞–º–º–∞—Ç–∏–∫"""
    
    class SimpleExpressionValidator:
        """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –ø—Ä–æ—Å—Ç—ã—Ö –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
        
        def __init__(self):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–º–º–∞—Ç–∏–∫—É: E ‚Üí T (('+' | '-') T)*
            #                      T ‚Üí F (('*' | '/') F)*  
            #                      F ‚Üí '(' E ')' | number | identifier
            pass
        
        def validate(self, tokens):
            """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏"""
            self.tokens = tokens
            self.position = 0
            
            try:
                self.parse_expression()
                return self.position == len(tokens), "–í—ã—Ä–∞–∂–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ"
            except Exception as e:
                return False, f"–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}"
        
        def current_token(self):
            """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞"""
            if self.position < len(self.tokens):
                return self.tokens[self.position]
            return None, None
        
        def consume_token(self, expected_type=None):
            """–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞"""
            if self.position >= len(self.tokens):
                raise ValueError("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –∫–æ–Ω–µ—Ü –≤—ã—Ä–∞–∂–µ–Ω–∏—è")
            
            token_type, token_value = self.tokens[self.position]
            
            if expected_type and token_type != expected_type:
                raise ValueError(f"–û–∂–∏–¥–∞–ª—Å—è {expected_type}, –ø–æ–ª—É—á–µ–Ω {token_type}")
            
            self.position += 1
            return token_type, token_value
        
        def parse_expression(self):
            """E ‚Üí T (('+' | '-') T)*"""
            self.parse_term()
            
            while True:
                token_type, _ = self.current_token() or (None, None)
                if token_type in ['PLUS', 'MINUS']:
                    self.consume_token()
                    self.parse_term()
                else:
                    break
        
        def parse_term(self):
            """T ‚Üí F (('*' | '/') F)*"""
            self.parse_factor()
            
            while True:
                token_type, _ = self.current_token() or (None, None)
                if token_type in ['MULTIPLY', 'DIVIDE']:
                    self.consume_token()
                    self.parse_factor()
                else:
                    break
        
        def parse_factor(self):
            """F ‚Üí '(' E ')' | NUMBER | IDENTIFIER"""
            token_type, token_value = self.current_token() or (None, None)
            
            if token_type == 'LPAREN':
                self.consume_token('LPAREN')
                self.parse_expression()
                self.consume_token('RPAREN')
            elif token_type in ['NUMBER', 'IDENTIFIER']:
                self.consume_token()
            else:
                raise ValueError(f"–û–∂–∏–¥–∞–ª—Å—è NUMBER, IDENTIFIER –∏–ª–∏ '(', –ø–æ–ª—É—á–µ–Ω {token_type}")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
    validator = SimpleExpressionValidator()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    def simple_tokenize(text):
        import re
        pattern = r'(\d+|\w+|[+\-*/()])'
        tokens = []
        for match in re.finditer(pattern, text):
            value = match.group(0)
            if value.isdigit():
                tokens.append(('NUMBER', value))
            elif value.isalpha():
                tokens.append(('IDENTIFIER', value))
            elif value == '+':
                tokens.append(('PLUS', value))
            elif value == '-':
                tokens.append(('MINUS', value))
            elif value == '*':
                tokens.append(('MULTIPLY', value))
            elif value == '/':
                tokens.append(('DIVIDE', value))
            elif value == '(':
                tokens.append(('LPAREN', value))
            elif value == ')':
                tokens.append(('RPAREN', value))
        return tokens
    
    test_cases = [
        ("2 + 3", True),
        ("(a + b) * c", True),
        ("x + y + z", True),
        ("2 + + 3", False),
        ("(a + b", False),
        ("* 2 + 3", False),
    ]
    
    for expression, expected in test_cases:
        tokens = simple_tokenize(expression)
        is_valid, message = validator.validate(tokens)
        status = "‚úì" if is_valid == expected else "‚úó"
        print(f"{status} '{expression}': {message}")

syntax_validator_demo()
```

---

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã

### üìö –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–≤—è–∑–∏
- **[[–ê–≤—Ç–æ–º–∞—Ç—ã]]** - –º–æ–¥–µ–ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤
- **[[–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è]]** - –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤  
- **[[–ú–∞—à–∏–Ω—ã –¢—å—é—Ä–∏–Ω–≥–∞]]** - —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤ —Ç–∏–ø–∞ 0
- **[[–°–ª–æ–∂–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π]]** - —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤

### üîß –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
- **[[–ö–æ–º–ø–∏–ª—è—Ç–æ—Ä—ã –∏ –ø–∞—Ä—Å–µ—Ä—ã]]** - —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
- **[[–§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è]]** - —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º
- **[[–ö—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è]]** - –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤

### üìñ –°–º–µ–∂–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã
- **[[–î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞]]** - –º–Ω–æ–∂–µ—Å—Ç–≤–∞, –æ—Ç–Ω–æ—à–µ–Ω–∏—è
- **[[–õ–æ–≥–∏–∫–∞]]** - —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
- **[[–ê–ª–≥–æ—Ä–∏—Ç–º—ã]]** - –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫

---

## üéì –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è

### üìù –ù–∞—á–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤**
   - [ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ –¥–ª—è email –∞–¥—Ä–µ—Å–æ–≤
   - [ ] –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—É—é –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –Ω–æ–º–µ—Ä–æ–≤
   - [ ] –°–æ–∑–¥–∞—Ç—å –ö–°-–≥—Ä–∞–º–º–∞—Ç–∏–∫—É –¥–ª—è JSON –æ–±—ä–µ–∫—Ç–æ–≤

2. **–û–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ —è–∑—ã–∫–∞–º–∏**
   - [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤
   - [ ] –í—ã—á–∏—Å–ª–∏—Ç—å –∑–∞–º—ã–∫–∞–Ω–∏–µ –ö–ª–∏–Ω–∏ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ —è–∑—ã–∫–∞
   - [ ] –ù–∞–π—Ç–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞

### üîß –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
3. **–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≥—Ä–∞–º–º–∞—Ç–∏–∫**
   - [ ] –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—É—é –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –≤ –î–ö–ê
   - [ ] –£—Å—Ç—Ä–∞–Ω–∏—Ç—å –ª–µ–≤—É—é —Ä–µ–∫—É—Ä—Å–∏—é –≤ –ö–°-–≥—Ä–∞–º–º–∞—Ç–∏–∫–µ
   - [ ] –ü–æ—Å—Ç—Ä–æ–∏—Ç—å LL(1) –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π

4. **–ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤**
   - [ ] –î–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ —è–∑—ã–∫ {a‚Åøb‚Åø} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º
   - [ ] –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ö–°-–≥—Ä–∞–º–º–∞—Ç–∏–∫—É –¥–ª—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∫–æ–±–æ–∫
   - [ ] –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏

### üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å
5. **–°–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏**
   - [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å LR –ø–∞—Ä—Å–µ—Ä
   - [ ] –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
   - [ ] –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ö–° —è–∑—ã–∫–æ–≤

---

*–§–æ—Ä–º–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–∏ –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ - —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è* üìöüîß 